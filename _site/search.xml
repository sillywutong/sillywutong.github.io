<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[MIT 6.828 JOS 内存管理详解 操作系统实验5]]></title>
      <url>/lab%20report/2020/02/01/OS5memory/</url>
      <content type="text"><![CDATA[本文详解JOS操作系统虚拟内存的结构，以及内存管理单元的结构与实现。在这次实验中，将探索以下问题：  计算机启动时如何开启虚拟内存  程序虚拟地址空间的结构  如何将内核物理地址与虚拟地址映射  访问一个虚拟地址的时候，会发生什么  如何管理内存空闲空间  如何保护特定的代码和数据  ……操作系统lab5： 内存管理在lab1中，我们看到JOS 物理内存的结构：最初物理内存只有1MB， 之后扩展到了4GB，这时物理内存的640KB 到1MB之间就成为IO hole，是不可用的，用来分配给外部IO设备，如上图，640KB 到1MB之间被分配给了VGA Display、BIOS ROM以及其他的外部设备，称为IO hole。在JOS中，从0x0到640KB这部分称为 basemem，是可用的， 1MB以上的空间称为extented memory，也是可用的。为了更有效地管理和使用内存空间，JOS使用了虚拟内存，虚拟内存通过对程序存储地址与真实内存物理地址的解耦，有效解决了内存大小相对于大量用户程序所需空间不足的问题。引入虚拟内存之后，需要解决如何将多个程序分配到物理内存上，以及程序的虚拟地址如何与物理地址映射的问题。JOS通过分页的方式来管理内存和虚拟地址空间 ，将程序地址空间分为固定大小的页，将内存分为同样大小的页框，以页为单位将程序分配到内存物理空间上。页表记录了一个虚拟页对应的物理页框，以及这些页的相关信息，当程序执行中访问一个虚拟地址时，首先要访问它的页表，然后从页表中找到对应的真实地址，再访问真实的物理地址。在操作系统中，页表的管理、从虚拟地址到物理地址的转换、页面的分配回收以及缓存的管理等等，都是由内存管理单元(MMU)来完成的。内存管理与虚拟内存对用户是不可见的。在这次实验中，将探索以下问题：  计算机启动时如何开启虚拟内存  程序虚拟地址空间的结构  如何将内核物理地址与虚拟地址映射  访问一个虚拟地址的时候，会发生什么  如何管理内存空闲空间  如何保护特定的代码和数据  ……JOS的虚拟地址空间布局在lab1中，我们追踪了开机时bootloader加载内核的过程，加载完成后，物理内存的布局为：（图片来自https://blog-1253119293.cos.ap-beijing.myqcloud.com/图片来自https://blog-1253119293.cos.ap-beijing.myqcloud.com/&lt;/img&gt;JOS用了手写的内存映射，将物理地址0x00000000-0x00400000之间4MB的空间映射到了虚拟地址0xf0000000-0xf0400000处。0xf0000000即为在虚拟地址空间中内核部分的起始。真正开启虚拟内存之后，对于内核和用户程序来说，虚拟地址的布局在memlayout.h中定义：/* * Virtual memory map:                                Permissions *                                                    kernel/user * *    4 Gig --------&gt;  +------------------------------+ *                     |                              | RW/-- *                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ *                     :              .               : *                     :              .               : *                     :              .               : *                     |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| RW/-- *                     |                              | RW/-- *                     |   Remapped Physical Memory   | RW/-- *                     |                              | RW/-- *    KERNBASE, ----&gt;  +------------------------------+ 0xf0000000      --+ *    KSTACKTOP        |     CPU0's Kernel Stack      | RW/--  KSTKSIZE   | *                     | - - - - - - - - - - - - - - -|                   | *                     |      Invalid Memory (*)      | --/--  KSTKGAP    | *                     +------------------------------+                   | *                     |     CPU1's Kernel Stack      | RW/--  KSTKSIZE   | *                     | - - - - - - - - - - - - - - -|                 PTSIZE *                     |      Invalid Memory (*)      | --/--  KSTKGAP    | *                     +------------------------------+                   | *                     :              .               :                   | *                     :              .               :                   | *    MMIOLIM ------&gt;  +------------------------------+ 0xefc00000      --+ *                     |       Memory-mapped I/O      | RW/--  PTSIZE * ULIM, MMIOBASE --&gt;  +------------------------------+ 0xef800000 *                     |  Cur. Page Table (User R-)   | R-/R-  PTSIZE *    UVPT      ----&gt;  +------------------------------+ 0xef400000 *                     |          RO PAGES            | R-/R-  PTSIZE *    UPAGES    ----&gt;  +------------------------------+ 0xef000000 *                     |           RO ENVS            | R-/R-  PTSIZE * UTOP,UENVS ------&gt;  +------------------------------+ 0xeec00000 * UXSTACKTOP -/       |     User Exception Stack     | RW/RW  PGSIZE *                     +------------------------------+ 0xeebff000 *                     |       Empty Memory (*)       | --/--  PGSIZE *    USTACKTOP  ---&gt;  +------------------------------+ 0xeebfe000 *                     |      Normal User Stack       | RW/RW  PGSIZE *                     +------------------------------+ 0xeebfd000 *                     |                              | *                     |                              | *                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ *                     .                              . *                     .                              . *                     .                              . *                     |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| *                     |     Program Data &amp; Heap      | *    UTEXT --------&gt;  +------------------------------+ 0x00800000 *    PFTEMP -------&gt;  |       Empty Memory (*)       |        PTSIZE *                     |                              | *    UTEMP --------&gt;  +------------------------------+ 0x00400000      --+ *                     |       Empty Memory (*)       |                   | *                     | - - - - - - - - - - - - - - -|                   | *                     |  User STAB Data (optional)   |                 PTSIZE *    USTABDATA ----&gt;  +------------------------------+ 0x00200000        | *                     |       Empty Memory (*)       |                   | *    0 ------------&gt;  +------------------------------+                 --+ *[KERNBASE, 4Gig ] :  这部分映射到物理内存上中断向量表、引导扇区代码、IOhole以及内核代码、数据。在这部分，一个虚拟地址 - KERNBASE就是它的物理地址。内核部分会被同样地映射到每个进程的高地址空间，用户是没有权限访问的。KERNBASE往下是进程的地址空间，如之前报告中所述，进程地址空间的高处是内核栈，这部分地址是用户模式下不可访问的。[MMIOBASE, MMIOLIM ] : 这部分空间属于内存映射的IO设备，与IO设备通信要陷入内核完成，因此用户模式也不可访问。[UVPT, ULIM ] : 从ULIM往下直到UTOP是用户模式下只读的地址。UVPT到ULIM的这部分是当前的页目录，用户可以读取页表知道一个虚拟地址所在的物理页面，但不可操纵页表。[UPAGES, UVPT] ：这部分对应着pages数组在物理内存中存放的位置，用户也可以通过uvpt[n].pp_ref来知道某个物理页框是否已经被占用，但也不可操纵。[0, UTOP] : 这部分才真正是用户模式下可以读写的地址空间，它包括了用户程序的代码段、数据段、堆栈等。JOS中的三种地址JOS中有三种地址： 逻辑地址(virtual address)， 线性地址(linear address),  物理地址(physical address). 逻辑地址是程序编译链接之后变量的符号，实际上，逻辑地址是变量的段内偏移。 线性地址是逻辑地址经过保护模式的段地址变换之后的虚拟地址，线性地址=段首地址+逻辑地址。物理地址则是内存存储单元的编址，它会被直接送到内存的地址线上进行读写。逻辑地址到线性地址的变换在保护模式下自动完成。如果没有开启页式地址转换（Paging），那么线性地址就是物理地址，如同我们在lab 1中，mov %eax, cr3之前看到的一样。如果开启分页，线性地址就会按查询页表的方式转换成物理地址。后面的实验内容中，我们直接将线性地址称为虚拟地址。JOS的页表结构页表记录了从虚拟地址到真实物理地址之间的映射，JOS的页表结构、虚拟地址组成定义在mmu.h中，它使用的是一个两级页表： A linear address 'la' has a three-part structure as follows:// +--------10------+-------10-------+---------12----------+// | Page Directory |   Page Table   | Offset within Page  |// |      Index     |      Index     |                     |// +----------------+----------------+---------------------+//  \--- PDX(la) --/ \--- PTX(la) --/ \---- PGOFF(la) ----///  \---------- PGNUM(la) ----------/地址最高10位表示页表目录，中间10位表示页表索引，最后12位表示在一个页面内的偏移，因此，页面总数为$2^{10}$ *$2^{10}$=1024x1024，页面大小为2^12=4096字节。JOS使用两级页表，将全部的地址空间分为了一个页表目录和1024个页表，由于页表有1024个条目，每个条目的长度是4字节，则每个页表刚好就占一个页面，因此页表的地址只需要20位来区分。所以，在页面目录中，我们只需要20位来存放索引对应页表所在的物理地址，剩下的12位用来存放各种标志。页面目录中也含有1024条目，所以页面目录也只占一个页面。所以在用户的虚拟地址中，只需要存放一份页面目录的镜像，就可以让用户程序访问到页表，而不需要将所有1024个页表都映射到用户的虚拟地址空间。一个页表目录条目（或者页表条目，一样的）的结构为：一个目录条目的前20位记录了一个页表的物理地址。访问一个虚拟地址时，首先根据前10位目录索引从page directory上找到相应的条目，取出前20位作为页表的物理地址，然后访问该页表，根据10位的页表索引找到页表上对应的物理地址（也是前20位，与PGSIZE对齐），这个20位的物理地址加上offset就得到了物理地址。如图：条目剩下的低12位用来存放各种标志，来表示一个页表/页面的状态，所有的状态在mmu.h中定义：// Page table/directory entry flags.#define PTE_P		0x001	// Present#define PTE_W		0x002	// Writeable#define PTE_U		0x004	// User#define PTE_PWT		0x008	// Write-Through#define PTE_PCD		0x010	// Cache-Disable#define PTE_A		0x020	// Accessed#define PTE_D		0x040	// Dirty#define PTE_PS		0x080	// Page Size#define PTE_G		0x100	// Global其中，Present位是用来判断对应的页表或者条目是否存在物理内存中，如果存在则为1. 在后面的代码中，我们判断一个虚拟页是否与一个物理页框映射，即是否驻留在内存时，就可以通过 entry &amp; PTE_P来判断。TODO 1: Physical Page Management 代码阅读mem_init()mem_init()在内核刚启动时调用，它的任务是在开机之后，设置好分页系统，并完成内核部分虚拟地址与物理地址的映射。目前只完成了一部分，它需要初始化的变量如下：// These variables are set in mem_init()pde_t *kern_pgdir;		// Kernel's initial page directorystruct PageInfo *pages;		// Physical page state arraystatic struct PageInfo *page_free_list;	// Free list of physical pageskern_pgdir是页表目录。PageInfo是一个用来描述物理页框的结构体，它定义在memlayout.h中, 由一个指向下一个节点的指针，和引用位构成。每一个物理页框都对应着一个PageInfo结构，引用位表示该页框是否已经被占用。pages数组记录了所有物理页框（总共npages个）的信息，而为了分配页面时更快地找到一个空的页框，JOS还维护了page_free_list链表，动态地保存所有空闲的页框。当需要分配页面时，从page_free_list的头部指针获取第一个空闲页框，然后将头部指针后移；当有新的空闲页面时，将这个新页面的指针添加到page_free_list中。struct PageInfo {	// Next page on the free list.	struct PageInfo *pp_link;	// pp_ref is the count of pointers (usually in page table entries)	// to this page, for pages allocated using page_alloc.	// Pages allocated at boot time using pmap.c's	// boot_alloc do not have valid reference count fields.	uint16_t pp_ref;};mem_init具体实现如下：// Set up a two-level page table://    kern_pgdir is its linear (virtual) address of the root//// This function only sets up the kernel part of the address space// (ie. addresses &gt;= UTOP).  The user part of the address space// will be set up later.//// From UTOP to ULIM, the user is allowed to read but not write.// Above ULIM the user cannot read or write.voidmem_init(void){	uint32_t cr0;	size_t n;	// Find out how much memory the machine has (npages &amp; npages_basemem).	i386_detect_memory();     //检测机器有多少物理内存。	// Remove this line when you're ready to test this function.	panic("mem_init: This function is not finished\n");	//////////////////////////////////////////////////////////////////////	// create initial page directory.	kern_pgdir = (pde_t *) boot_alloc(PGSIZE);	memset(kern_pgdir, 0, PGSIZE);	//////////////////////////////////////////////////////////////////////	// Recursively insert PD in itself as a page table, to form	// a virtual page table at virtual address UVPT.	// (For now, you don't have understand the greater purpose of the	// following line.)	// Permissions: kernel R, user R	kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P;	//////////////////////////////////////////////////////////////////////	// Allocate an array of npages 'struct PageInfo's and store it in 'pages'.	// The kernel uses this array to keep track of physical pages: for	// each physical page, there is a corresponding struct PageInfo in this	// array.  'npages' is the number of physical pages in memory.  Use memset	// to initialize all fields of each struct PageInfo to 0.	pages = (struct PageInfo *) boot_alloc(npages * sizeof(struct PageInfo));	memset(pages, 0, npages * sizeof(struct PageInfo));	//////////////////////////////////////////////////////////////////////	// Now that we've allocated the initial kernel data structures, we set	// up the list of free physical pages. Once we've done so, all further	// memory management will go through the page_* functions. In	// particular, we can now map memory using boot_map_region	// or page_insert	page_init();	check_page_free_list(1);	check_page_alloc();	check_page();在JOS开机的时候，我们会看到一句输出：给出了物理内存的可用空间，base是底部的basemem的大小（640K），extended是extended memory的大小，是1MB以上的可用空间。检测是在函数i386_detect_memory()中完成的：static voidi386_detect_memory(void){	size_t basemem, extmem, ext16mem, totalmem;	// Use CMOS calls to measure available base &amp; extended memory.	// (CMOS calls return results in kilobytes.)	basemem = nvram_read(NVRAM_BASELO);	extmem = nvram_read(NVRAM_EXTLO);	ext16mem = nvram_read(NVRAM_EXT16LO) * 64;	// Calculate the number of physical pages available in both base	// and extended memory.	if (ext16mem)		totalmem = 16 * 1024 + ext16mem;	else if (extmem)		totalmem = 1 * 1024 + extmem;	else		totalmem = basemem;	npages = totalmem / (PGSIZE / 1024);	npages_basemem = basemem / (PGSIZE / 1024);	cprintf("Physical memory: %uK available, base = %uK, extended = %uK\n",		totalmem, basemem, totalmem - basemem);}注意到读取basemem、extmem和ext16mem的大小使用了函数nvram_read。nvram_read实际上又调用了mc146818_read函数，这个函数通过IO端口0x70与0x71从实时时钟RTC中读取数据。RTC使用芯片mc146818，在系统电源关闭时，RTC仍保持工作，维护系统的日期和时间，当系统启动时，就从RTC中读取日期时间的基准值。时钟和这里的物理内存其实没有关系，但mc146818芯片中带有一个非易失性的RAM，也就是non-volatile-ram（nvram），系统的物理内存basemem和extmem的大小，都存放在这个芯片上，这样可以保证系统电源关闭时，这些信息不会被擦除。i386_detect_memory通过nvram_read从mc146818芯片中读取出basemem和extmem大小（以KB为单位），然后根据它们计算出内存总的可用空间以及总的页面数npages,npages_basemem。PGSIZE定义在mmu.h中，为4096字节。检测出可用内存大小之后，mem_init开始设置内核的页表。首先调用boot_alloc在物理内存中分配内核的页表。boot_alloc()boot_alloc()只会在JOS初始化虚拟内存之前被调用一次，之后分配页面的时候都只会使用page_allocator(). 之所以要写一个单独的boot_alloc是因为： 在启动时需要将内核的物理地址映射到虚拟地址，这种映射需要通过访问内核的页表来实现，创建页表涉及到分配页表所在的页面，可是分配页面又是在虚拟内存设置好才可以做到。所以，JOS使用了一个单独的boot_alloc，将需要分配的页面映射到一些固定的虚拟地址，并返回所分配的内容的起始虚拟地址。static void *boot_alloc(uint32_t n)   //n表示需要分配的字节数{	static char *nextfree;	// virtual address of next byte of free memory	char *result;    //result 用来保存分配的一片虚拟地址的起始地址	// Initialize nextfree if this is the first time.	// 'end' is a magic symbol automatically generated by the linker,	// which points to the end of the kernel's bss segment:	// the first virtual address that the linker did *not* assign	// to any kernel code or global variables.	if (!nextfree) {                     		extern char end[];		nextfree = ROUNDUP((char *) end, PGSIZE);	}	// Allocate a chunk large enough to hold 'n' bytes, then update	// nextfree.  Make sure nextfree is kept aligned	// to a multiple of PGSIZE.	result = nextfree;	nextfree = ROUNDUP(nextfree+n, PGSIZE);	if((uint32_t)nextfree - KERNBASE &gt; (npages*PGSIZE))		panic("Out of memory!\n");	return result;}nextfree表示下一个未用的虚拟地址, 是一个静态变量。当mem_init调用kern_pgdir = (pde_t *) boot_alloc(PGSIZE)时，nextfree还未初始化，它会被初始化在内核.bss段的结束，并与页面大小4096B 对齐。这里使用了函数ROUNDUP(char* a, uint32_t n)，它同ROUNDDOWN一起在type.h中定义，分别是求 a/n的向上和向下取整，因此，ROUNDUP可以用来将地址a与n对齐。用result保存nextfree作为起始地址后，将nextfree向后移动n个字节（也要和PGSIZE对齐），作为下次分配的起始地址。在分配时，还要检查是否是一个合法的虚拟地址。 从上面JOS虚拟地址空间布局，我们知道nextfree-KERNBASE实际上就是nextfree的物理地址，这个物理地址不可超过内存可用的物理空间大小(页框总数npages*页面大小PGSIZE)，否则抛出错误。回到mem_init中，	kern_pgdir = (pde_t *) boot_alloc(PGSIZE);	memset(kern_pgdir, 0, PGSIZE);将内核页表分配在虚拟地址空间中内核.bss段的后面，然后用memset将页表初始化为全0.kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P;在虚拟地址布局中，我们看到从[UVPT, ULIM]（大小为一个页表大小）这一段是用户和内核都可读的页表目录的复制，那么就要将虚拟地址UVPT映射到kern_pgdir的真实物理地址上去， 而要完成这种映射，就是要在kern_pgdir页表目录中，对应虚拟地址UVPT的条目中，将页表地址改为kern_pgdir的物理地址。 这样，当用户或内核访问UVPT与ULIM之间的虚拟地址时，就要首先访问kern_pgdir，查找uvpt对应的物理地址，然后发现该物理地址就是kern_pgdir所在的物理地址。PDX(la)在mmu.h中定义，计算la对应的页表目录索引。PADDR是将传入的虚拟地址减去KERNBASE，得到物理地址。 PTE_U表示用户有权限（则内核也有权限），PTE_P表示物理地址存在。这个语句将页表目录中UVPT起始的页面对应的条目置为页表目录的物理页面地址， 并设置用户可读。接下来，初始化pages数组，调用boot_alloc将它分配在内核页表目录kern_pgdir之后，并用memset初始化为全0.	pages = (struct PageInfo *) boot_alloc(npages * sizeof(struct PageInfo));	memset(pages, 0, npages * sizeof(struct PageInfo));pages数组用来一对一地记录每一个物理页框是否被占用，可以通过pages[i].pp_ref来判断。之后，mem_init调用page_init().page_init()我们已经初始化了页表目录和pages数组，则page_init()的任务就是初始化page_free_list，记录哪一些物理页框是空闲的，并设置pages中每一个页框的结构。page_free_list实际上只是一个PageInfo结构体，此结构体中包含了指向下一个的指针，也就是下一个空闲的页框。所以，我们可以遍历pages数组，将那些已经分配出去的页框pp_ref置为1，将空闲的页框pp_ref置为0，并让page_free_list指向这个页框，从而将它插入空闲页框链表。根据注释提示，第一个物理页框已经分配给中断向量表和其他的BIOS结构，basemem中剩下的部分([PGSIZE, npages_basemem*PGSIZE])还是空闲的。extmem中，我们刚才已经分配了一部分给内核，要知道分配了多少，我们可以调用boot_alloc(0)来获取分配完pages之后，下一个可用的虚拟地址，将它减去KERNBASE得到物理地址，再除以PGSIZE就得到分配出去的页框数目。IOhole部分，也就是从640KB到1MB之间的96个页面，都分配给了外部IO设备。IOhole和extmem是连续的，因此page_init的实现如下：voidpage_init(void){	//  1) Mark physical page 0 as in use.	//     This way we preserve the real-mode IDT and BIOS structures	//     in case we ever need them.  (Currently we don't, but...)	//  2) The rest of base memory, [PGSIZE, npages_basemem * PGSIZE)	//     is free.	//  3) Then comes the IO hole [IOPHYSMEM, EXTPHYSMEM), which must	//     never be allocated.	//  4) Then extended memory [EXTPHYSMEM, ...).	//     Some of it is in use, some is free. Where is the kernel	//     in physical memory?  Which pages are already in use for	//     page tables and other data structures?	//	size_t i;	page_free_list = NULL;	//num_alloc：在extmem区域已经被占用的页的个数	int num_alloc = ((uint32_t)boot_alloc(0) - KERNBASE) / PGSIZE;	//num_iohole：在io hole区域占用的页数	int num_iohole = 96;	for(i=0; i&lt;npages; i++)	{	    if(i==0)	    {	        pages[i].pp_ref = 1;    //第一个页面已经被占用	    }    	    else if(i &gt;= npages_basemem &amp;&amp; i &lt; npages_basemem + num_iohole + num_alloc)	    {   //从IOhole到extmem中已分配的部分	        pages[i].pp_ref = 1;	    }	    else	    {  //页面空闲，将pp_ref=0，将pp_link置为下一个空闲页面的指针，这样，当该页面被分配出去的时候，我们可以让page_free_list指向pp_link来将它从链表中移出。	        pages[i].pp_ref = 0;	        pages[i].pp_link = page_free_list;	        page_free_list = &amp;pages[i];   //将pages[i]插入到page_free_list的头部。	    }	}}page_alloc()在pages设置好之后，分配页面就不可以再调用boot_alloc了，必须调用page_alloc通过在page_free_link查找空页框的方式来分配页面。page_alloc分配一个页面，返回该页面的PageInfo指针。它同时接收一个参数alloc_flags, 如果它的值为1（ALLOC_ZERO), 就将分配到的物理页面设置为全0。如果没有可用页框，则返回空指针NULL。所以该函数的步骤为：  从page_free_list中取出一个空闲页框的PageInfo结构体。  将这个页框从page_free_list中移去，并将链表头指针指向下一个空闲页框。  修改取出的PageInfo相关信息，如果有ALLOC_ZERO, 修改该内存页。struct PageInfo *page_alloc(int alloc_flags){    struct PageInfo *result;       if (page_free_list == NULL)     //没有空闲页框，返回NULL。        return NULL;      result= page_free_list;       //将第一个空闲页框分配出去      page_free_list = result-&gt;pp_link;    //让page_free_list指向该页框的pp_link,也就是链表上的下一个空闲页框，从而将队首pop出去。      result-&gt;pp_link = NULL;           if (alloc_flags &amp; ALLOC_ZERO)        memset(page2kva(result), 0, PGSIZE);  //用memset将该页面设置为全0.      return result;}其中，page2kva()函数是将传进去的result加上KERNBASE, 得到result的物理地址。 memset将result对应的物理地址开始，一个页面大小的物理内存设置为0.page_free()这个函数将一个被分配的页框归还，只有当该页框的引用位pp_ref为0时，才可以调用：voidpage_free(struct PageInfo *pp){    // Fill this function in    // Hint: You may want to panic if pp-&gt;pp_ref is nonzero or    // pp-&gt;pp_link is not NULL.      assert(pp-&gt;pp_ref == 0);      assert(pp-&gt;pp_link == NULL);      pp-&gt;pp_link = page_free_list;      page_free_list = pp;}assert是断言，用来判断条件是否满足，否则发出panic错误。当页框在page_alloc中被分配出去时，会将pp_link设置为NULL，而页框不再被使用时，pp_ref会置回0，只有这两个条件满足才可以调用page_free.page_free只要简单地将页框插入到page_free_list的表头即可，为此，将pp_link指向现在的链表头部：page_free_list, 然后将头部指针指向该页框（pp).TODO 2:  Virtual Memory参考pgdir_walk, boot_map_region 和page_insert函数，实现page_lookup和page_remove。首先阅读这三个函数的代码，为了方便解释代码，先看一下mmu.h中提供的一些宏，以及pmap.h中提供的功能函数。宏与功能函数在types.h中定义了与内存管理相关的类型：typedef uint32_t uintptr_t;    //表示虚拟地址typedef uint32_t physaddr_t;    //表示物理地址// Page numbers are 32 bits long.typedef uint32_t ppn_t;   //表示页面编号的类型typedef uint32_t pte_t;   //表示一个页表条目的类型typedef uint32_t pde_t;   //表示一个页目录中的条目的类型对于一个虚拟地址va，如果它在KERNBASE以上，说明它是一个内核的虚拟地址，而内核部分是始终驻留在内存中的，我们可以使用pmap.h中定义的PADDR(va)直接将其减去KERNBASE得到物理地址。如果它不在KERNBASE上，那么就要通过MMU访问页表来将它转换成物理地址。相应的，如果是一个内核的物理地址pa, 才可以使用KADDR将它加上KERNBASE得到虚拟地址。在mmu.h中，定义了一些宏，方便从一个虚拟地址获得对应的页目录、页表条目信息以及物理地址：  PGNUM(la): 表示一个虚拟地址的页编号，因为每个页是4096字节，又编号是从0开始顺序编号的，只要将la右移12位。  PDX(la)：对应页目录索引  PTX(la): 对应页表索引  PGOFF(la): 在页面内的偏移  PGADDR(d,t,o): 从已知的页目录索引、页表索引和页内偏移还原一个虚拟地址。  PTE_ADDR(pte): 从一个页目录条目或者一个页表条目中取出它的高20位物理地址部分。在pmap.h中，函数page2pa()实现了给出一个页面，获取这个页面开始处的物理地址； 函数pa2page()实现了给出一个与页大小对齐的物理地址，返回它所在的页面的PageInfo。pgdir_walk()这个函数的功能是，给出一个虚拟地址va， 访问两级页表，找到它对应的页表条目，返回页表条目的指针。但是，由于页表不是一直都整个驻留在内存中的，所以va对应的条目所在的页表页可能还不在内存中，这时，如果create为False，就返回空指针，否则就要使用page_alloc()函数，分配这个页表页。这个过程是：  从虚拟地址va得到它的页目录索引  在页目录上根据索引找到对应条目  判断该条目的present位是否为1， 如果置位，说明对应的页表在内存中，否则不在          如果create置位，要在内存中为这个页表分配一个页框，使用page_alloc().                  如果分配不成功，只能返回NULL。          分配成功，要将这个页表页的引用数pp_ref 加上1， 因为我们现在正要从页表上查va对应的条目。并且，还要在页目录中，记录这个页表的物理地址，设置present为1，并设置权限为用户可读写。                    create为0，返回NULL。        找到了页表后，计算va对应的页表索引  获取页表上该条目，返回条目的地址。（这里所说的地址是该条目的虚拟地址）pte_t * pgdir_walk(pde_t *pgdir, const void * va, int create){      unsigned int page_off;    //页偏移      pte_t * page_base = NULL;    //页表页的基址（虚拟地址）      struct PageInfo* new_page = NULL;    //可能需要分配页表页            unsigned int dic_off = PDX(va);    //用PDX获取va的页目录索引      pde_t * dic_entry_ptr = pgdir + dic_off;    //页目录中对应的条目的虚拟地址，等于页目录的起始地址+条目在页目录中的索引号      if(!(*dic_entry_ptr &amp; PTE_P))    //*dic_entry_ptr获取目录条目中的内容，与PTE_P 判断present是否置位，也就是该页表是否在内存中，如果不在：      {            if(create)   //在内存中分配该页表页            {                   new_page = page_alloc(1);   //分配一个页表页，参数为1，表示该片内存被初始化为全0（之后会将该页表从磁盘读取到这片内存，这不是MMU的工作）                   if(new_page == NULL) return NULL;  //分配失败                   new_page-&gt;pp_ref++;      //分配成功，则该页面的引用数要增加1.                   *dic_entry_ptr = (page2pa(new_page) | PTE_P | PTE_W | PTE_U);// 使用page2pa(newpage)获得该页的起始物理地址，将它存放在对应的页目录条目中，并且置present为1， 更改权限为用户可以读写。            }           else               return NULL;            }       //用PTX(va)宏，获取va对应的页表索引，这就是要返回的条目在页表中的偏移。      page_off = PTX(va);    //page_base用来表示该页表页所在的虚拟地址。要获得此虚拟地址，首先要从页目录表上获得该页表页的物理地址，再用KADDR转换成虚拟地址。      page_base = KADDR(PTE_ADDR(*dic_entry_ptr));      return &amp;page_base[page_off];}最后三行代码最为关键，经过上面的判断和分配页表页，现在该页表页的物理地址已经存放在页目录对应的条目中了， 用PTE_ADDR(*dic_entry_ptr)就可以从条目中取出该页表页的物理地址。用KADDR可以将物理地址转换为页目录的虚拟地址，这时，其实将page_base+page_off就可以得到该条目的虚拟地址了。因此，最后return &amp;page_base[page_off]也可以替换为return page_base+page_off。boot_map_region()boot_map_region的功能是，将虚拟地址[va, va+size]映射到物理地址[pa, pa+size]上，意思就是在页表中[va, va+size]对应的条目中设置物理地址为[pa, pa+size]。 这里va, pa,size都是保证与页面大小对齐的，size的单位是页。perm参数给出了这块内存空间的权限。这个函数是用来“静态“地映射UTOP以上的用户只读空间的。过程是：  遍历从[va, va+size]的每一个虚拟页，使用pgdir_walk找到它在页表中的对应条目                              将该条目的内容设置为 [pa , pa+size ]          perm          PTE_P. 表示present置位，这些页面存在于内存中，并设置了权限。                    static voidboot_map_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm){    int nadd;    pte_t *entry = NULL;    for(nadd = 0; nadd &lt; size; nadd += PGSIZE)    {        entry = pgdir_walk(pgdir,(void *)va, 1);    //Get the table entry of this page.        *entry = (pa | perm | PTE_P);                            pa += PGSIZE;        va += PGSIZE;            }}page_insert()这个函数是将一个物理页框pp映射到虚拟地址va上。要考虑两种情况，一是该va已经映射到了其他的物理页框上，这时就要接触这个映射关系； 另一种是该va本来就映射到pp上了。过程是：  调用pgdir_walk得到va在页表上的条目。  如果找不到该条目，说明内存不足，返回错误码 -E_NO_MEM。  要先让pp-&gt;pp_ref++，之后解释原因。  如果该条目已经存在，说明va本来已经映射到一个物理页框上，          tlb_invalidate使该va对应的页表条目失效，这样，才不会使进程在这个过程中访问到不正确的物理地址。这是因为进程会缓存它用到的页表，快速访问页表时，它先访问缓存中是否有该页表，如果没有，才从页目录去找。      page_remove解除va和原来物理页框的映射。        无论之前该条目是否存在，现在va已经不与任何物理页框绑定，将条目内容设置为pp的物理地址，设置present为1，并设置权限为permintpage_insert(pde_t *pgdir, struct PageInfo *pp, void *va, int perm){      pte_t *entry = NULL;    entry =  pgdir_walk(pgdir, va, 1);    //Get the mapping page of this address va.    if(entry == NULL) return -E_NO_MEM;    pp-&gt;pp_ref++;    if((*entry) &amp; PTE_P)             //If this virtual address is already mapped.    {        tlb_invalidate(pgdir, va);        page_remove(pgdir, va);    }    *entry = (page2pa(pp) | perm | PTE_P);    pgdir[PDX(va)] |= perm;                  //Remember this step!            return 0;}在这个函数中，只用(*entry) &amp; PTE_P判断页表条目是否存在，来判断va是否已经有映射关系，但没有区分va是否与pp映射。如果我们将pp-&gt;pp_ref++移到if块后面，那么当va已经与pp映射时，pp原来的引用数有可能为0，在if中，我们会不加判断地调用page_remove，然后因为引用数为0，直接调用page_free将它释放掉。 既然释放了，它就会处在空闲链表page_free_list中，pp_ref应该保持为0. 我们之后再用pp-&gt;pp_ref++时，就会让空闲链表管理出错，下一次分配页框时，可能在空闲链表中找到这个pp，但它是不可用的。接下来我们实现page_lookup和page_remove函数。page_lookup()这个函数的功能是给出虚拟地址va，找到它映射到的物理页框。如果传入的pte_store不为NULL的话，就将该虚拟地址对应的页表条目指针存放到pte_store中。实现过程是：  调用pgdir_walk找到va对应的条目，这里create应该设置为0，即若va所在的页表页不在内存，我们也不分配它。  如果返回的是NULL，表示va所在的页表页不在内存，即va现在没有映射到物理页框，返回NULL。  如果va对应的页表页在内存中，但是条目的present位为0，说明va没有映射到物理页框:          判断pte_store，如果不为NULL，将条目存放在pte_store中      返回NULL        用PTE_ADDR从条目上获得va对应的物理地址  用pa2page获取物理地址对应页框的PageInfo  如果pte_store非空，将pte保存，最后返回PageInfostruct PageInfo *page_lookup(pde_t *pgdir, void *va, pte_t **pte_store){	pte_t *pte = NULL;	struct PageInfo *page;    pte = pgdir_walk(pgdir, va, 0);     //不存在，不分配    if(pte==NULL)            //页表页不存在        return NULL;        if(pte_store)              *pte_store = pte;    if(!(*pte &amp; PTE_P)) //页表页存在，但是va并不与一个物理页框映射        return NULL;    	page = pa2page(PTE_ADDR(*pte));	return page;}page_remove()这个函数的功能是解除va与它对应的物理页框之间的映射关系。这不一定说明该物理页框已经空闲，可以回到空闲链表中被分配。因为该页框的引用数并不一定为0，例如在共享内存时，有可能不同进程会共享一部分物理内存，不同的虚拟地址会映射到同一个物理页框上。因此，这个函数的实现过程应该是：  调用page_lookup查找va对应的物理页，并保存其页表条目的地址在&amp;pte中。  如果该va有映射到某个物理页框，解除映射：          调用page_decref，这里面做的是，将pp_ref减去1，如果等于0，可以调用page_free释放空间，将页框归还。      因为使用快速页表访问时，进程可能缓存了最近使用过的页表条目，所以要调用tlb_invalidate让这条va的缓存条目无效，否则进程会优先访问缓存中的条目，进而访问到非法的物理地址。      将va对应的条目内容清0.      voidpage_remove(pde_t *pgdir, void *va){    pte_t *pte;	struct PageInfo *page = page_lookup(pgdir, va, &amp;pte);		if(page){		page_decref(page);		tlb_invalidate(pgdir, va);   //使TLB中可能缓存的这条页表条目无效		*pte=0;	}}检查pmap.c中实现了几个函数对代码进行了检查。这些检查函数在mem_init中被调用，其中check_page()是检查分页的基础功能是否已经实现好，包括page_alloc(), page_insert(), page_remove()，page_lookup(), pgdir_walk() 以及page_free()。重新编译并启动qemu，看到控制台输出：” check_page() succeeded!” 表明实现是正确的。TODO 3: Kernel Address Space###完成mem_init()上面的函数已经完成了分页机制，页表也已经创建好。现在，我们就可以通过修改页表上的条目，完成UTOP以上用户不可操作的空间与物理地址的映射。首先，将[UPAGES, UVPT]这部分虚拟地址映射到pages数组上，权限设置为内核与用户只读，相当于为用户保留了物理页框信息的拷贝。这样，虚拟地址空间上实际有两份pages，一部分在KERNBASE以上，用户不可见，内核可读写，这一份就是在mem_init刚开始分配的pages；另一份是为用户准备的只读拷贝，两者通过页表映射到同一片物理内存上，但这份拷贝设置的权限是只读，所以用户不能对这部分虚拟地址的内容进行操作；又因为用户不可访问KERNBASE上面的虚拟地址（在用户访问虚拟地址时，内核会判断虚拟地址是否超出ULIM），所以用户不可读写pages。这样就实现了对pages数组的保护。这个映射用boot_map_region(kern_pgdir, UPAGES, PTSIZE, PADDR(pages), PTE_U);来实现。然后要完成内核栈的映射。从[KSTACKTOP - PTSIZE, KSTACKTOP]这部分都属于内核栈， 但被分为两个部分，上面[KSTACKTOP-KSTKSIZE, KSTACKTOP]是真正的内核栈，与某些物理页框映射，而[KSTACKTOP - PTSIZE, KSTACKTOP-KSTKSIZE ]不与物理地址映射，只是用来防止内核栈向下增长的时候发生溢出，然后覆盖了Memory-mapped IO部分，称为保护页。如果内核栈溢出，它就会发现物理地址不存在，抛出错误。 *    KERNBASE, ----&gt;  +------------------------------+ 0xf0000000      --+ *    KSTACKTOP        |     CPU0's Kernel Stack      | RW/--  KSTKSIZE   | *                     | - - - - - - - - - - - - - - -|                   | *                     |      Invalid Memory (*)      | --/--  KSTKGAP    | *                     +------------------------------+                   | *                     |     CPU1's Kernel Stack      | RW/--  KSTKSIZE   | *                     | - - - - - - - - - - - - - - -|                 PTSIZE *                     |      Invalid Memory (*)      | --/--  KSTKGAP    | *                     +------------------------------+                   | *                     :              .               :                   | *                     :              .               :                   | *    MMIOLIM ------&gt;  +------------------------------+ 0xefc00000      --+ *                     |       Memory-mapped I/O      | RW/--  PTSIZE * ULIM, MMIOBASE --&gt;  +------------------------------+ 0xef800000内核栈是内核可读写，但用户不可见的，这些页面的权限要被设置为PTE_W。调用boot_map_region, 将KSTACKTOP-KSTKIZE开始到KSTACKTOP的虚拟地址映射到bootstack的物理地址上，大小如上面结构所示，为KSTKSIZE，但要注意使用ROUNDUP与页面大小对齐：boot_map_region(kern_pgdir, KSTACKTOP-KSTKSIZE, ROUNDUP(KSTKSIZE, PGSIZE), PADDR(bootstack), PTE_W);将整一个[KERNBASE, 2^32）的整个内核地址空间映射到内存的[0, 2^32-KERNBASE)，权限是内核可修改但用户不可见。我们知道KERNBASE的虚拟地址是0xf0000000, 而整个虚拟空间的大小是2^32也就是4G，所以内核的大小总共是256MB=0X10000000，这已经是与页面大小对齐的。 内存将一直有256MB的空间被内核占用。调用boot_map_region实现如下：boot_map_region(kern_pgdir, KERNBASE, 0x10000000, 0, PTE_W);检验重新编译，启动QEMU，所有的检查都已经通过，说明分页机制与内核的分配已经正确实现：总结分页机制建立和开启全过程：完整地阅读mem_init，voidmem_init(void){	uint32_t cr0;	size_t n;	i386_detect_memory();	kern_pgdir = (pde_t *) boot_alloc(PGSIZE);	memset(kern_pgdir, 0, PGSIZE);    	kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P;	pages = (struct PageInfo *) boot_alloc(npages * sizeof(struct PageInfo));	memset(pages, 0, npages * sizeof(struct PageInfo));	page_init();	boot_map_region(kern_pgdir, UPAGES, PTSIZE, PADDR(pages), PTE_U);	boot_map_region(kern_pgdir, KSTACKTOP-KSTKSIZE, ROUNDUP(KSTKSIZE, PGSIZE), PADDR(bootstack), PTE_W);    	boot_map_region(kern_pgdir, KERNBASE, 0x10000000, 0, PTE_W);	lcr3(PADDR(kern_pgdir));	cr0 = rcr0();	cr0 |= CR0_PE|CR0_PG|CR0_AM|CR0_WP|CR0_NE|CR0_MP;	cr0 &amp;= ~(CR0_TS|CR0_EM);	lcr0(cr0);}内核部分虚拟地址空间的初始化全过程是：      检测总共可用的物理内存大小，由basemem+extmem构成，记录总页数npages和低地址页数npages_basemem        要初始化内核虚拟地址，必须通过页表来映射，但一开始，页表还不存在，页表的虚拟地址也还没有映射。因此，首先要分配页表目录的虚拟地址。这个分配无法通过分页机制来完成，只能通过静态映射：kern_pgdir = boot_alloc(PGSIZE). 页目录分配好后，首先初始化为全0.        因为用户进程在访问虚拟地址时，需要访问页表，因此我们需要在ULIM以下为用户准备一份页表的只读拷贝。然而，总共1024份页表的开销较大，其实只要能够访问到页目录，就可以通过页目录访问到页表。 而我们甚至不用真的在内存中存放两份页目录，只需要将拷贝的虚拟地址也指向kern_pgdir的物理地址即可。 所以，我们在页目录上让UVPT条目指向页目录本身，并设置用户只读：kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P;, 结构如图：                这里CR3寄存器存放的是进程页目录的虚拟地址。操作系统中有内核页表和进程页表两种页表，进程页表是每个进程独自有一份的，页目录的虚拟地址存放在cr3寄存器中，当进程切换时，会加载页目录虚拟地址到cr3寄存器。进程页表既包含了用户态，也包含了内核态的虚拟地址，内核态的虚拟地址是所有进程都一样的，就是内核页表的拷贝，它的虚拟地址就在UVPT，大小为一个页面。            拷贝好内核页目录后，因为接下来涉及到管理物理内存，需要记录每个页框的信息，这些信息保存在pages数组中。同样，需要用boot_alloc将pages静态映射到一片虚拟地址。如果打印出pages和kern_pgdir的虚拟地址，可以看到他们的虚拟地址分别是f0119000和f0118000， 是在f0400000之内的，这两个数据结构已经在物理内存中了。        将pages初始化为0，然后用page_init将已经分配出去的物理页框引用位置为1，并将空闲物理页框添加到page_free_list链表中，接下来，内核就可以使用page_free_list和pages两个数据结构管理物理页框。        允许用户读取pages来知道内存的占用情况，但为了保护该数据结构，同样要在ULIM以下为它保留一份拷贝，这份拷贝在UPAGES到UVPT之间，用boot_map_region来进行映射。        boot 的时候， 已经为内核初始化了一个栈，栈顶的界限是bootstack，用boot_map_region将虚拟地址的KSTACK部分映射到bootstack所在的物理内存上，进程进入内核态，并进入公共部分时，实际上运行在这个内核栈上。        最后，把整个高地址部分的内核虚拟空间映射到物理内存0地址开始处，实际上是内核部分一直驻留在内存中，且内核的虚拟地址空间被拷贝到每个进程的高地址部分。        lcr3(PADDR(kern_pgdir))是把内核页目录基址放在cr3寄存器中，之后如果开启分页，访问虚拟地址时，就会从cr3加载页目录地址，从而访问页目录。        将控制寄存器cr0置位为：CR0_PE|CR0_PG|CR0_AM|CR0_WP|CR0_NE|CR0_MP ，~(CR0_TS|CR0_EM)各个字段含义如下：                  #define CR0_PE		0x00000001	// Protection Enable#define CR0_MP		0x00000002	// Monitor coProcessor#define CR0_EM		0x00000004	// Emulation#define CR0_TS		0x00000008	// Task Switched#define CR0_ET		0x00000010	// Extension Type#define CR0_NE		0x00000020	// Numeric Errror#define CR0_WP		0x00010000	// Write Protect#define CR0_AM		0x00040000	// Alignment Mask#define CR0_NW		0x20000000	// Not Writethrough#define CR0_CD		0x40000000	// Cache Disable#define CR0_PG		0x80000000	// Paging                    cr0被设置为，开启保护模式，保护模式开启时只是开启了段级保护，没有开启分页，即逻辑地址转换成线性地址，线性地址直接等于物理地址, CR0_PG开启分页，CR0_AM 开启地址对齐检查， 开启写保护， 开启协处理器错误， 开启监控协处理器。TS任务已切换标志为0，EM为0，表示有协处理器，会将浮点指令交给协处理器用软件来模拟。            完成。  如何保护内核数据和代码：  通过虚拟地址空间的隔离。检查用户访问的虚拟地址与ULIM，可以防止用户访问高地址。  为ULIM以下的每个页面设置权限，并启用cr0中的非法写保护，可以防止无权限用户修改只读页面。如何管理内存空闲空间，以及管理的开销：  通过一对一地维护每个页框的信息，并动态维护一个空闲页框链表（头指针）来实现。  每个页框PageInfo的大小是8B，pages的大小是PTSIZE=4MB, 总共可存放512K个PageInfo，即可维护512K个物理页，总共512K*PGSIZE=2G物理内存。  如果全部2G的物理内存都分配出去，那么维护的开销是，pages大小+kern_pgdir大小+所有页表页大小=4MB+4K+2MB的额外内存。推荐阅读： OS操作系统实验 xv6调度算法实现]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> JOS </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OS操作系统实验：xv6调度(RR, 优先级调度, 优先级队列） 实现和分析 Part 1- xv6代码讲解]]></title>
      <url>/lab%20report/2020/01/31/OS4/</url>
      <content type="text"><![CDATA[Lab 4：调度调度任何操作系统中，都可能出现进程的个数大于处理器个数的情况，这就需要考虑如何分配处理器资源。一般进程的执行是CPU计算和IO操作的循环，当进程长时间等待某种资源时，为了更好地利用CPU资源，应选择其他准备好的进程来代替它；当进程完成所需的时间过长时，为了让其他已经在系统中的进程等待时间不要过长，也需要在某个适当的时间暂停当前的进程, 因此便需要多进程并发，也就需要调度。调度指的是决定一个进程在什么时候、是否要暂停，从一个等待队列中选择另一个进程来代替它，调度涉及调度策略的选择，也包含完成进程切换的动作。操作系统通过不断地调度，造成并发的效果，同时也为每个进程造成独占资源的假象。调度涉及以下的问题：  如何进行进程的切换？这是通过上下文的切换来实现的。上下文是一个进程运行状态的描述，包括程序计数器(%eip), 栈底指针(%ebp), 以及其他一些寄存器的值。在进程切换时，首先要保存旧进程的上下文在内核栈上，选择一个新进程，从该进程的内核栈上加载它的上下文，然后CPU就开始执行新进程%eip指向的指令。上下文的保存和加载使得程序可以从上次调度被暂停的地方接着进行，对进程来说，就好像切换从来没有发生过一样。  如果进程不是调用sleep主动放弃CPU，如何让进程的切换透明化呢？ xv6简单地使用时钟中断来完成。当时钟中断到来时，进程陷入中断处理程序，在内核中调用yield来进行上下文切换。  多个CPU同时在切换进程时，由于需要对进程表进行修改，可能会产生竞态条件，因此还要用锁来避免竞争。  进程结束时，需要释放资源。进程不能自己释放自己的所有资源，因此内核中还必须有一个角色负责监测进程的结束、释放资源。  当进程调用sleep进入睡眠时，调度也会发生。这时，要确保有其他进程可以唤起该进程，因此xv6需要提供一套进程间通信的机制，例如sleep和wake up。本次实验将详细研究整个调度的过程，看xv6如何解决上述问题，并实现优先级调度算法。在开始实验之前，需要了解以下事实： xv6永远不会从一个用户态进程切换到另一个用户态进程。在xv6中，调度发生在以下几种情况：1. 进程调用sleep进入休眠，主动放弃CPU，这会导致进程进入内核态，保存进程的上下文并加载调度器的上下文，当调度器返回时，该进程仍处于内核态；2. 进程收到时钟中断，已经运行完一个时间片，这也会导致进程进入内核态，并在yield中将控制权交给调度器；3. 进程调用exit结束。 在这些情况下，切换的过程都是 陷入内核→保存上下文→切换到调度器的上下文 → 切换到新进程的上下文（在内核态中） → 返回新进程的用户态。TODO1: 阅读proc.c中的函数我们先看调度发生的一般场景：进程运行完时间片，被迫放弃CPU，选择下一个进程调度。xv6的时钟每100毫秒就产生一个中断，以此实现进程时间分片。时钟中断是由lapic产生的，因此每个cpu可以独立地接收它的时钟中断。当接收到时钟中断时，进程会开启保护模式，陷入到内核态，来到中断处理程序的入口，然后在alltraps中保存中断帧，调用traps，traps根据中断号来判断应该执行哪种程序。在traps的最后，有可能调用yield使进程放弃CPU：  // Force process to give up CPU on clock tick.  // If interrupts were on while locks held, would need to check nlock.  if(myproc() &amp;&amp; myproc()-&gt;state == RUNNING &amp;&amp;     tf-&gt;trapno == T_IRQ0+IRQ_TIMER)   //当目前CPU上有正在运行的进程，且中断为时钟中断时，才会调用yield    yield();yieldyield函数在proc.c中实现:// Give up the CPU for one scheduling round.voidyield(void){  acquire(&amp;ptable.lock);  //DOC: yieldlock  myproc()-&gt;state = RUNNABLE;  sched();  release(&amp;ptable.lock);}yield是将目前进程的状态从RUNNING改为RUNNABLE，让进程进入等待队列，然后调用sched将控制权转移给调度器。由于进程PCB存放在进程表上，因此对状态进行修改之前要首先acquire(&amp;ptable.lock)获取进程表的锁，等进程再次被调度时，它会返回到yield中sched的下一行，释放进程表锁。这里要注意，在上一个实验中，我们知道xv6的内核中临界节内不允许中断，所以在进入sched之前，中断是已经关闭的状态。schedsched的任务是首先确保进程有放弃CPU，进行调度的条件，然后调用swtch进行上下文切换，转到cpu调度器scheduler上。// Enter scheduler.  Must hold only ptable.lock// and have changed proc-&gt;state. Saves and restores// intena because intena is a property of this// kernel thread, not this CPU. It should// be proc-&gt;intena and proc-&gt;ncli, but that would// break in the few places where a lock is held but// there's no process.voidsched(void){  int intena;  struct proc *p = myproc();  if(!holding(&amp;ptable.lock))    panic("sched ptable.lock");  if(mycpu()-&gt;ncli != 1){    panic("sched locks");  }  if(p-&gt;state == RUNNING)    panic("sched running");  if(readeflags()&amp;FL_IF)    panic("sched interruptible");  intena = mycpu()-&gt;intena;  swtch(&amp;p-&gt;context, mycpu()-&gt;scheduler);  mycpu()-&gt;intena = intena;}首先，holding(&amp;ptable.lock)判断该cpu是否已经持有了进程表的锁，因为多个CPU在调度过程中都需要访问进程表，假如这个cpu进入调度之前，没有先持有锁，那就有可能使其他cpu也同时进行调度，同时访问进程表，可能会出现两个cpu选择了同一个进程调度的情况。然后mycpu()-&gt;ncli!=1判断该cpu调用pushcli的次数是否恰好为1，否则会报错。在上个实验我们知道，每一次调用acquire获得锁，就会使ncli加一，释放锁后ncli减一。所以ncli为1就说明这里只允许进程持有一个锁，也就是说，进程被切换时，必须持有进程表的锁，并且必须释放其他所有的锁。持有进程表的锁是为了保证CPU的调度是互斥的，防止竞态条件，而释放其他所有锁是为了防止死锁的情况出现。如果p-&gt;state==RUNNING，进程的状态是仍在运行，不可以进入调度。这是操作系统中约定好的分工：sched应该只负责进入调度器，而不应该判断进程是因为什么原因而被暂停的，所以假如进程是终止了，应该由exit来将状态变为ZOMBIE，如果进程是被时钟中断了，应该由yield将状态变为RUNNABLE，休眠也同理。进入sched之前，进程的状态应该已经改变好。sched最后通过readeflags()FL_IF，检查标志寄存器中IF段的值，确保中断已经关闭，然后它将mycpu()-&gt;intena暂时保存起来，这个变量表示CPU在调用yield之前，中断是否被允许，因为之后在调度器中要调用sti开启中断，可能会破坏原来CPU的中断状态，所以暂存起来，等从调度器返回（进程被重新调度）的时候，再恢复这个值。sched调用swtch(&amp;p-&gt;context, mycpu()-&gt;scheduler)来切换上下文，swtch的汇编代码如下：# Context switch##   void swtch(struct context **old, struct context *new);# # Save the current registers on the stack, creating# a struct context, and save its address in *old.# Switch stacks to new and pop previously-saved registers..globl swtchswtch:  movl 4(%esp), %eax  movl 8(%esp), %edx              # swtch的第二个参数，即新的上下文  # Save old callee-saved registers  pushl %ebp                      # 保存旧进程内核栈的栈底指针  pushl %ebx                      # 保存旧进程%ebx寄存器  pushl %esi					# 保存旧进程%esi  pushl %edi					#               和%edi寄存器  # Switch stacks  movl %esp, (%eax)  movl %edx, %esp				#   # Load new callee-saved registers  popl %edi  popl %esi  popl %ebx  popl %ebp  ret首先，进程上下文中包含的信息有：struct context {  uint edi;  uint esi;  uint ebx;  uint ebp;  uint eip;};为什么只需要保存这些呢？假设进程在某个函数中，发生了上下文切换，那么首先不需要保存的是调用者保存的寄存器，如%eax，%ecx，%edx，因为该函数的调用者已经提前把它们保存在进程的栈上了。也不需要保存段寄存器，如%cs等，因为在指令地址发生改变的时候，这些寄存器也会同时改变。所以，要保存的有栈底指针、%ebx、程序计数器%eip、参数寄存器%edi、%esi。上下文保存在进程内核栈上：在进程从yield进入到sched再进入到swtch的这个时候，cpu首先是运行在旧进程的内核栈上的。在这里，swtch传入两个参数，第一个是旧进程上下文的指针的地址，第二个是该cpu调度器进程的上下文的指针，调度器的上下文也是调度器上一次调用swtch时保存的。我们逐条指令分析上下文切换的过程：swtch:  movl 4(%esp), %eax                  # 第一个参数  movl 8(%esp), %edx                  # 第二个参数这两条指令中，%esp指向旧进程内核栈现在的栈底，因为它调用了swtch，所以（%esp）上存放的是sched中swtch的返回地址。4(%esp)和8(%esp)分别是swtch的第一个和第二个参数，也就是旧进程上下文和新进程（调度器）上下文的指针。画出旧进程内核栈：                   …                  旧进程上下文指针      p-&gt;context              调度器上下文指针      mycpu()-&gt;scheduler              %esp→      mycpu()-&gt;intena = intena 对应指令的地址                            # Save old callee-saved registers  pushl %ebp                      # 保存旧进程内核栈的栈底指针  pushl %ebx                      # 保存旧进程%ebx寄存器  pushl %esi					# 保存旧进程%esi  pushl %edi					#               和%edi寄存器这四条指令是将旧进程的上下文保存到当前栈（旧进程的内核栈）上。                   …                  旧进程上下文指针      &amp;p-&gt;context              调度器上下文指针      mycpu()-&gt;scheduler                     mycpu()-&gt;intena = intena 对应指令的地址                     ebp                     ebx                     esi              %esp→      edi                            # Switch stacks  movl %esp, (%eax)       # 令p-&gt;context = %esp  movl %edx, %esp				然后交换栈。从第一、二条指令我们知道，现在（%eax）中是旧进程的上下文指针，令（%eax）=%esp，也就是让旧进程上下文指针重新指到现在它保存的地方。而%edx中是调度器上下文指针，把%edx赋给%esp，使栈底指针指向了调度器上下文所在的位置，这样，就从旧进程的内核栈切换到了调度器所在的栈（前者是代表用户进程的，是用户进程在内核态下运行时使用的栈，后者不代表任何用户进程，它是内核进程进行时使用的栈）。            (旧进程的内核栈)      …                  旧进程上下文指针      &amp;p-&gt;context              调度器上下文指针      mycpu()-&gt;scheduler              旧进程中swtch返回地址      mycpu()-&gt;intena = intena 对应指令的地址                     ebp                     ebx                     esi              p-&gt;context→      edi                                        （调度器所在的栈）      …                  调度器中swtch返回地址      ret              调度器的上下文      ebp                     ebx                     esi              %esp→      edi      栈切换之后，栈底指针指向调度器上下文所在的地址。现在，就可以从栈上pop出调度器的上下文了：# Load new callee-saved registers  popl %edi  popl %esi  popl %ebx  popl %ebp  ret最后，ret会返回到scheduler中，swtch的下一行位置（而不是返回sched)。这样，cpu控制权就从旧的进程转移到了调度器。从swtch指令中，我们没有看到对%eip的显式保存，这是因为旧进程的%eip在用call swtch调用swtch时就已经隐式地保存在了%ebp的前面，同样，ret指令也隐式地把调度器栈上的返回地址加载到了%eip中。schedulerscheduler就是上述的调度器。每一个进程最终都会将控制权返回到调度器，调度器会从等待队列中选择一个进程开始运行，它会调用swtch保存自己的上下文，然后切换到该进程的上下文开始运行。//PAGEBREAK: 42// Per-CPU process scheduler.// Each CPU calls scheduler() after setting itself up.// Scheduler never returns.  It loops, doing://  - choose a process to run//  - swtch to start running that process//  - eventually that process transfers control//      via swtch back to the scheduler.voidscheduler(void){  struct proc *p;  struct cpu *c = mycpu();  c-&gt;proc = 0;    for(;;){    // Enable interrupts on this processor.    sti();     //每一次从sched进入调度器，都会开启中断    // Loop over process table looking for process to run.    acquire(&amp;ptable.lock);    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;state != RUNNABLE)        continue;      // Switch to chosen process.  It is the process's job	    ........    }    release(&amp;ptable.lock);  }}调度器是一个两层的for循环。外层循环是无限循环，这意味着调度器永远不会返回。内层循环中，调度器遍历进程表，选择进程运行。在CPU开始的时候，它就会调用scheduler.scheduler每一次从内层循环退出，进入外层循环，都要显示地执行sti指令允许中断，并且要将ptable锁释放之后再重新获取。这两个步骤都是很有必要的，因为从内层循环退出，意味着调度器可能检查了一遍ptable，没有找到可以运行的进程，这时有可能所有的进程都在等待IO中断，如果不执行sti开启中断的话，IO中断永远也不能到达进程。而如果一个闲置CPU的调度器一直不释放锁，那么其他CPU也不能访问ptable，进行上下文或者系统调用了，所以就没有CPU能够将一个进程的状态改为RUNNABLE，这个CPU也无法跳出循环。scheduler内层循环遍历进程表，寻找下一个RUNNABLE的进程并切换到进程的上下文中运行。for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;state != RUNNABLE)        continue;      // Switch to chosen process.  It is the process's job      // to release ptable.lock and then reacquire it      // before jumping back to us.      // 找到了一个RUNNABLE的进程      c-&gt;proc = p;      //该cpu上现在运行的进程为*p      switchuvm(p);     // 加载该进程的地址空间      p-&gt;state = RUNNING;   //将进程状态变为RUNNING      swtch(&amp;(c-&gt;scheduler), p-&gt;context);      switchkvm();      // Process is done running for now.      // It should have changed its p-&gt;state before coming back.      c-&gt;proc = 0;    }当调度器找到了一个RUNNABLE的进程，就将cpu-&gt;proc 设置为它的PCB指针，然后调用switchuvm将该进程的内存空间、页表加载进内存中，并将进程状态设置为RUNNING.然后，调用swtch进行上下文的切换。现在，第一个参数（旧的上下文）是调度器的上下文指针的地址&amp;(c-&gt;scheduler), 第二个参数是新进程上下文的指针。这个swtch将保存调度器的上下文，并将c-&gt;scheduler指向保存的位置，然后从调度器的栈换到新进程的内核栈，从栈上加载新进程的上下文，然后转到新进程sched中swtch返回处运行。新进程将马上释放ptable 锁。有的时候，swtch也不一定是返回到新进程的sched处。如果该新进程是刚刚被fork产生，这是它第一次被调度，那么swtch就会返回到forkret处，在forkret中释放ptable锁，再从forkret返回到trapret，退出内核态。当调度器从swtch返回，意味着有某个进程调用了sched把控制权返还给它，它首先调用swtchkvm转换到内核的页表，然后将c-&gt;proc置为0，表示暂时没有进程在该cpu上运行。然后，如果该进程不是位于ptable的最后一个槽，调度器就会继续查找下一个RUNNABLE的进程，重复以上步骤。否则，它将释放ptable锁，并开启中断。睡眠与唤醒睡眠和唤醒提供了进程间通信的机制，它们可以让一个进程暂时休眠，等待某个特定事件的发生，然后当特定事件发生时，另一个进程会唤醒该进程。睡眠与唤醒通常被称为顺序合作或者有条件同步机制。 睡眠是调度发生的另一种情况，当进程调用sleep进入休眠时，它会调用sched把控制权交给调度器。sleep有两个参数，第一个参数chan是休眠被唤醒的信号，这个信号使得进程可以互相通信，一个进程调用sleep(chan)进入休眠，另一个进程用同样的chan调用wakeup(chan)就可以把它唤醒。第二个参数lk是调用休眠时必须持有的一个锁，这个锁主要是为了防止“遗失的唤醒”问题。一般，进程如果需要休眠，它需要循环判断某个条件是否成立（例如磁盘是否已经准备好），如果还不成立，就会调用sleep进入休眠。例如：while(r = something() == 0){    sleep();}之所以需要while循环判断，是因为如果某次事件成立了，进程从休眠中唤醒，但它被唤醒之后可能不是马上就被调度、马上就开始执行后面的代码，所以在这中间，有可能条件又不成立了，所以需要唤醒之后马上继续判断。而如果没有上面所说的lk锁，就可能发生，while判断条件不成立，进程准备进入休眠；但是这时候发生调度，另一个进程使得条件成立，想要唤醒进程，但这时候因为它还没休眠，所以找不到进程可以唤醒。再切换回原来的进程时，这个进程不知道条件已经成立了，它会进入休眠，并且之后再没有办法唤醒它。 因此，必须确保在条件判断和调用sleep是不会被wakeup打断的，即wakeup不可以在进程真正进入sleep之前被调用。这可以用锁来实现。// Atomically release lock and sleep on chan.// Reacquires lock when awakened.voidsleep(void *chan, struct spinlock *lk){struct proc *p = myproc();//调用sleep的进程if(p == 0)panic("sleep");if(lk == 0)panic("sleep without lk");//必须持有lk锁// Must acquire ptable.lock in order to// change p-&gt;state and then call sched.// Once we hold ptable.lock, we can be// guaranteed that we won't miss any wakeup// (wakeup runs with ptable.lock locked),// so it's okay to release lk.// 下面sleep必须修改ptable，把这个进程的状态改为SLEEPING,并把p-&gt;chan也就是休眠的等待事件改为//第一个参数chan，因此要比较传进来的第二个参数lk是不是ptable，如果是ptable，那进程就应该一直持有它，//不应该释放。if(lk != &amp;ptable.lock){ //DOC: sleeplock0acquire(&amp;ptable.lock); //DOC: sleeplock1release(lk);}sleep首先判断是否持有lk锁，否则报错。 然后，sleep需要释放lk锁并获得ptable-&gt;lock。这里有两种情况：  lk不是ptable-&gt;lock，则因为wakeup函数也要求获得ptable-&gt;lock，所以释放lk是没问题的，ptable-&gt;lock代替了lk，保证不会出现“遗失的唤醒”问题。必须在释放lk之前，先获取ptable.lock，否则有可能在释放lk之后，获取ptable.lock之前，发生调度，另一个进程此时便可以调用wakeup。而之所以要获取ptable-&gt;lock是因为之后要对进程表进行访问、修改，并且要调用sched进入调度器。  lk就是ptable-&gt;lock，则不需要任何操作。不可以先释放ptable.lock然后再重新获取，原因跟上面lk释放与ptable.lock获取的顺序不能调换的原因是一样的。然后进程就要进入休眠。// Go to sleep.p-&gt;chan = chan;         //休眠等待信号，wakeup函数中在ptable中查找p-&gt;chan=chan并且状态为休眠的进程去唤醒。p-&gt;state = SLEEPING;    //修改进程状态sched();              //将控制权交给调度器p-&gt;chan = 0;// Reacquire original lock.if(lk != &amp;ptable.lock){ //DOC: sleeplock2release(&amp;ptable.lock);acquire(lk);}调用sched之后，该进程就被暂时挂起了，sched会将上下文切换到调度器，再从调度器切换到另外的进程。当这里的sched返回的时候，表明进程已经被唤醒。它要将等待条件清为0，然后重新获得lk锁。重新获得锁时，则不必先获得lk,再释放ptable.lock，因为此时进程已经从休眠中唤醒了，它不会担心在释放和重新获得之间，其他进程调用wakeup，其他进程是否在这中间调用wakeup对该进程是没有影响的。对应地，wakup函数从ptable中找到状态为SLEEPING并且在chan上休眠的进程（可能多个），将它的状态设置为RUNNABLE，这样它就可以被CPU的调度器调度。// Wake up all processes sleeping on chan.// The ptable lock must be held.static voidwakeup1(void *chan){  struct proc *p;  for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++)    if(p-&gt;state == SLEEPING &amp;&amp; p-&gt;chan == chan)      p-&gt;state = RUNNABLE;}// Wake up all processes sleeping on chan.voidwakeup(void *chan){  acquire(&amp;ptable.lock);  wakeup1(chan);  release(&amp;ptable.lock);}wakeup是wakeup1的加锁版本。wakeup因为要访问并修改ptable，所以需要持有ptable.lock。wakeup作为系统调用让用户去调用时，操作系统而非用户要负责获取ptable.lock锁； 但在系统中，例如exit等函数也会调用wakeup，但在这之前它就已经获得了ptable.lock锁，所以也为内核提供了不加锁的wakeup1版本。waitwait让父进程等待子进程结束，并回收子进程的资源。它返回退出的子进程的pid，如果没有子进程或其他错误情况，则返回-1.// Wait for a child process to exit and return its pid.// Return -1 if this process has no children.intwait(void){	struct proc *p;	int havekids, pid;	struct proc *curproc = myproc();	acquire(&amp;ptable.lock);    //要遍历ptable中的所有进程，找到调用进程的一个子进程    //如果能够找到，需要修改该子进程的状态，因此要对ptable做出修改    //不允许两个进程同时访问ptable，因此要先占有ptable的锁才能对其进行访问和修改。	for(;;){        // Scan through table looking for exited children.        havekids = 0;        for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){        	if(p-&gt;parent != curproc)        		continue;            //找到了该进程的子进程            havekids = 1; //havekids表示调用的进程有子进程            if(p-&gt;state == ZOMBIE){ //有一个子进程调用了exit或因其他原因退出，处于ZOMBIE状态                // Found one.                pid = p-&gt;pid; //准备返回该子进程的pid                kfree(p-&gt;kstack); //释放子进程占用的内存空间，这里是释放子进程的内核栈                p-&gt;kstack = 0; //将进程内核栈底指针重置为0                freevm(p-&gt;pgdir); //释放页表，释放所有用户空间所占据的物理页框                p-&gt;pid = 0; //接下来是将PCB全都重置为0                p-&gt;parent = 0;                p-&gt;name[0] = 0;                p-&gt;killed = 0;                p-&gt;state = UNUSED; //该PCB状态变为UNUSED，之后调用allocproc的时候，这块PCB可能就会被分配给一个新的进程                release(&amp;ptable.lock); //释放ptable锁                return pid; //返回退出的子进程的pid   	 		}		}        // ptable的遍历已经完成，到这里没有返回，说明没有子进程处于ZOMBIE        // No point waiting if we don't have any children.        if(!havekids || curproc-&gt;killed){ //如果该进程并没有子进程，或者该进程在wait的过程中被杀死（比如用ctrl+C等），那就不需要等待了，直接释放ptable锁，返回-1.        	release(&amp;ptable.lock);        	return -1;		}		//否则，该进程有子进程， 但所有的子进程都还在运行，必须让该进程进入休眠，等待一个子进程的结束		// Wait for children to exit. (See wakeup1 call in proc_exit.)		sleep(curproc, &amp;ptable.lock);		//调用sleep进入休眠。    }}wait()函数等待它的一个子进程终止。在exit()函数中，我们看到exit()只是关闭了进程打开的文件和从目录中退出，但仍然保留了进程的信息，进程占据的内存和PCB都没有被释放，只是处于ZOMBIE状态。进程信息和内存空间的释放由wait()来完成。wait遍历过一次进程表，发现有子进程，但是都还没结束，那么它就调用sleep(curproc, &amp;ptable.lock)进入休眠。这个休眠会被子进程在exit中，调用wakeup(myproc()-&gt;parent)来唤醒。Part 2 是如何在xv6中实现Round Robin调度情况统计，统计用户进程的周转、等待、执行时间。下一篇： Part 2]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> xv6 </tag>
        
          <tag> scheduling </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OS操作系统实验：xv6调度(RR, 优先级调度, 优先级队列） 实现和分析 Part 3- xv6 优先级调度算法实现]]></title>
      <url>/lab%20report/2020/01/31/OS4-3/</url>
      <content type="text"><![CDATA[上一篇： Part 2 RR调度周转、等待时间等的统计第一篇： Part 1 xv6调度代码讲解TODO3:  实现优先级调度算法基于优先级的调度算法是一类算法，基本思想是为每一个进程赋予不同的优先级，在调度时优先选择优先级最高的进程来调度。优先级调度有许多种，大致可以分为：      非抢占式的优先级调度：    非抢占式的优先级调度算法中，每个进程只在初始时赋予一个优先级，在运行过程中不会被改变。每一次重新调度时，选择最高优先级的一个进程，将它运行完毕，然后再选择另外一个进程开始运行。一个进程在运行过程中如果有更高优先级的进程到来，它不会被打断，只是将该进程放到等待队列的队首。    xv6是开启抢占的，进程会被时钟中断打断，然后调度器选择另外一个进程（如果有的话）来代替它。        抢占式优先级调度：    抢占式优先级调度中，如果进程运行过程有更高优先级的进程到来，当它运行完这个时间片，就会被抢占。进程被赋予一个初始优先级，但这个优先级是可变的。具体的实现也有多种：                  静态优先级调度：        静态优先级调度指的是进程最初有一个优先级，运行过程中可以通过系统调用改变这个优先级，但是不会随着等待时间或运行时间的增加而自动改变。                    动态优先级调度：        进程最初被指定一个优先级，同样也可以通过系统调用改变。为了惩罚执行时间较长的进程，优先级会随着运行时间增加而逐渐降低。                    多级反馈队列调度：        在系统中设置每个优先级对应的等待队列，进程初始化时进入某个队列。调度时，首先从最高优先级队列中找进程，只有当更高优先级队列为空时，才会调度某个低优先级队列中的进程。 同一个优先级的进程按FCFS调度。        多级反馈队列也有不同的实现。静态实现时，进程优先级不在运行过程中自动改变；动态实现时，可以让进程每运行完k个时间片，就下降到低一级的等待队列，一段时间后，再将进程提高到最高优先级。这样，既可以充分考虑IO-bound、CPU-bound进程的不同特性，又考虑到进程运行过程中IO-bound到CPU-bound的动态转变，减少进程饥饿的发生。                    我将实现静态优先级调度和静态的多级反馈队列调度。静态优先级调度静态优先级调度是为每个进程赋予一个初始优先级，用户可以指定优先级。在每次重新调度时，遍历整个进程表，找到状态为RUNNABLE且优先级最高的进程来运行。优先级的范围是[1,4], 数字越低代表优先级越高。首先为进程添加priority变量：// Per-process statestruct proc {  uint sz;                     // Size of process memory (bytes)  pde_t* pgdir;                // Page table  char *kstack;                // Bottom of kernel stack for this process  .....  int priority;                // 进程的优先级, 取值范围是1-4  uint ctime;                   // 创建时间  ...优先级调度的一个问题是，如何决定进程初始化时的优先级。初始化时的优先级也是系统进程的优先级，这个优先级不能太高，否则会让用户进程响应和等待的时间过长；但也不能太低，否则当用户需要某个内核进程的服务时，等待时间也会太长。这里，先将初始优先级设置为2. 在allocproc()中，进程初始化时：found:  p-&gt;state = EMBRYO;  p-&gt;pid = nextpid++;														//modified here  p-&gt;ctime = ticks;  p-&gt;rutime = 0;  p-&gt;sltime = 0;  p-&gt;retime = 0;														//priority modified  p-&gt;priority = 2;  release(&amp;ptable.lock);新的scheduler函数，在内层for循环中，再遍历整个进程表，用phigh保存目前为止优先级最高的进程。voidscheduler(void){  struct proc *p;  struct cpu *c = mycpu();  c-&gt;proc = 0;    for(;;){    // Enable interrupts on this processor.    sti();    // Loop over process table looking for process to run.    acquire(&amp;ptable.lock);														//PRIORITY modified    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      #if defined RR      if(p-&gt;state != RUNNABLE)            // 原来的 Round Robin 调度        continue;      #elif defined PRIORITY             // 静态优先级调度      struct proc *pnow;                 // 用来遍历ptable的指针      struct proc *phigh=0;              // 遍历过程中保存最高优先级的进程      if(p-&gt;state!= RUNNABLE)         continue;            phigh = p;           // 遍历整个表之前，phigh首先是下一个RUNNABLE的进程。      for( pnow = ptable.proc; pnow&lt;&amp;ptable.proc[NPROC]; pnow++){        if(pnow-&gt;state!=RUNNABLE)     // 遍历表中所有RUNNABLE的进程          continue;         if(pnow-&gt;priority &lt; phigh-&gt;priority){   //如果有一个RUNNABLE的进程优先级比phigh高	      phigh = pnow;							//更新phigh	    }      }      p = phigh;                    //令下一个执行的进程为phigh      #endif            // Switch to chosen process.  It is the process's job      // to release ptable.lock and then reacquire it      // before jumping back to us.      if(p!=0){         cprintf("%d is being scheduled on cpu: %d, its priority is %d.\n", p-&gt;pid, c-&gt;apicid, p-&gt;priority);	 	c-&gt;proc = p;        switchuvm(p);	 	p-&gt;state = RUNNING;	 	swtch(&amp;(c-&gt;scheduler), p-&gt;context);        switchkvm();	 // Process is done running for now.	 // It should have changed its p-&gt;state before coming back.	 	c-&gt;proc = 0;      }    }    release(&amp;ptable.lock);  }}调度器的内层循环用指针p遍历整个进程表，本来每一次找到一个RUNNABLE进程，就切换到该进程。 用priority算法，每次调度一个进程后返回调度器时，p指向下一个RUNNABLE进程，phigh首先指向这个进程，然后遍历整个进程表，如果有更高级的进程，phigh会指向接下来第一个优先级最高的进程；如果没有，则下一个进程就是表中下一个可执行进程。实现系统调用set_priority：intset_priority(int pid, int priority){    struct proc *p=0;    acquire(&amp;ptable.lock);    for(p=ptable.proc; p&lt; &amp;ptable.proc[NPROC]; p++){       if(p-&gt;pid == pid){           p-&gt;priority=priority;           release(&amp;ptable.lock);           return 0;       }              }    // 找不到该pid，错误    release(&amp;ptable.lock);    return -1;}把它封装好之后，修改RRsta.c， 加入set_priority：int stdout=1;int main(int argc, char* argv[]){    if(argc!=2){        printf(1, "usage: priority_sta &lt;fork number&gt; \n");	exit();    }    int forknumber=atoi(argv[1]);    int status;    for(int i=0;i&lt;forknumber;i++)    {        status=fork();        if(status)		{	    	set_priority(status, i%4+1);   //在父进程中，将每个子进程的优先级设置为i%4+1.        }        if(status==0){	    	for(int count=0;count&lt;40; count++){				printf(1," ");		    	for(int k=0;k&lt;1000000;k++){					int result=0;					for( int j=0; j&lt;5000; j++){			    		result+=j*j;                        result-=j*j;                        result= result/j;                        result=result*j;                        result = result+result/j;					}		   		}            }                        exit();        }    }    int rutime;    int retime;    int sltime;    int pid;    int sum=0;    while((pid = waitSch(&amp;rutime,&amp;retime,&amp;sltime))!=-1){        sum+= rutime+retime+sltime;    }    printf(1, " average turn-around time of %d process is %d\n", forknumber, sum/forknumber);    exit();}在内核中，添加一些额外的输出来验证算法实现的正确性。重新编译内核，然后执行priority_sta.c, fork 4个进程：$ priority_sta 4pid为3的进程是父进程，子进程的pid分别为4， 5， 6， 7，创建后set_priority, 使它们的优先级分别为1， 2， 3， 4；进程的初始优先级被设置为2.当进程4被创建，并设置优先级为1之后，它是系统中优先级最高的进程，正确情况下会一直运行直到结束（除非它进入休眠，或者有同样优先级为1的进程到来）；因为有两个cpu，进程3可以在另一个cpu上调度，继续创建进程5，6. 这时，进程4已经结束，系统中有3，5， 6三个进程，但正确情况下只有3和5可以被调度，6必须等待直到3和5不是RUNNING或RUNNABLE状态。从这段程序的输出来看，目前的调度都是正确的。进程6一直没有被调度，直到进程3进入休眠：进程3创建了进程7（最后一个子进程）之后，它就进入休眠等待所有子进程结束。这时进程6被第一次调度，而直到进程5结束，系统只剩下3，6，7时，在进程3休眠时进程7才可被调度。统计所有进程的运行情况：优先级越高的进程平均周转时间越长。从图中可以看出，进程的运行时间相差不大，而优先级最低的进程（7，11）明显等待时间比其他进程要长很多（11）。但从单个进程来看，不一定优先级越低进程周转时间越长，例如进程10（优先级为3）的周转时间就比进程11（优先级为4）长很多，这是因为进程11到达更晚，且它到达时系统中更高优先级的进程几乎已经都执行结束。静态多级反馈队列静态多级反馈其实跟上面的静态优先级调度类似，但是它使用了多级队列的方法，使优先级相同的进程能够按照FCFS来调度。静态优先级调度则不一定是先来先服务的，因为它是遍历整个进程表找最高优先级的进程，这样，当pid序号在前的进程退出被回收之后，前面就会出现空槽，更新创建的进程反而会比老的进程pid序号更低，更先被调度。实现多级反馈队列，首先要为每个优先级定义对应的等待队列。等待队列由一个proc结构体的指针数组来表示，考虑到进队和出队的动作一般发生在进程的优先级发生改变或者状态改变的时候，都会持有ptable.lock,则访问等待队列已经可以保证互斥。因此可以省略等待队列的锁。同时，用变量size来表示等待队列目前的大小，也就是最后一个进程的下标。#if defined SMLtypedef struct queue_t{  int size;  struct proc* proc[NPROC];  int priority;    //for debug}queue_t;struct queue_t pqueue_1;   //4个优先级队列struct queue_t pqueue_2;struct queue_t pqueue_3;struct queue_t pqueue_4;#endif内核开始时，要初始化4条优先级队列，定义一个初始化函数来完成：voidpqueue_init(struct queue_t *queue, int priority){   queue-&gt;priority=priority;     //为方便输出debug，加入变量priority   queue-&gt;size=0;   for(int i=0; i&lt;NPROC; i++)    //初始时指针为0      queue-&gt;proc[i]=0;}voidpinit(void){  initlock(&amp;ptable.lock, "ptable");				  #if defined SML									//sml modified  pqueue_init(&amp;pqueue_1,1);  pqueue_init(&amp;pqueue_2,2);  pqueue_init(&amp;pqueue_3,3);  pqueue_init(&amp;pqueue_4,4);  #endif}等待队列涉及三种操作，即入队，删除队列中的一个进程，以及取等待队列中最早到达的一个RUNNABLE的进程。由于进程的优先级在运行过程中可能动态变化，所以可能要将一个进程从某个队列删除后添加到另一个队列中，这时，为了保持队列从到达时间早到晚排序，要遍历队列，找到该进程的插入位置。从最后（q-&gt;size-1)往前扫描，如果当前指针进程的到达时间ctime比新进程p-&gt;ctime大（晚），就把它往后挪。直到指向一个进程，到达时间&lt;=p-&gt;ctime，然后把p插入到它的后面。最后，要将q-&gt;size增加一。void enqueue(struct queue_t *q, struct proc *p){    cprintf("proc %d enqueueing %d.\n", p-&gt;pid, q-&gt;priority);    int pos;    pos = q-&gt;size - 1;    while(pos &gt;= 0 &amp;&amp; q-&gt;proc[pos]-&gt;ctime &gt; p-&gt;ctime){   //从后往前扫描        q-&gt;proc[pos+1] = q-&gt;proc[pos];    //如果目前进程比新插入进程到达晚，就把它向后挪一个位置		pos--;    }    // 目前pos指向的进程比插入的进程到达时间早或相同。    q-&gt;proc[pos+1] = p;       q-&gt;size ++;   }删除队列的操作，是从前往后遍历该等待队列，找到要删除的进程，如果能找到，则将从该位置开始后面的进程都往前挪一个位置，最后一个位置变为0，最后把size减1.voidremoveq(struct queue_t* q, struct proc *p){   cprintf("removing %d from q: %d\n",p-&gt;pid, q-&gt;priority);   int pos;   int found = 0;   pos = 0;   while(pos &lt; q-&gt;size){      if(q-&gt;proc[pos]==p){   //从前往后，找到该进程          found=1;          break;      }      else{        pos++;      }   }   if(found){   //若找到，此时pos指向要删除的进程      while(pos &lt; q-&gt;size-1){    //从 pos 到倒数第二个进程位置            q-&gt;proc[pos] = q-&gt;proc[pos+1];  //将所有进程往前挪一个位置            pos++;      }      q-&gt;proc[q-&gt;size-1] = 0;      q-&gt;size --;    }   }查找等待队列中第一个RUNNABLE的进程，只要从前往后遍历，找到第一个RUNNABLE的进程就返回即可。如果未找到，则返回0.struct proc*headq(struct queue_t* q){    struct proc* p = 0;    int pos;    for(pos = 0; pos &lt; q-&gt;size; pos++){      if(q-&gt;proc[pos]-&gt;state == RUNNABLE){    //找到第一个RUNNABLE进程，注意等待队列是从到达时间小到大排序的        p = q-&gt;proc[pos];        break;      }    }    return p;}接下来要就是要实现进程在执行过程中状态或优先级变化时相应的等待队列的操作。由于等待队列代价比较高，所以尽量在确定进程已经正确分配好各种资源，才将它加入到初始优先级队列中去。 在userinit中，设置默认优先级为2：  #if defined SML														//sml modified  p-&gt;priority=2;  enqueue(&amp;pqueue_2, p);  #endif在fork()中，让子进程继承父进程的优先级（默认为2），然后进入相应的等待队列：  if((np-&gt;pgdir = copyuvm(curproc-&gt;pgdir, curproc-&gt;sz)) == 0){    kfree(np-&gt;kstack);    np-&gt;kstack = 0;    np-&gt;state = UNUSED;									    return -1;  }  np-&gt;sz = curproc-&gt;sz;  np-&gt;parent = curproc;  *np-&gt;tf = *curproc-&gt;tf;														//sml modified  np-&gt;priority = curproc-&gt;priority;  #if defined SML  switch(np-&gt;priority){     //进入相应的优先级队列     case 1: enqueue(&amp;pqueue_1, np); break;     case 2: enqueue(&amp;pqueue_2, np); break;     case 3: enqueue(&amp;pqueue_3, np); break;     case 4: enqueue(&amp;pqueue_4, np); break;     default: panic("priority must be between 1-4.");  }       #endif在进程终止，被父进程回收资源时，要将它们从相应的等待队列中删除，在wait和waitSch中作同样的改变：intwait(void){  struct proc *p;  int havekids, pid;  struct proc *curproc = myproc();    acquire(&amp;ptable.lock);  for(;;){    // Scan through table looking for exited children.    havekids = 0;    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;parent != curproc)        continue;      havekids = 1;      if(p-&gt;state == ZOMBIE){        // Found one.        pid = p-&gt;pid;        kfree(p-&gt;kstack);        p-&gt;kstack = 0;        freevm(p-&gt;pgdir);                  												//SML modified        #if defined SML        switch(p-&gt;priority){                    //删除等待队列中的进程			case 1: removeq(&amp;pqueue_1, p); break;	     	case 2: removeq(&amp;pqueue_2, p); break;	     	case 3: removeq(&amp;pqueue_3, p); break;	     	case 4: removeq(&amp;pqueue_4, p); break;	     	default: ;		}		#endif	        p-&gt;pid = 0;        p-&gt;parent = 0;        p-&gt;name[0] = 0;        p-&gt;killed = 0;        												// modified        p-&gt;ctime=0;        p-&gt;rutime=0;        p-&gt;retime=0;        p-&gt;sltime=0;        												// end        p-&gt;state = UNUSED;        release(&amp;ptable.lock);调度器的算法则跟之前较为不同。RR和静态优先级调度scheduler都是内循环遍历整个表，每次寻找最高优先级或第一个RUNNABLE进程进行调度，现在，静态多级反馈队列不用遍历整个表，它从最高优先级队列开始找起，如果某个队列里面有RUNNABLE的进程，就运行它，否则就往下一个优先级队列继续寻找：    #if defined RR ||PRIORITY    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      #if defined RR      if(p-&gt;state != RUNNABLE)        continue;      #elif defined PRIORITY      struct proc *pnow;      struct proc *phigh=0;      if(p-&gt;state!= RUNNABLE)         continue;            phigh = p;      for( pnow = ptable.proc; pnow&lt;&amp;ptable.proc[NPROC]; pnow++){        if(pnow-&gt;state!=RUNNABLE)          continue;        if(pnow-&gt;priority &lt; phigh-&gt;priority){	  phigh = pnow;	}      }      p = phigh;      #endif              // Switch to chosen process.  It is the process's job      // to release ptable.lock and then reacquire it      // before jumping back to us.      if(p!=0){         cprintf("%d is being scheduled on cpu: %d, its priority is %d.\n", p-&gt;pid, c-&gt;apicid, p-&gt;priority);	 c-&gt;proc = p;         switchuvm(p);	 p-&gt;state = RUNNING;	 swtch(&amp;(c-&gt;scheduler), p-&gt;context);         switchkvm();	 // Process is done running for now.	 // It should have changed its p-&gt;state before coming back.	 c-&gt;proc = 0;      }    }    #endif                                      //=====================modified begin ================//    #if defined SML                            //多级反馈队列从这里开始    p = headq(&amp;pqueue_1);    if(p==0)      p=headq(&amp;pqueue_2);    if(p==0)      p=headq(&amp;pqueue_3);    if(p==0)      p=headq(&amp;pqueue_4);    if(p){                          //若找得到进程，则运行它，否则暂时释放ptable锁，开启中断，然后再次获取ptable锁，继续从第一优先级队列开始寻找         cprintf("%d is being scheduled on cpu: %d, its priority is %d.\n", p-&gt;pid, c-&gt;apicid, p-&gt;priority);	     c-&gt;proc = p;         switchuvm(p);	     p-&gt;state = RUNNING;	     swtch(&amp;(c-&gt;scheduler), p-&gt;context);         switchkvm();	 // Process is done running for now.	 // It should have changed its p-&gt;state before coming back.	     c-&gt;proc = 0;     }    #endif  当调用set_priority时，有可能进程的优先级改变，则要将它从旧的优先级队列中删除并放到新的优先级队列中：intset_priority(int pid, int priority){    struct proc *p=0;    acquire(&amp;ptable.lock);    for(p=ptable.proc; p&lt; &amp;ptable.proc[NPROC]; p++){       if(p-&gt;pid == pid){                  #if defined SML           if(p-&gt;priority!=priority){           	switch(p-&gt;priority){           	    case 1: removeq(&amp;pqueue_1, p);   break;           	    case 2: removeq(&amp;pqueue_2, p);   break;           	    case 3: removeq(&amp;pqueue_3, p);   break;           	    case 4: removeq(&amp;pqueue_4, p);   break;           	    default: ;           	}           	switch(priority){		       case 1: enqueue(&amp;pqueue_1, p);   break;		       case 2: enqueue(&amp;pqueue_2, p);   break;		       case 3: enqueue(&amp;pqueue_3, p);   break;		       case 4: enqueue(&amp;pqueue_4, p);   break;		       default: ;		}           }           #endif                                 p-&gt;priority=priority;               release(&amp;ptable.lock);           return 0;       }              }    release(&amp;ptable.lock);    return -1;}多级反馈队列实现已经完成。将它编译，执行priority_sta.c，还是将子进程数目设置为4，得到如下输出：进程3是父进程，4，5，6，7分别是4个子进程，优先级被设置为1，2，3，4. 可以看到，每个进程在fork完成之前，都先进入了默认的第二优先级队列。当调用set_priority时，每个进程都从原来的queue 2离开，进入到相应新的队列中（进程5除外，进程5本来就在queue 2中）。当所有进程都创建好之后，因为3比5先到达，则只有进程3和4可以被调度；当进程3在倒数第二行进入休眠之后，进程5才可以得到调度。然后，等进程4退出并被回收了，它从queue 1中被删除。当进程3进入休眠的时候，进程5和6可以得到调度，在这整个过程中，进程7的优先级都不足，无法被调度。而当进程6进入休眠时，进程7才和5一起被调度。说明算法逻辑正确。统计多级反馈下程序运行时间：这个结果是符合预期的，如果程序执行的内容都一样，优先级越低，周转时间应该越长，因为更高优先级的进程应该更早被调度。简单比较比较所有进程的平均轮转时间：            进程个数      RR      静态优先级      多级反馈队列                  4      6      4      4              10      8      4      7              30      15      10      17      （轮转时间的计算跟机器状态也有关系，有些时候机器运转较快，则用户程序的运行和调度算法都比较快，测得时间就会少）如果忽略机器状况的因素，可以发现RR算法的平均轮转时间在进程个数一定时是多于静态优先级调度的。这是因为所有进程轮流调度，本来可以更早结束的进程时间被延长了；而静态优先级调度虽然低优先级的进程周转时间比平均长很多，但优先级高的进程周转时间也对应地低很多，并且如果先来先服务，高优先级进程基本上可以到达之后就一直进行直到结束，所以平均周转时间可能会更好。但也要考虑算法的花销。在进程个数很多的时候，多级反馈队列每一次插入和删除的开销会快速增长，这导致进程的等待时间变得非常长。为了缓解饥饿的问题，可以对优先级调度算法进行优化，采用动态的多级反馈队列：      进程刚开始时进入最高优先级队列。    不同的优先级队列中，进程有不同的时间片。优先级越高，时间片越短。这样，低优先级进程可以有更多机会完成任务。  每运行完一个/k个时间片，进程就下降优先级，最低优先级的进程反而重新回到最高优先级这样做的好处是，进程刚开始创建优先级最高，可以有更快的响应时间；优先级随着进程运行下降，则可以偏向短作业，让短作业更快结束，从而缩短等待时间；每隔一段时间就将最低优先级的进程提高到高优先级，这样可以防止饥饿发生。但是，动态多级反馈队列显然需要更多的算法花销，不同时间片的设计更为复杂，且需要设计每个优先级适当的时间片大小，以及按进程特性决定创建时进入哪个队列。即便如此，动态多级反馈队列仍然是一种比较优的算法。]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> xv6 </tag>
        
          <tag> scheduling </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OS操作系统实验：xv6调度(RR, 优先级调度, 优先级队列） 实现和分析 Part 2- xv6 RR调度情况统计]]></title>
      <url>/lab%20report/2020/01/31/OS4-2/</url>
      <content type="text"><![CDATA[上一篇： Part 1-xv6 调度代码讲解TODO2： 统计RR调度情况实现waitSch系统调用todo2需要增加一个系统调用waitSch(int* rutime, int* retime, int* sltime)作为原来wait的扩展，除了执行原有功能以外，要将进程的运行时间、在等待队列中的时间和休眠时间输入到三个参数所代表的地址中。首先我们将需要维护的数据结构定义在proc.h中：struct proc {  uint sz;                     // Size of process memory (bytes)  pde_t* pgdir;                // Page table  char *kstack;                // Bottom of kernel stack for this process  ...    uint ctime;                   // 创建时间  uint rutime;                  // 处于RUNNING下的时间  uint retime;                  // 处于ready状态下的时间  uint sltime;                  // 处于Sleeping状态下的时间};然后，在proc.c中，也应该做出相关的修改。首先在进程被初始化的时候，应该将p-&gt;ctime设置为当时的时钟ticks，然后其他的几个变量应该初始化为0。考虑到第一个用户进程并不是由fork产生，我们将初始化放在allocproc中：found:  p-&gt;state = EMBRYO;  p-&gt;pid = nextpid++;														//modified here  p-&gt;ctime = ticks;   //创建时间  p-&gt;rutime = 0;        p-&gt;sltime = 0;  p-&gt;retime = 0;  release(&amp;ptable.lock);  // Allocate kernel stack.  if((p-&gt;kstack = kalloc()) == 0){    p-&gt;state = UNUSED;    p-&gt;ctime = 0;     //分配错误的时候，要恢复为0.    return 0;  }相应地，当进程结束，资源要被回收时，应该将这几个变量清空。我们不修改在exit中，因为此时这些进程运行的统计数据仍然要保留，直到父进程调用wait找到它，将它回收的时候，再把这些数据处理完清空。在wait中： if(p-&gt;state == ZOMBIE){        // Found one.        pid = p-&gt;pid;        kfree(p-&gt;kstack);        p-&gt;kstack = 0;        freevm(p-&gt;pgdir);        p-&gt;pid = 0;        p-&gt;parent = 0;        p-&gt;name[0] = 0;        p-&gt;killed = 0;        												// modified        p-&gt;ctime=0;	    p-&gt;rutime=0;	    p-&gt;retime=0;	    p-&gt;sltime=0;        												// end        p-&gt;state = UNUSED;        release(&amp;ptable.lock);        return pid;接下来是实现waitSch，waitSch的大部分逻辑都和wait一样，不同的只是要先把p-&gt;rutime赋给*rutime，把p-&gt;retime赋给*retime, 以及p-&gt;sltime→*sltime，然后再把这些变量清空:int waitSH(int* rutime, int* retime, int* sltime){  struct proc *p;  int havekids, pid;  struct proc *curproc = myproc();    acquire(&amp;ptable.lock);  for(;;){    // Scan through table looking for exited children.    havekids = 0;    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;parent != curproc)        continue;      havekids = 1;      if(p-&gt;state == ZOMBIE){        // Found one.        pid = p-&gt;pid;        kfree(p-&gt;kstack);        p-&gt;kstack = 0;        freevm(p-&gt;pgdir);        p-&gt;pid = 0;        p-&gt;parent = 0;        p-&gt;name[0] = 0;        p-&gt;killed = 0;        																// modified	    *rutime = p-&gt;rutime;	    *sltime = p-&gt;sltime;	    *retime = p-&gt;retime;	    p-&gt;ctime=0;	    p-&gt;rutime=0;	    p-&gt;retime=0;	    p-&gt;sltime=0;        																// end        p-&gt;state = UNUSED;        release(&amp;ptable.lock);        return pid;      }    }    // No point waiting if we don't have any children.    if(!havekids || curproc-&gt;killed){      release(&amp;ptable.lock);      return -1;    }    // Wait for children to exit.  (See wakeup1 call in proc_exit.)    sleep(curproc, &amp;ptable.lock);  //DOC: wait-sleep  }} 现在的问题是，如何动态地维护进程的运行时间、等待时间和休眠时间呢？首先一个直观的思路是，在每一次进程状态变化的时候，例如由A→B，记录下这次状态转换的时间作为B状态的开始时间$t_{0}$, 等下一次再从B→C的时候，B的持续时间就加上这个时刻减去$t_{0}$. 但是这种方法实现起来非常复杂，首先进程状态转换可能在很多种情况下发生，每一次转换时，都要判断旧的状态和新的状态是什么，然后更新旧状态的持续时间，保存新状态的开始时间。这样代码会变得冗长而且容易出错，还需要为进程维护两倍的变量（对每一种状态，例如RUNNING, 至少需要维护最近一次RUNNING状态开始的时刻，以及累计运行时间）。另一种思路是，在每一次收到时钟中断时，判断进程处于何种状态，为这种状态的持续时间加一。这样的计算是一种很粗糙的近似。这样计算，是假设进程会将这种状态维持一个时间片，在中间不发生状态的变换，并假设这种状态大概在时钟中断发生时开始。然而，在两个时钟中断之间，进程是很有可能调用sleep或wait进行休眠的，也有可能在下一个时钟中断到来之前，调度器就已经调度了另一个进程并将该进程唤醒，状态变为RUNNABLE，这样，中间的sleep阶段就没有被我们的计算捕捉到。不过因为进程一般是运行和等待的时间占大多数，sleep占比很少，运行和等待之间的切换又一般是通过时钟中断引发的yield, 所以这种近似还是可以接受的。xv6 的时钟机制xv6的时钟在timer.c中实现，每过100ms，硬件就会产生一个时钟中断。每个cpu都可以独立地接收时钟中断，并陷入中断处理。在trap.c中定义了一个uint类型的变量ticks，每一次这个变量加一，代表系统的时钟往前走了一步。两个cpu收到时钟中断后，先后进入中断处理程序，为了让系统的时钟(ticks)能够真正在每次timer产生时钟中断的时候自增一，只在某一个固定的cpu收到中断时让ticks++, 因此xv6中每一次CPU0收到一个时钟中断，ticks就自增1：switch(tf-&gt;trapno){  case T_IRQ0 + IRQ_TIMER:     //收到了时钟中断    if(cpuid() == 0){          //如果是cpu0收到了这个时钟中断，才将tick增加1；这样对于不同的cpu，时钟都是保持同步的，所有的cpu都会共用这个tick变量。      acquire(&amp;tickslock);             ticks++;      wakeup(&amp;ticks);      release(&amp;tickslock);    }    lapiceoi();    cprintf("ashajh,time: %d, cpu: %d\n",ticks, cpuid());    break;编译这段代码，输出是：这段输出中，每一次cpu id 为0时，对应的时钟比上一次printf增加一，而对于cpu1则不一定是这样。如果我们要在每次时钟中断时判断进程的状态，也有两种思路。第一个是，在时钟中断的时候，调用myproc先判断cpu上是否有进程在运行，如果有，则判断它的状态并更新变量。这种思路首先因为进程会在两个cpu上调度，而在cpu1上调度时，两次中断之间ticks不一定有增加，所以要增加一个lasttick保存上一次中断时ticks的值，如果这个值有改变，才会进行判断。但即使是添加了这个逻辑，这种方法也是错误的，因为如果myproc()能返回进程的PCB， 进程一定是处于RUNNING状态（否则它就不会被cpu调度了），这样我们无法知道进程何时在休眠或等待。第二个思路是，在ticks++的时候，遍历整一个ptable表，对每个进程判断它是在何种状态，然后给变量+1.这样便可实现上面那种近似的算法。 在trap.c中作以下修改：  switch(tf-&gt;trapno){  case T_IRQ0 + IRQ_TIMER:    if(cpuid() == 0){      acquire(&amp;tickslock);      ticks++;      									//modified      update();                         //每一次ticks++时，更新进程状态变量      wakeup(&amp;ticks);      release(&amp;tickslock);    }    lapiceoi();    break;其中，判断进程状态并更新变量的函数在proc.c中实现：void update(){    struct proc *p;    acquire(&amp;ptable.lock);    //对进程变量进行修改，要先获得ptable锁。    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){    switch(p-&gt;state) {   //遍历整个表，判断进程状态      case RUNNING:      //运行中        p-&gt;rutime++;     //则运行时间增加一个时间片        break;      case SLEEPING:    //休眠中        p-&gt;sltime++;     //休眠时间增加一个时间片        break;      case RUNNABLE:    //等待中        p-&gt;retime++;    //等待时间增加一个时间片        break;      default: ;    // default，有ZOMBIE\EMBRYO\UNUSED，这些状态我们不关心    }  }  release(&amp;ptable.lock);  //释放锁}    我们按照上次实验的步骤将waitSch包装成系统调用。 实现过程中比较重要的是，xv6syscall.c中系统调用函数的统一格式是返回值int, 不允许带参数。如果需要加入参数，则参数会被压入栈中，通过argint和argptr从栈上取出参数进行解析。在sysproc.c中包装waitSch如下：intsys_waitsch(void){    int *rutime;    int *retime;     int *sltime;    int pid;    rutime=retime=sltime=0;    if(argptr(0, (char**)&amp;rutime, sizeof(int)) &lt; 0)        return -1;    if(argptr(1, (char**)&amp;retime, sizeof(int)) &lt; 0)        return -1;    if(argptr(2, (char**)&amp;sltime, sizeof(int)) &lt; 0)        return -1;    pid = waitSH(rutime, retime, sltime);    if(pid!=-1)    //如果有子进程退出，打印出它的pid，运行时间、等待时间和休眠时间。        cprintf("pid: %d, runtime:%d, ready time: %d, sleep time %d\n", pid, *rutime, *retime, *sltime);    return pid;} 在syscall.h以及系统调用表中，为waitSch增加系统调用号，并在user.h中定义接口。waitSch的实现已经完成。编写RRsta.c 统计RR调度情况编写RRsta.c， main函数接收一个命令行参数n, fork n个子进程进行相同的大规模计算，然后父进程调用waitSch，等待每个子进程的结束，并输出每个子进程的运行时间、等待时间和休眠时间。最后统计n个进程在RR（Round Robin）调度下的平均轮转时间。#include "param.h"#include "types.h"#include "stat.h"#include "user.h"#include "fs.h"#include "fcntl.h"#include "syscall.h"#include "traps.h"#include "memlayout.h"int stdout=1;int main(int argc, char* argv[]){    int forknumber=atoi(argv[1]);   //命令行参数，为fork 子进程的个数    int status;    for(int i=0;i&lt;forknumber;i++)        {        status=fork();        if(status==0){    //在子进程内status=pid=0            for(int count=0;count&lt;40; count++){  //执行相同的运算，主要是大规模乘、除法计算。                printf(1," ");                for(int k=0;k&lt;1000000;k++){                    int result=0;                    for( int j=0; j&lt;5000; j++){                        result+=j*j;                        result-=j*j;                        result= result/j;                        result=result*j;                        result = result+result/j;                    }		    	}            }                        exit();   //一定要注意exit，否则子进程也会执行上面的fork循环。        }    }    int rutime;   //运行时间    int retime;    int sltime;    int pid;      //子进程pid    int sum=0; //周转时间总和    while((pid = waitSch(&amp;rutime,&amp;retime,&amp;sltime))!=-1){        sum += rutime+retime+sltime;    }    printf(1,"average turn-around time of %d process is %d.\n", forknumber, sum/forknumber);    exit();}将RRsta.c编译到qemu中，在shell中运行make cleanmakemake qemu$ RRsta 10得到输出：（实际上，手动计算平均周转时间的话会得到5.8， 但是xv6的printf并没有使用C的标准输出库，不支持输出浮点数，只能将小数省去了。下一篇： Part 3： 优先级和动态多及反馈队列的实现、调度算法比较]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> xv6 </tag>
        
          <tag> scheduling </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[论文笔记]基于个性化注意力机制的新闻推荐]]></title>
      <url>/paper%20reading/2020/01/30/NPA/</url>
      <content type="text"><![CDATA[基于个性化注意力机制的新闻推荐论文链接：https://arxiv.org/pdf/1907.05559.pdf这篇论文是2019年SIGKDD的7篇精选论文之一。主要解决的是新闻推荐中个性化的问题。Introduction首先介绍了为什么需要新闻推荐（减少信息过载，高效获取信息）新闻推荐有什么样的难题（可以归结为如何表示新闻，以及如何表示用户兴趣）：这篇论文的作者之前已经做过一项工作，就是多视角学习的新闻推荐，所谓多视角，就是用标题、内容、类别和子类别分别去表示一篇新闻，结果是类别和子类别获得了最高的attention权重。但是在这一篇论文中，作者选择了用标题来表示新闻，原因可能是，相比内容标题更短代价更小，但是相比类别又具有更多潜在信息。但是，一个新闻标题中，并不是所有词汇都同样关键，例如That 和 Crazy的重要程度就显然不同，因此可以引入基于单词的注意力机制， 也就是给不同的单词不同的权重，来捕捉关键信息。对于第二个问题，也就是如何对用户兴趣建模，每个用户虽然点了很多篇新闻，但是，他们不是对这些新闻都同样地感兴趣，不同的新闻对用户兴趣的建模关键程度也不同。这样就可以引入一个基于新闻的注意力机制来解决。然后作者列举了已有的一些新闻推荐模型，这些模型有的也有使用attention机制来识别关键因素，但是这些attention网络都是静态的，都无法做到权重根据用户来调整。不同单词对新闻内容表示的重要程度，以及新闻对用户兴趣表示的重要程度，都是因人而异的。不同的用户看到一样的标题，他们的关注点可能不同，点击了同样的几篇文章，他们对同一篇文章的感兴趣程度也不同。因此，这两层的注意力机制还必须是个性化的，才能解决个性化推荐的问题。NPA模型模型分为三个部分：  编码新闻的news encoder  编码用户兴趣的 user encoder  预测点击新闻的概率的click predictornews encoder和user encoder中都使用了word-和new-两个层级的注意力网络，并且注意力网络的权重是个性化的， 相同单词、相同新闻对每个用户的权重可能不同。News Encoder根据新闻的标题来编码一篇新闻。分为3个子模块。  word embedding(word2vec算法)          NLP中的最小单位是单词，单词组成句子，句子再组成文章。而神经网络只能接受数值性输入，因此要将自然语言的单词转换成数值向量，就是word embedding的过程。      skip-gram方法：                  输入一个单词x，预测它的上下文y；x的表示形式用one hot encode，V是词典的大小，则输入的向量就是V维的。从x到输出f(X)是一个有一个隐层的神经网络，但是隐层是线性的，没有激活函数！ 输出也是V维的，输出是x的上下文是这V个单词的概率。          在训练完成之后，得到神经网络的权重（输入到隐层，隐层到输出层）分别是输入向量和输出向量，输入向量的维度则和隐藏层节点个数相同，因为x只有一个节点是1，其他都是0，所以输入到隐层节点的权重不会每一个都一样（否则输出就会一样），所以可以用输入向量（或者输出向量）唯一表示一个单词。这样相当于将V维的输入降维。                    论文里说用一个V*D维的word embedding矩阵来做word embedding，D的维数为300, 这里用的是已经预训练好的GloVe矩阵        [NLP] 秒懂词向量Word2vec的本质    CNN： 用来观察标题中的每一个局部，发掘出局部中隐藏的信息。这个卷积层的意义是，让每一个词的表示向量不仅包含这个单词本身的信息，还要包含窗口为2k+1的上下文信息。          这个卷积就只是一层\( N_{f} \)个卷积核，加偏置B（B的维度就是Nf维），然后用Relu激活）。      window size = 2k+1，\( \mathbf{e} \)是word embeddings连接起来得到的矩阵。输出是\( c_{1} \cdot \cdot \cdot c_{m} \)，是包含了上下文信息的词表示向量。        Word Level Attention Network： 这是这个模型的重要特点之一，这里体现了个性化的注意力机制。因为每个用户在标题中对每个单词的关注度是不同的，所以attention 网络就不能像传统的注意力网络一样，对每一个用户有相同的query vector。这里使用的方法是：          先将用户的ID进行embedding变成一个De维的向量\( e_{u} \)（ 这里的问题，user ID不是单词是各种符号的集合如何embedding?  )      再将用户IDembedding向量，经过一个单层的网络映射成preference query vector。这个网络有一个Relu激活： （fig3中的红色向量）                  每一个词表示向量的权重计算是：综合了词表示向量和query vector。 （fig3中的橙色向量就是权重向量α）                      最终，新闻表示就是所有词表示的加权和：          User Encoder  user encoder是对于一个用户，将所有他点击过的新闻（用该用户的ID embedding进行word attention 之后得到的新闻表示）作为输入，经过一个news level attention 网络，得到该用户的表示。      个性化attention网络的思路跟前面差不多，还是利用了user ID embedding，生成另一个query vector：            query vector qd 和 ID embedding 内积得到新闻表示的权重向量，用同样的方法：        最终，用户的表示向量 \( \mathbf{u} \) 是该用户点击过的新闻表示向量加权和。Click Predictor在新闻推荐中，因为用户会在看到的大量新闻中，只点击其中非常少数的新闻，所以在这个问题中，正负样本是非常不平衡的，如果直接在所有candidate新闻样本上预测，训练效果显然不好，并且训练过程要花费很多时间。因此，提出一种正负样本平衡的训练方法，即联合预测K+1个样本，其中有K个负样本，1个正样本，预测每一个样本的click score：预测值yi用 新闻的表示和用户表示向量的内积得到，得到的yi值 要在K+1个样本中softmax归一化。这样，整个predictor就可以看成是一个伪二分类问题，可以使用交叉熵loss函数来训练，loss function是：只考虑正样本的损失值这样，模型从predictor到最底层的attention、CNN的参数，都是可以用back propogation来调节的。这里使用Adam来优化。实验数据集数据在MSN新闻上采集，具体情况看table1，正负样本的比例是13. 这一小节还具体列举了模型一些超参数的设置。 测试集采用最近一个星期的数据，另外随机采样了10%的数据作为验证其中，user embedding（用user ID得到的）维度是De=50， 两个查询向量 的维度Dq和Dd的维度都是200.metrics：  AUC： AUC的涵义是ROC曲线下的面积。 TPrate是指，真值为1 的数据被模型预测为真的频率， FPrate是指，真值为0的数据被模型预测为真的频率。 ROC曲线的纵轴是TPrate，横轴是FPrate。 如果曲线是y=x，那么说明无论真值是0还是1，模型都以一样的概率预测为真或假，说明模型毫无辨别能力。 AUC则是ROC曲线下的面积，一般来说要在一定的FPrate下，TPrate越高越好，那么面积就要越大越好，最坏的情况就是0.5.  AUC的好处是同时考虑了模型对正例和负例的预测能力，可以规避正负样本很不平衡时带来的问题。如何理解机器学习和统计中的AUC？      MRR： 用来评价搜索算法的一种指标，如果第n个结果匹配，得分为 1/n ， 最后得分为总和。        DCGp： 是用来评价搜索算法排序好坏的一种指标，要人为地将结果分为几个等级，每个等级对应一个分数，然后分数根据排序位置衰减，最终DCG分数是p个结果得分的总和（论文里面有预测5个的也有预测10个的分数，10个分数肯定是更高的）        nDCG： 是相对DCG，是先将人工排好序的结果作为理想状态，计算此状态下的IDCG，然后用预测得到的结果除以IDCG,得到相对的nDCG。  实验结果  有使用神经网络的模型比使用矩阵分解的传统算法效果好  使用了negative mining的算法比不使用的算法效果更好  使用attention 机制的算法也普遍比没有使用的要好，因为新闻中的不同词以及不同新闻对于新闻本身以及用户兴趣的表现重要程度确实不同。  NPA算法在被比较的算法中表现最好。]]></content>
      <categories>
        
          <category> Paper Reading </category>
        
      </categories>
      <tags>
        
          <tag> recommend system </tag>
        
          <tag> attention </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
