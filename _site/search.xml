<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[OS操作系统实验：xv6调度(RR, 优先级调度, 优先级队列） 实现和分析 Part 1- xv6代码讲解]]></title>
      <url>/lab%20report/2020/01/31/OS4/</url>
      <content type="text"><![CDATA[Lab 4：调度调度任何操作系统中，都可能出现进程的个数大于处理器个数的情况，这就需要考虑如何分配处理器资源。一般进程的执行是CPU计算和IO操作的循环，当进程长时间等待某种资源时，为了更好地利用CPU资源，应选择其他准备好的进程来代替它；当进程完成所需的时间过长时，为了让其他已经在系统中的进程等待时间不要过长，也需要在某个适当的时间暂停当前的进程, 因此便需要多进程并发，也就需要调度。调度指的是决定一个进程在什么时候、是否要暂停，从一个等待队列中选择另一个进程来代替它，调度涉及调度策略的选择，也包含完成进程切换的动作。操作系统通过不断地调度，造成并发的效果，同时也为每个进程造成独占资源的假象。调度涉及以下的问题：  如何进行进程的切换？这是通过上下文的切换来实现的。上下文是一个进程运行状态的描述，包括程序计数器(%eip), 栈底指针(%ebp), 以及其他一些寄存器的值。在进程切换时，首先要保存旧进程的上下文在内核栈上，选择一个新进程，从该进程的内核栈上加载它的上下文，然后CPU就开始执行新进程%eip指向的指令。上下文的保存和加载使得程序可以从上次调度被暂停的地方接着进行，对进程来说，就好像切换从来没有发生过一样。  如果进程不是调用sleep主动放弃CPU，如何让进程的切换透明化呢？ xv6简单地使用时钟中断来完成。当时钟中断到来时，进程陷入中断处理程序，在内核中调用yield来进行上下文切换。  多个CPU同时在切换进程时，由于需要对进程表进行修改，可能会产生竞态条件，因此还要用锁来避免竞争。  进程结束时，需要释放资源。进程不能自己释放自己的所有资源，因此内核中还必须有一个角色负责监测进程的结束、释放资源。  当进程调用sleep进入睡眠时，调度也会发生。这时，要确保有其他进程可以唤起该进程，因此xv6需要提供一套进程间通信的机制，例如sleep和wake up。本次实验将详细研究整个调度的过程，看xv6如何解决上述问题，并实现优先级调度算法。在开始实验之前，需要了解以下事实： xv6永远不会从一个用户态进程切换到另一个用户态进程。在xv6中，调度发生在以下几种情况：1. 进程调用sleep进入休眠，主动放弃CPU，这会导致进程进入内核态，保存进程的上下文并加载调度器的上下文，当调度器返回时，该进程仍处于内核态；2. 进程收到时钟中断，已经运行完一个时间片，这也会导致进程进入内核态，并在yield中将控制权交给调度器；3. 进程调用exit结束。 在这些情况下，切换的过程都是 陷入内核→保存上下文→切换到调度器的上下文 → 切换到新进程的上下文（在内核态中） → 返回新进程的用户态。TODO1: 阅读proc.c中的函数我们先看调度发生的一般场景：进程运行完时间片，被迫放弃CPU，选择下一个进程调度。xv6的时钟每100毫秒就产生一个中断，以此实现进程时间分片。时钟中断是由lapic产生的，因此每个cpu可以独立地接收它的时钟中断。当接收到时钟中断时，进程会开启保护模式，陷入到内核态，来到中断处理程序的入口，然后在alltraps中保存中断帧，调用traps，traps根据中断号来判断应该执行哪种程序。在traps的最后，有可能调用yield使进程放弃CPU：  // Force process to give up CPU on clock tick.  // If interrupts were on while locks held, would need to check nlock.  if(myproc() &amp;&amp; myproc()-&gt;state == RUNNING &amp;&amp;     tf-&gt;trapno == T_IRQ0+IRQ_TIMER)   //当目前CPU上有正在运行的进程，且中断为时钟中断时，才会调用yield    yield();yieldyield函数在proc.c中实现:// Give up the CPU for one scheduling round.voidyield(void){  acquire(&amp;ptable.lock);  //DOC: yieldlock  myproc()-&gt;state = RUNNABLE;  sched();  release(&amp;ptable.lock);}yield是将目前进程的状态从RUNNING改为RUNNABLE，让进程进入等待队列，然后调用sched将控制权转移给调度器。由于进程PCB存放在进程表上，因此对状态进行修改之前要首先acquire(&amp;ptable.lock)获取进程表的锁，等进程再次被调度时，它会返回到yield中sched的下一行，释放进程表锁。这里要注意，在上一个实验中，我们知道xv6的内核中临界节内不允许中断，所以在进入sched之前，中断是已经关闭的状态。schedsched的任务是首先确保进程有放弃CPU，进行调度的条件，然后调用swtch进行上下文切换，转到cpu调度器scheduler上。// Enter scheduler.  Must hold only ptable.lock// and have changed proc-&gt;state. Saves and restores// intena because intena is a property of this// kernel thread, not this CPU. It should// be proc-&gt;intena and proc-&gt;ncli, but that would// break in the few places where a lock is held but// there's no process.voidsched(void){  int intena;  struct proc *p = myproc();  if(!holding(&amp;ptable.lock))    panic("sched ptable.lock");  if(mycpu()-&gt;ncli != 1){    panic("sched locks");  }  if(p-&gt;state == RUNNING)    panic("sched running");  if(readeflags()&amp;FL_IF)    panic("sched interruptible");  intena = mycpu()-&gt;intena;  swtch(&amp;p-&gt;context, mycpu()-&gt;scheduler);  mycpu()-&gt;intena = intena;}首先，holding(&amp;ptable.lock)判断该cpu是否已经持有了进程表的锁，因为多个CPU在调度过程中都需要访问进程表，假如这个cpu进入调度之前，没有先持有锁，那就有可能使其他cpu也同时进行调度，同时访问进程表，可能会出现两个cpu选择了同一个进程调度的情况。然后mycpu()-&gt;ncli!=1判断该cpu调用pushcli的次数是否恰好为1，否则会报错。在上个实验我们知道，每一次调用acquire获得锁，就会使ncli加一，释放锁后ncli减一。所以ncli为1就说明这里只允许进程持有一个锁，也就是说，进程被切换时，必须持有进程表的锁，并且必须释放其他所有的锁。持有进程表的锁是为了保证CPU的调度是互斥的，防止竞态条件，而释放其他所有锁是为了防止死锁的情况出现。如果p-&gt;state==RUNNING，进程的状态是仍在运行，不可以进入调度。这是操作系统中约定好的分工：sched应该只负责进入调度器，而不应该判断进程是因为什么原因而被暂停的，所以假如进程是终止了，应该由exit来将状态变为ZOMBIE，如果进程是被时钟中断了，应该由yield将状态变为RUNNABLE，休眠也同理。进入sched之前，进程的状态应该已经改变好。sched最后通过readeflags()FL_IF，检查标志寄存器中IF段的值，确保中断已经关闭，然后它将mycpu()-&gt;intena暂时保存起来，这个变量表示CPU在调用yield之前，中断是否被允许，因为之后在调度器中要调用sti开启中断，可能会破坏原来CPU的中断状态，所以暂存起来，等从调度器返回（进程被重新调度）的时候，再恢复这个值。sched调用swtch(&amp;p-&gt;context, mycpu()-&gt;scheduler)来切换上下文，swtch的汇编代码如下：# Context switch##   void swtch(struct context **old, struct context *new);# # Save the current registers on the stack, creating# a struct context, and save its address in *old.# Switch stacks to new and pop previously-saved registers..globl swtchswtch:  movl 4(%esp), %eax  movl 8(%esp), %edx              # swtch的第二个参数，即新的上下文  # Save old callee-saved registers  pushl %ebp                      # 保存旧进程内核栈的栈底指针  pushl %ebx                      # 保存旧进程%ebx寄存器  pushl %esi					# 保存旧进程%esi  pushl %edi					#               和%edi寄存器  # Switch stacks  movl %esp, (%eax)  movl %edx, %esp				#   # Load new callee-saved registers  popl %edi  popl %esi  popl %ebx  popl %ebp  ret首先，进程上下文中包含的信息有：struct context {  uint edi;  uint esi;  uint ebx;  uint ebp;  uint eip;};为什么只需要保存这些呢？假设进程在某个函数中，发生了上下文切换，那么首先不需要保存的是调用者保存的寄存器，如%eax，%ecx，%edx，因为该函数的调用者已经提前把它们保存在进程的栈上了。也不需要保存段寄存器，如%cs等，因为在指令地址发生改变的时候，这些寄存器也会同时改变。所以，要保存的有栈底指针、%ebx、程序计数器%eip、参数寄存器%edi、%esi。上下文保存在进程内核栈上：在进程从yield进入到sched再进入到swtch的这个时候，cpu首先是运行在旧进程的内核栈上的。在这里，swtch传入两个参数，第一个是旧进程上下文的指针的地址，第二个是该cpu调度器进程的上下文的指针，调度器的上下文也是调度器上一次调用swtch时保存的。我们逐条指令分析上下文切换的过程：swtch:  movl 4(%esp), %eax                  # 第一个参数  movl 8(%esp), %edx                  # 第二个参数这两条指令中，%esp指向旧进程内核栈现在的栈底，因为它调用了swtch，所以（%esp）上存放的是sched中swtch的返回地址。4(%esp)和8(%esp)分别是swtch的第一个和第二个参数，也就是旧进程上下文和新进程（调度器）上下文的指针。画出旧进程内核栈：                   …                  旧进程上下文指针      p-&gt;context              调度器上下文指针      mycpu()-&gt;scheduler              %esp→      mycpu()-&gt;intena = intena 对应指令的地址                            # Save old callee-saved registers  pushl %ebp                      # 保存旧进程内核栈的栈底指针  pushl %ebx                      # 保存旧进程%ebx寄存器  pushl %esi					# 保存旧进程%esi  pushl %edi					#               和%edi寄存器这四条指令是将旧进程的上下文保存到当前栈（旧进程的内核栈）上。                   …                  旧进程上下文指针      &amp;p-&gt;context              调度器上下文指针      mycpu()-&gt;scheduler                     mycpu()-&gt;intena = intena 对应指令的地址                     ebp                     ebx                     esi              %esp→      edi                            # Switch stacks  movl %esp, (%eax)       # 令p-&gt;context = %esp  movl %edx, %esp				然后交换栈。从第一、二条指令我们知道，现在（%eax）中是旧进程的上下文指针，令（%eax）=%esp，也就是让旧进程上下文指针重新指到现在它保存的地方。而%edx中是调度器上下文指针，把%edx赋给%esp，使栈底指针指向了调度器上下文所在的位置，这样，就从旧进程的内核栈切换到了调度器所在的栈（前者是代表用户进程的，是用户进程在内核态下运行时使用的栈，后者不代表任何用户进程，它是内核进程进行时使用的栈）。            (旧进程的内核栈)      …                  旧进程上下文指针      &amp;p-&gt;context              调度器上下文指针      mycpu()-&gt;scheduler              旧进程中swtch返回地址      mycpu()-&gt;intena = intena 对应指令的地址                     ebp                     ebx                     esi              p-&gt;context→      edi                                        （调度器所在的栈）      …                  调度器中swtch返回地址      ret              调度器的上下文      ebp                     ebx                     esi              %esp→      edi      栈切换之后，栈底指针指向调度器上下文所在的地址。现在，就可以从栈上pop出调度器的上下文了：# Load new callee-saved registers  popl %edi  popl %esi  popl %ebx  popl %ebp  ret最后，ret会返回到scheduler中，swtch的下一行位置（而不是返回sched)。这样，cpu控制权就从旧的进程转移到了调度器。从swtch指令中，我们没有看到对%eip的显式保存，这是因为旧进程的%eip在用call swtch调用swtch时就已经隐式地保存在了%ebp的前面，同样，ret指令也隐式地把调度器栈上的返回地址加载到了%eip中。schedulerscheduler就是上述的调度器。每一个进程最终都会将控制权返回到调度器，调度器会从等待队列中选择一个进程开始运行，它会调用swtch保存自己的上下文，然后切换到该进程的上下文开始运行。//PAGEBREAK: 42// Per-CPU process scheduler.// Each CPU calls scheduler() after setting itself up.// Scheduler never returns.  It loops, doing://  - choose a process to run//  - swtch to start running that process//  - eventually that process transfers control//      via swtch back to the scheduler.voidscheduler(void){  struct proc *p;  struct cpu *c = mycpu();  c-&gt;proc = 0;    for(;;){    // Enable interrupts on this processor.    sti();     //每一次从sched进入调度器，都会开启中断    // Loop over process table looking for process to run.    acquire(&amp;ptable.lock);    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;state != RUNNABLE)        continue;      // Switch to chosen process.  It is the process's job	    ........    }    release(&amp;ptable.lock);  }}调度器是一个两层的for循环。外层循环是无限循环，这意味着调度器永远不会返回。内层循环中，调度器遍历进程表，选择进程运行。在CPU开始的时候，它就会调用scheduler.scheduler每一次从内层循环退出，进入外层循环，都要显示地执行sti指令允许中断，并且要将ptable锁释放之后再重新获取。这两个步骤都是很有必要的，因为从内层循环退出，意味着调度器可能检查了一遍ptable，没有找到可以运行的进程，这时有可能所有的进程都在等待IO中断，如果不执行sti开启中断的话，IO中断永远也不能到达进程。而如果一个闲置CPU的调度器一直不释放锁，那么其他CPU也不能访问ptable，进行上下文或者系统调用了，所以就没有CPU能够将一个进程的状态改为RUNNABLE，这个CPU也无法跳出循环。scheduler内层循环遍历进程表，寻找下一个RUNNABLE的进程并切换到进程的上下文中运行。for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;state != RUNNABLE)        continue;      // Switch to chosen process.  It is the process's job      // to release ptable.lock and then reacquire it      // before jumping back to us.      // 找到了一个RUNNABLE的进程      c-&gt;proc = p;      //该cpu上现在运行的进程为*p      switchuvm(p);     // 加载该进程的地址空间      p-&gt;state = RUNNING;   //将进程状态变为RUNNING      swtch(&amp;(c-&gt;scheduler), p-&gt;context);      switchkvm();      // Process is done running for now.      // It should have changed its p-&gt;state before coming back.      c-&gt;proc = 0;    }当调度器找到了一个RUNNABLE的进程，就将cpu-&gt;proc 设置为它的PCB指针，然后调用switchuvm将该进程的内存空间、页表加载进内存中，并将进程状态设置为RUNNING.然后，调用swtch进行上下文的切换。现在，第一个参数（旧的上下文）是调度器的上下文指针的地址&amp;(c-&gt;scheduler), 第二个参数是新进程上下文的指针。这个swtch将保存调度器的上下文，并将c-&gt;scheduler指向保存的位置，然后从调度器的栈换到新进程的内核栈，从栈上加载新进程的上下文，然后转到新进程sched中swtch返回处运行。新进程将马上释放ptable 锁。有的时候，swtch也不一定是返回到新进程的sched处。如果该新进程是刚刚被fork产生，这是它第一次被调度，那么swtch就会返回到forkret处，在forkret中释放ptable锁，再从forkret返回到trapret，退出内核态。当调度器从swtch返回，意味着有某个进程调用了sched把控制权返还给它，它首先调用swtchkvm转换到内核的页表，然后将c-&gt;proc置为0，表示暂时没有进程在该cpu上运行。然后，如果该进程不是位于ptable的最后一个槽，调度器就会继续查找下一个RUNNABLE的进程，重复以上步骤。否则，它将释放ptable锁，并开启中断。睡眠与唤醒睡眠和唤醒提供了进程间通信的机制，它们可以让一个进程暂时休眠，等待某个特定事件的发生，然后当特定事件发生时，另一个进程会唤醒该进程。睡眠与唤醒通常被称为顺序合作或者有条件同步机制。 睡眠是调度发生的另一种情况，当进程调用sleep进入休眠时，它会调用sched把控制权交给调度器。sleep有两个参数，第一个参数chan是休眠被唤醒的信号，这个信号使得进程可以互相通信，一个进程调用sleep(chan)进入休眠，另一个进程用同样的chan调用wakeup(chan)就可以把它唤醒。第二个参数lk是调用休眠时必须持有的一个锁，这个锁主要是为了防止“遗失的唤醒”问题。一般，进程如果需要休眠，它需要循环判断某个条件是否成立（例如磁盘是否已经准备好），如果还不成立，就会调用sleep进入休眠。例如：while(r = something() == 0){    sleep();}之所以需要while循环判断，是因为如果某次事件成立了，进程从休眠中唤醒，但它被唤醒之后可能不是马上就被调度、马上就开始执行后面的代码，所以在这中间，有可能条件又不成立了，所以需要唤醒之后马上继续判断。而如果没有上面所说的lk锁，就可能发生，while判断条件不成立，进程准备进入休眠；但是这时候发生调度，另一个进程使得条件成立，想要唤醒进程，但这时候因为它还没休眠，所以找不到进程可以唤醒。再切换回原来的进程时，这个进程不知道条件已经成立了，它会进入休眠，并且之后再没有办法唤醒它。 因此，必须确保在条件判断和调用sleep是不会被wakeup打断的，即wakeup不可以在进程真正进入sleep之前被调用。这可以用锁来实现。// Atomically release lock and sleep on chan.// Reacquires lock when awakened.voidsleep(void *chan, struct spinlock *lk){struct proc *p = myproc();//调用sleep的进程if(p == 0)panic("sleep");if(lk == 0)panic("sleep without lk");//必须持有lk锁// Must acquire ptable.lock in order to// change p-&gt;state and then call sched.// Once we hold ptable.lock, we can be// guaranteed that we won't miss any wakeup// (wakeup runs with ptable.lock locked),// so it's okay to release lk.// 下面sleep必须修改ptable，把这个进程的状态改为SLEEPING,并把p-&gt;chan也就是休眠的等待事件改为//第一个参数chan，因此要比较传进来的第二个参数lk是不是ptable，如果是ptable，那进程就应该一直持有它，//不应该释放。if(lk != &amp;ptable.lock){ //DOC: sleeplock0acquire(&amp;ptable.lock); //DOC: sleeplock1release(lk);}sleep首先判断是否持有lk锁，否则报错。 然后，sleep需要释放lk锁并获得ptable-&gt;lock。这里有两种情况：  lk不是ptable-&gt;lock，则因为wakeup函数也要求获得ptable-&gt;lock，所以释放lk是没问题的，ptable-&gt;lock代替了lk，保证不会出现“遗失的唤醒”问题。必须在释放lk之前，先获取ptable.lock，否则有可能在释放lk之后，获取ptable.lock之前，发生调度，另一个进程此时便可以调用wakeup。而之所以要获取ptable-&gt;lock是因为之后要对进程表进行访问、修改，并且要调用sched进入调度器。  lk就是ptable-&gt;lock，则不需要任何操作。不可以先释放ptable.lock然后再重新获取，原因跟上面lk释放与ptable.lock获取的顺序不能调换的原因是一样的。然后进程就要进入休眠。// Go to sleep.p-&gt;chan = chan;         //休眠等待信号，wakeup函数中在ptable中查找p-&gt;chan=chan并且状态为休眠的进程去唤醒。p-&gt;state = SLEEPING;    //修改进程状态sched();              //将控制权交给调度器p-&gt;chan = 0;// Reacquire original lock.if(lk != &amp;ptable.lock){ //DOC: sleeplock2release(&amp;ptable.lock);acquire(lk);}调用sched之后，该进程就被暂时挂起了，sched会将上下文切换到调度器，再从调度器切换到另外的进程。当这里的sched返回的时候，表明进程已经被唤醒。它要将等待条件清为0，然后重新获得lk锁。重新获得锁时，则不必先获得lk,再释放ptable.lock，因为此时进程已经从休眠中唤醒了，它不会担心在释放和重新获得之间，其他进程调用wakeup，其他进程是否在这中间调用wakeup对该进程是没有影响的。对应地，wakup函数从ptable中找到状态为SLEEPING并且在chan上休眠的进程（可能多个），将它的状态设置为RUNNABLE，这样它就可以被CPU的调度器调度。// Wake up all processes sleeping on chan.// The ptable lock must be held.static voidwakeup1(void *chan){  struct proc *p;  for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++)    if(p-&gt;state == SLEEPING &amp;&amp; p-&gt;chan == chan)      p-&gt;state = RUNNABLE;}// Wake up all processes sleeping on chan.voidwakeup(void *chan){  acquire(&amp;ptable.lock);  wakeup1(chan);  release(&amp;ptable.lock);}wakeup是wakeup1的加锁版本。wakeup因为要访问并修改ptable，所以需要持有ptable.lock。wakeup作为系统调用让用户去调用时，操作系统而非用户要负责获取ptable.lock锁； 但在系统中，例如exit等函数也会调用wakeup，但在这之前它就已经获得了ptable.lock锁，所以也为内核提供了不加锁的wakeup1版本。waitwait让父进程等待子进程结束，并回收子进程的资源。它返回退出的子进程的pid，如果没有子进程或其他错误情况，则返回-1.// Wait for a child process to exit and return its pid.// Return -1 if this process has no children.intwait(void){	struct proc *p;	int havekids, pid;	struct proc *curproc = myproc();	acquire(&amp;ptable.lock);    //要遍历ptable中的所有进程，找到调用进程的一个子进程    //如果能够找到，需要修改该子进程的状态，因此要对ptable做出修改    //不允许两个进程同时访问ptable，因此要先占有ptable的锁才能对其进行访问和修改。	for(;;){        // Scan through table looking for exited children.        havekids = 0;        for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){        	if(p-&gt;parent != curproc)        		continue;            //找到了该进程的子进程            havekids = 1; //havekids表示调用的进程有子进程            if(p-&gt;state == ZOMBIE){ //有一个子进程调用了exit或因其他原因退出，处于ZOMBIE状态                // Found one.                pid = p-&gt;pid; //准备返回该子进程的pid                kfree(p-&gt;kstack); //释放子进程占用的内存空间，这里是释放子进程的内核栈                p-&gt;kstack = 0; //将进程内核栈底指针重置为0                freevm(p-&gt;pgdir); //释放页表，释放所有用户空间所占据的物理页框                p-&gt;pid = 0; //接下来是将PCB全都重置为0                p-&gt;parent = 0;                p-&gt;name[0] = 0;                p-&gt;killed = 0;                p-&gt;state = UNUSED; //该PCB状态变为UNUSED，之后调用allocproc的时候，这块PCB可能就会被分配给一个新的进程                release(&amp;ptable.lock); //释放ptable锁                return pid; //返回退出的子进程的pid   	 		}		}        // ptable的遍历已经完成，到这里没有返回，说明没有子进程处于ZOMBIE        // No point waiting if we don't have any children.        if(!havekids || curproc-&gt;killed){ //如果该进程并没有子进程，或者该进程在wait的过程中被杀死（比如用ctrl+C等），那就不需要等待了，直接释放ptable锁，返回-1.        	release(&amp;ptable.lock);        	return -1;		}		//否则，该进程有子进程， 但所有的子进程都还在运行，必须让该进程进入休眠，等待一个子进程的结束		// Wait for children to exit. (See wakeup1 call in proc_exit.)		sleep(curproc, &amp;ptable.lock);		//调用sleep进入休眠。    }}wait()函数等待它的一个子进程终止。在exit()函数中，我们看到exit()只是关闭了进程打开的文件和从目录中退出，但仍然保留了进程的信息，进程占据的内存和PCB都没有被释放，只是处于ZOMBIE状态。进程信息和内存空间的释放由wait()来完成。wait遍历过一次进程表，发现有子进程，但是都还没结束，那么它就调用sleep(curproc, &amp;ptable.lock)进入休眠。这个休眠会被子进程在exit中，调用wakeup(myproc()-&gt;parent)来唤醒。Part 2 是如何在xv6中实现Round Robin调度情况统计，统计用户进程的周转、等待、执行时间。下一篇： Part 2]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> xv6 </tag>
        
          <tag> scheduling </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OS操作系统实验：xv6调度(RR, 优先级调度, 优先级队列） 实现和分析 Part 3- xv6 优先级调度算法实现]]></title>
      <url>/lab%20report/2020/01/31/OS4-3/</url>
      <content type="text"><![CDATA[上一篇： Part 2 RR调度周转、等待时间等的统计第一篇： Part 1 xv6调度代码讲解TODO3:  实现优先级调度算法基于优先级的调度算法是一类算法，基本思想是为每一个进程赋予不同的优先级，在调度时优先选择优先级最高的进程来调度。优先级调度有许多种，大致可以分为：      非抢占式的优先级调度：    非抢占式的优先级调度算法中，每个进程只在初始时赋予一个优先级，在运行过程中不会被改变。每一次重新调度时，选择最高优先级的一个进程，将它运行完毕，然后再选择另外一个进程开始运行。一个进程在运行过程中如果有更高优先级的进程到来，它不会被打断，只是将该进程放到等待队列的队首。    xv6是开启抢占的，进程会被时钟中断打断，然后调度器选择另外一个进程（如果有的话）来代替它。        抢占式优先级调度：    抢占式优先级调度中，如果进程运行过程有更高优先级的进程到来，当它运行完这个时间片，就会被抢占。进程被赋予一个初始优先级，但这个优先级是可变的。具体的实现也有多种：                  静态优先级调度：        静态优先级调度指的是进程最初有一个优先级，运行过程中可以通过系统调用改变这个优先级，但是不会随着等待时间或运行时间的增加而自动改变。                    动态优先级调度：        进程最初被指定一个优先级，同样也可以通过系统调用改变。为了惩罚执行时间较长的进程，优先级会随着运行时间增加而逐渐降低。                    多级反馈队列调度：        在系统中设置每个优先级对应的等待队列，进程初始化时进入某个队列。调度时，首先从最高优先级队列中找进程，只有当更高优先级队列为空时，才会调度某个低优先级队列中的进程。 同一个优先级的进程按FCFS调度。        多级反馈队列也有不同的实现。静态实现时，进程优先级不在运行过程中自动改变；动态实现时，可以让进程每运行完k个时间片，就下降到低一级的等待队列，一段时间后，再将进程提高到最高优先级。这样，既可以充分考虑IO-bound、CPU-bound进程的不同特性，又考虑到进程运行过程中IO-bound到CPU-bound的动态转变，减少进程饥饿的发生。                    我将实现静态优先级调度和静态的多级反馈队列调度。静态优先级调度静态优先级调度是为每个进程赋予一个初始优先级，用户可以指定优先级。在每次重新调度时，遍历整个进程表，找到状态为RUNNABLE且优先级最高的进程来运行。优先级的范围是[1,4], 数字越低代表优先级越高。首先为进程添加priority变量：// Per-process statestruct proc {  uint sz;                     // Size of process memory (bytes)  pde_t* pgdir;                // Page table  char *kstack;                // Bottom of kernel stack for this process  .....  int priority;                // 进程的优先级, 取值范围是1-4  uint ctime;                   // 创建时间  ...优先级调度的一个问题是，如何决定进程初始化时的优先级。初始化时的优先级也是系统进程的优先级，这个优先级不能太高，否则会让用户进程响应和等待的时间过长；但也不能太低，否则当用户需要某个内核进程的服务时，等待时间也会太长。这里，先将初始优先级设置为2. 在allocproc()中，进程初始化时：found:  p-&gt;state = EMBRYO;  p-&gt;pid = nextpid++;														//modified here  p-&gt;ctime = ticks;  p-&gt;rutime = 0;  p-&gt;sltime = 0;  p-&gt;retime = 0;														//priority modified  p-&gt;priority = 2;  release(&amp;ptable.lock);新的scheduler函数，在内层for循环中，再遍历整个进程表，用phigh保存目前为止优先级最高的进程。voidscheduler(void){  struct proc *p;  struct cpu *c = mycpu();  c-&gt;proc = 0;    for(;;){    // Enable interrupts on this processor.    sti();    // Loop over process table looking for process to run.    acquire(&amp;ptable.lock);														//PRIORITY modified    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      #if defined RR      if(p-&gt;state != RUNNABLE)            // 原来的 Round Robin 调度        continue;      #elif defined PRIORITY             // 静态优先级调度      struct proc *pnow;                 // 用来遍历ptable的指针      struct proc *phigh=0;              // 遍历过程中保存最高优先级的进程      if(p-&gt;state!= RUNNABLE)         continue;            phigh = p;           // 遍历整个表之前，phigh首先是下一个RUNNABLE的进程。      for( pnow = ptable.proc; pnow&lt;&amp;ptable.proc[NPROC]; pnow++){        if(pnow-&gt;state!=RUNNABLE)     // 遍历表中所有RUNNABLE的进程          continue;         if(pnow-&gt;priority &lt; phigh-&gt;priority){   //如果有一个RUNNABLE的进程优先级比phigh高	      phigh = pnow;							//更新phigh	    }      }      p = phigh;                    //令下一个执行的进程为phigh      #endif            // Switch to chosen process.  It is the process's job      // to release ptable.lock and then reacquire it      // before jumping back to us.      if(p!=0){         cprintf("%d is being scheduled on cpu: %d, its priority is %d.\n", p-&gt;pid, c-&gt;apicid, p-&gt;priority);	 	c-&gt;proc = p;        switchuvm(p);	 	p-&gt;state = RUNNING;	 	swtch(&amp;(c-&gt;scheduler), p-&gt;context);        switchkvm();	 // Process is done running for now.	 // It should have changed its p-&gt;state before coming back.	 	c-&gt;proc = 0;      }    }    release(&amp;ptable.lock);  }}调度器的内层循环用指针p遍历整个进程表，本来每一次找到一个RUNNABLE进程，就切换到该进程。 用priority算法，每次调度一个进程后返回调度器时，p指向下一个RUNNABLE进程，phigh首先指向这个进程，然后遍历整个进程表，如果有更高级的进程，phigh会指向接下来第一个优先级最高的进程；如果没有，则下一个进程就是表中下一个可执行进程。实现系统调用set_priority：intset_priority(int pid, int priority){    struct proc *p=0;    acquire(&amp;ptable.lock);    for(p=ptable.proc; p&lt; &amp;ptable.proc[NPROC]; p++){       if(p-&gt;pid == pid){           p-&gt;priority=priority;           release(&amp;ptable.lock);           return 0;       }              }    // 找不到该pid，错误    release(&amp;ptable.lock);    return -1;}把它封装好之后，修改RRsta.c， 加入set_priority：int stdout=1;int main(int argc, char* argv[]){    if(argc!=2){        printf(1, "usage: priority_sta &lt;fork number&gt; \n");	exit();    }    int forknumber=atoi(argv[1]);    int status;    for(int i=0;i&lt;forknumber;i++)    {        status=fork();        if(status)		{	    	set_priority(status, i%4+1);   //在父进程中，将每个子进程的优先级设置为i%4+1.        }        if(status==0){	    	for(int count=0;count&lt;40; count++){				printf(1," ");		    	for(int k=0;k&lt;1000000;k++){					int result=0;					for( int j=0; j&lt;5000; j++){			    		result+=j*j;                        result-=j*j;                        result= result/j;                        result=result*j;                        result = result+result/j;					}		   		}            }                        exit();        }    }    int rutime;    int retime;    int sltime;    int pid;    int sum=0;    while((pid = waitSch(&amp;rutime,&amp;retime,&amp;sltime))!=-1){        sum+= rutime+retime+sltime;    }    printf(1, " average turn-around time of %d process is %d\n", forknumber, sum/forknumber);    exit();}在内核中，添加一些额外的输出来验证算法实现的正确性。重新编译内核，然后执行priority_sta.c, fork 4个进程：$ priority_sta 4pid为3的进程是父进程，子进程的pid分别为4， 5， 6， 7，创建后set_priority, 使它们的优先级分别为1， 2， 3， 4；进程的初始优先级被设置为2.当进程4被创建，并设置优先级为1之后，它是系统中优先级最高的进程，正确情况下会一直运行直到结束（除非它进入休眠，或者有同样优先级为1的进程到来）；因为有两个cpu，进程3可以在另一个cpu上调度，继续创建进程5，6. 这时，进程4已经结束，系统中有3，5， 6三个进程，但正确情况下只有3和5可以被调度，6必须等待直到3和5不是RUNNING或RUNNABLE状态。从这段程序的输出来看，目前的调度都是正确的。进程6一直没有被调度，直到进程3进入休眠：进程3创建了进程7（最后一个子进程）之后，它就进入休眠等待所有子进程结束。这时进程6被第一次调度，而直到进程5结束，系统只剩下3，6，7时，在进程3休眠时进程7才可被调度。统计所有进程的运行情况：优先级越高的进程平均周转时间越长。从图中可以看出，进程的运行时间相差不大，而优先级最低的进程（7，11）明显等待时间比其他进程要长很多（11）。但从单个进程来看，不一定优先级越低进程周转时间越长，例如进程10（优先级为3）的周转时间就比进程11（优先级为4）长很多，这是因为进程11到达更晚，且它到达时系统中更高优先级的进程几乎已经都执行结束。静态多级反馈队列静态多级反馈其实跟上面的静态优先级调度类似，但是它使用了多级队列的方法，使优先级相同的进程能够按照FCFS来调度。静态优先级调度则不一定是先来先服务的，因为它是遍历整个进程表找最高优先级的进程，这样，当pid序号在前的进程退出被回收之后，前面就会出现空槽，更新创建的进程反而会比老的进程pid序号更低，更先被调度。实现多级反馈队列，首先要为每个优先级定义对应的等待队列。等待队列由一个proc结构体的指针数组来表示，考虑到进队和出队的动作一般发生在进程的优先级发生改变或者状态改变的时候，都会持有ptable.lock,则访问等待队列已经可以保证互斥。因此可以省略等待队列的锁。同时，用变量size来表示等待队列目前的大小，也就是最后一个进程的下标。#if defined SMLtypedef struct queue_t{  int size;  struct proc* proc[NPROC];  int priority;    //for debug}queue_t;struct queue_t pqueue_1;   //4个优先级队列struct queue_t pqueue_2;struct queue_t pqueue_3;struct queue_t pqueue_4;#endif内核开始时，要初始化4条优先级队列，定义一个初始化函数来完成：voidpqueue_init(struct queue_t *queue, int priority){   queue-&gt;priority=priority;     //为方便输出debug，加入变量priority   queue-&gt;size=0;   for(int i=0; i&lt;NPROC; i++)    //初始时指针为0      queue-&gt;proc[i]=0;}voidpinit(void){  initlock(&amp;ptable.lock, "ptable");				  #if defined SML									//sml modified  pqueue_init(&amp;pqueue_1,1);  pqueue_init(&amp;pqueue_2,2);  pqueue_init(&amp;pqueue_3,3);  pqueue_init(&amp;pqueue_4,4);  #endif}等待队列涉及三种操作，即入队，删除队列中的一个进程，以及取等待队列中最早到达的一个RUNNABLE的进程。由于进程的优先级在运行过程中可能动态变化，所以可能要将一个进程从某个队列删除后添加到另一个队列中，这时，为了保持队列从到达时间早到晚排序，要遍历队列，找到该进程的插入位置。从最后（q-&gt;size-1)往前扫描，如果当前指针进程的到达时间ctime比新进程p-&gt;ctime大（晚），就把它往后挪。直到指向一个进程，到达时间&lt;=p-&gt;ctime，然后把p插入到它的后面。最后，要将q-&gt;size增加一。void enqueue(struct queue_t *q, struct proc *p){    cprintf("proc %d enqueueing %d.\n", p-&gt;pid, q-&gt;priority);    int pos;    pos = q-&gt;size - 1;    while(pos &gt;= 0 &amp;&amp; q-&gt;proc[pos]-&gt;ctime &gt; p-&gt;ctime){   //从后往前扫描        q-&gt;proc[pos+1] = q-&gt;proc[pos];    //如果目前进程比新插入进程到达晚，就把它向后挪一个位置		pos--;    }    // 目前pos指向的进程比插入的进程到达时间早或相同。    q-&gt;proc[pos+1] = p;       q-&gt;size ++;   }删除队列的操作，是从前往后遍历该等待队列，找到要删除的进程，如果能找到，则将从该位置开始后面的进程都往前挪一个位置，最后一个位置变为0，最后把size减1.voidremoveq(struct queue_t* q, struct proc *p){   cprintf("removing %d from q: %d\n",p-&gt;pid, q-&gt;priority);   int pos;   int found = 0;   pos = 0;   while(pos &lt; q-&gt;size){      if(q-&gt;proc[pos]==p){   //从前往后，找到该进程          found=1;          break;      }      else{        pos++;      }   }   if(found){   //若找到，此时pos指向要删除的进程      while(pos &lt; q-&gt;size-1){    //从 pos 到倒数第二个进程位置            q-&gt;proc[pos] = q-&gt;proc[pos+1];  //将所有进程往前挪一个位置            pos++;      }      q-&gt;proc[q-&gt;size-1] = 0;      q-&gt;size --;    }   }查找等待队列中第一个RUNNABLE的进程，只要从前往后遍历，找到第一个RUNNABLE的进程就返回即可。如果未找到，则返回0.struct proc*headq(struct queue_t* q){    struct proc* p = 0;    int pos;    for(pos = 0; pos &lt; q-&gt;size; pos++){      if(q-&gt;proc[pos]-&gt;state == RUNNABLE){    //找到第一个RUNNABLE进程，注意等待队列是从到达时间小到大排序的        p = q-&gt;proc[pos];        break;      }    }    return p;}接下来要就是要实现进程在执行过程中状态或优先级变化时相应的等待队列的操作。由于等待队列代价比较高，所以尽量在确定进程已经正确分配好各种资源，才将它加入到初始优先级队列中去。 在userinit中，设置默认优先级为2：  #if defined SML														//sml modified  p-&gt;priority=2;  enqueue(&amp;pqueue_2, p);  #endif在fork()中，让子进程继承父进程的优先级（默认为2），然后进入相应的等待队列：  if((np-&gt;pgdir = copyuvm(curproc-&gt;pgdir, curproc-&gt;sz)) == 0){    kfree(np-&gt;kstack);    np-&gt;kstack = 0;    np-&gt;state = UNUSED;									    return -1;  }  np-&gt;sz = curproc-&gt;sz;  np-&gt;parent = curproc;  *np-&gt;tf = *curproc-&gt;tf;														//sml modified  np-&gt;priority = curproc-&gt;priority;  #if defined SML  switch(np-&gt;priority){     //进入相应的优先级队列     case 1: enqueue(&amp;pqueue_1, np); break;     case 2: enqueue(&amp;pqueue_2, np); break;     case 3: enqueue(&amp;pqueue_3, np); break;     case 4: enqueue(&amp;pqueue_4, np); break;     default: panic("priority must be between 1-4.");  }       #endif在进程终止，被父进程回收资源时，要将它们从相应的等待队列中删除，在wait和waitSch中作同样的改变：intwait(void){  struct proc *p;  int havekids, pid;  struct proc *curproc = myproc();    acquire(&amp;ptable.lock);  for(;;){    // Scan through table looking for exited children.    havekids = 0;    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;parent != curproc)        continue;      havekids = 1;      if(p-&gt;state == ZOMBIE){        // Found one.        pid = p-&gt;pid;        kfree(p-&gt;kstack);        p-&gt;kstack = 0;        freevm(p-&gt;pgdir);                  												//SML modified        #if defined SML        switch(p-&gt;priority){                    //删除等待队列中的进程			case 1: removeq(&amp;pqueue_1, p); break;	     	case 2: removeq(&amp;pqueue_2, p); break;	     	case 3: removeq(&amp;pqueue_3, p); break;	     	case 4: removeq(&amp;pqueue_4, p); break;	     	default: ;		}		#endif	        p-&gt;pid = 0;        p-&gt;parent = 0;        p-&gt;name[0] = 0;        p-&gt;killed = 0;        												// modified        p-&gt;ctime=0;        p-&gt;rutime=0;        p-&gt;retime=0;        p-&gt;sltime=0;        												// end        p-&gt;state = UNUSED;        release(&amp;ptable.lock);调度器的算法则跟之前较为不同。RR和静态优先级调度scheduler都是内循环遍历整个表，每次寻找最高优先级或第一个RUNNABLE进程进行调度，现在，静态多级反馈队列不用遍历整个表，它从最高优先级队列开始找起，如果某个队列里面有RUNNABLE的进程，就运行它，否则就往下一个优先级队列继续寻找：    #if defined RR ||PRIORITY    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      #if defined RR      if(p-&gt;state != RUNNABLE)        continue;      #elif defined PRIORITY      struct proc *pnow;      struct proc *phigh=0;      if(p-&gt;state!= RUNNABLE)         continue;            phigh = p;      for( pnow = ptable.proc; pnow&lt;&amp;ptable.proc[NPROC]; pnow++){        if(pnow-&gt;state!=RUNNABLE)          continue;        if(pnow-&gt;priority &lt; phigh-&gt;priority){	  phigh = pnow;	}      }      p = phigh;      #endif              // Switch to chosen process.  It is the process's job      // to release ptable.lock and then reacquire it      // before jumping back to us.      if(p!=0){         cprintf("%d is being scheduled on cpu: %d, its priority is %d.\n", p-&gt;pid, c-&gt;apicid, p-&gt;priority);	 c-&gt;proc = p;         switchuvm(p);	 p-&gt;state = RUNNING;	 swtch(&amp;(c-&gt;scheduler), p-&gt;context);         switchkvm();	 // Process is done running for now.	 // It should have changed its p-&gt;state before coming back.	 c-&gt;proc = 0;      }    }    #endif                                      //=====================modified begin ================//    #if defined SML                            //多级反馈队列从这里开始    p = headq(&amp;pqueue_1);    if(p==0)      p=headq(&amp;pqueue_2);    if(p==0)      p=headq(&amp;pqueue_3);    if(p==0)      p=headq(&amp;pqueue_4);    if(p){                          //若找得到进程，则运行它，否则暂时释放ptable锁，开启中断，然后再次获取ptable锁，继续从第一优先级队列开始寻找         cprintf("%d is being scheduled on cpu: %d, its priority is %d.\n", p-&gt;pid, c-&gt;apicid, p-&gt;priority);	     c-&gt;proc = p;         switchuvm(p);	     p-&gt;state = RUNNING;	     swtch(&amp;(c-&gt;scheduler), p-&gt;context);         switchkvm();	 // Process is done running for now.	 // It should have changed its p-&gt;state before coming back.	     c-&gt;proc = 0;     }    #endif  当调用set_priority时，有可能进程的优先级改变，则要将它从旧的优先级队列中删除并放到新的优先级队列中：intset_priority(int pid, int priority){    struct proc *p=0;    acquire(&amp;ptable.lock);    for(p=ptable.proc; p&lt; &amp;ptable.proc[NPROC]; p++){       if(p-&gt;pid == pid){                  #if defined SML           if(p-&gt;priority!=priority){           	switch(p-&gt;priority){           	    case 1: removeq(&amp;pqueue_1, p);   break;           	    case 2: removeq(&amp;pqueue_2, p);   break;           	    case 3: removeq(&amp;pqueue_3, p);   break;           	    case 4: removeq(&amp;pqueue_4, p);   break;           	    default: ;           	}           	switch(priority){		       case 1: enqueue(&amp;pqueue_1, p);   break;		       case 2: enqueue(&amp;pqueue_2, p);   break;		       case 3: enqueue(&amp;pqueue_3, p);   break;		       case 4: enqueue(&amp;pqueue_4, p);   break;		       default: ;		}           }           #endif                                 p-&gt;priority=priority;               release(&amp;ptable.lock);           return 0;       }              }    release(&amp;ptable.lock);    return -1;}多级反馈队列实现已经完成。将它编译，执行priority_sta.c，还是将子进程数目设置为4，得到如下输出：进程3是父进程，4，5，6，7分别是4个子进程，优先级被设置为1，2，3，4. 可以看到，每个进程在fork完成之前，都先进入了默认的第二优先级队列。当调用set_priority时，每个进程都从原来的queue 2离开，进入到相应新的队列中（进程5除外，进程5本来就在queue 2中）。当所有进程都创建好之后，因为3比5先到达，则只有进程3和4可以被调度；当进程3在倒数第二行进入休眠之后，进程5才可以得到调度。然后，等进程4退出并被回收了，它从queue 1中被删除。当进程3进入休眠的时候，进程5和6可以得到调度，在这整个过程中，进程7的优先级都不足，无法被调度。而当进程6进入休眠时，进程7才和5一起被调度。说明算法逻辑正确。统计多级反馈下程序运行时间：这个结果是符合预期的，如果程序执行的内容都一样，优先级越低，周转时间应该越长，因为更高优先级的进程应该更早被调度。简单比较比较所有进程的平均轮转时间：            进程个数      RR      静态优先级      多级反馈队列                  4      6      4      4              10      8      4      7              30      15      10      17      （轮转时间的计算跟机器状态也有关系，有些时候机器运转较快，则用户程序的运行和调度算法都比较快，测得时间就会少）如果忽略机器状况的因素，可以发现RR算法的平均轮转时间在进程个数一定时是多于静态优先级调度的。这是因为所有进程轮流调度，本来可以更早结束的进程时间被延长了；而静态优先级调度虽然低优先级的进程周转时间比平均长很多，但优先级高的进程周转时间也对应地低很多，并且如果先来先服务，高优先级进程基本上可以到达之后就一直进行直到结束，所以平均周转时间可能会更好。但也要考虑算法的花销。在进程个数很多的时候，多级反馈队列每一次插入和删除的开销会快速增长，这导致进程的等待时间变得非常长。为了缓解饥饿的问题，可以对优先级调度算法进行优化，采用动态的多级反馈队列：      进程刚开始时进入最高优先级队列。    不同的优先级队列中，进程有不同的时间片。优先级越高，时间片越短。这样，低优先级进程可以有更多机会完成任务。  每运行完一个/k个时间片，进程就下降优先级，最低优先级的进程反而重新回到最高优先级这样做的好处是，进程刚开始创建优先级最高，可以有更快的响应时间；优先级随着进程运行下降，则可以偏向短作业，让短作业更快结束，从而缩短等待时间；每隔一段时间就将最低优先级的进程提高到高优先级，这样可以防止饥饿发生。但是，动态多级反馈队列显然需要更多的算法花销，不同时间片的设计更为复杂，且需要设计每个优先级适当的时间片大小，以及按进程特性决定创建时进入哪个队列。即便如此，动态多级反馈队列仍然是一种比较优的算法。]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> xv6 </tag>
        
          <tag> scheduling </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OS操作系统实验：xv6调度(RR, 优先级调度, 优先级队列） 实现和分析 Part 2- xv6 RR调度情况统计]]></title>
      <url>/lab%20report/2020/01/31/OS4-2/</url>
      <content type="text"><![CDATA[上一篇： Part 1-xv6 调度代码讲解TODO2： 统计RR调度情况实现waitSch系统调用todo2需要增加一个系统调用waitSch(int* rutime, int* retime, int* sltime)作为原来wait的扩展，除了执行原有功能以外，要将进程的运行时间、在等待队列中的时间和休眠时间输入到三个参数所代表的地址中。首先我们将需要维护的数据结构定义在proc.h中：struct proc {  uint sz;                     // Size of process memory (bytes)  pde_t* pgdir;                // Page table  char *kstack;                // Bottom of kernel stack for this process  ...    uint ctime;                   // 创建时间  uint rutime;                  // 处于RUNNING下的时间  uint retime;                  // 处于ready状态下的时间  uint sltime;                  // 处于Sleeping状态下的时间};然后，在proc.c中，也应该做出相关的修改。首先在进程被初始化的时候，应该将p-&gt;ctime设置为当时的时钟ticks，然后其他的几个变量应该初始化为0。考虑到第一个用户进程并不是由fork产生，我们将初始化放在allocproc中：found:  p-&gt;state = EMBRYO;  p-&gt;pid = nextpid++;														//modified here  p-&gt;ctime = ticks;   //创建时间  p-&gt;rutime = 0;        p-&gt;sltime = 0;  p-&gt;retime = 0;  release(&amp;ptable.lock);  // Allocate kernel stack.  if((p-&gt;kstack = kalloc()) == 0){    p-&gt;state = UNUSED;    p-&gt;ctime = 0;     //分配错误的时候，要恢复为0.    return 0;  }相应地，当进程结束，资源要被回收时，应该将这几个变量清空。我们不修改在exit中，因为此时这些进程运行的统计数据仍然要保留，直到父进程调用wait找到它，将它回收的时候，再把这些数据处理完清空。在wait中： if(p-&gt;state == ZOMBIE){        // Found one.        pid = p-&gt;pid;        kfree(p-&gt;kstack);        p-&gt;kstack = 0;        freevm(p-&gt;pgdir);        p-&gt;pid = 0;        p-&gt;parent = 0;        p-&gt;name[0] = 0;        p-&gt;killed = 0;        												// modified        p-&gt;ctime=0;	    p-&gt;rutime=0;	    p-&gt;retime=0;	    p-&gt;sltime=0;        												// end        p-&gt;state = UNUSED;        release(&amp;ptable.lock);        return pid;接下来是实现waitSch，waitSch的大部分逻辑都和wait一样，不同的只是要先把p-&gt;rutime赋给*rutime，把p-&gt;retime赋给*retime, 以及p-&gt;sltime→*sltime，然后再把这些变量清空:int waitSH(int* rutime, int* retime, int* sltime){  struct proc *p;  int havekids, pid;  struct proc *curproc = myproc();    acquire(&amp;ptable.lock);  for(;;){    // Scan through table looking for exited children.    havekids = 0;    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;parent != curproc)        continue;      havekids = 1;      if(p-&gt;state == ZOMBIE){        // Found one.        pid = p-&gt;pid;        kfree(p-&gt;kstack);        p-&gt;kstack = 0;        freevm(p-&gt;pgdir);        p-&gt;pid = 0;        p-&gt;parent = 0;        p-&gt;name[0] = 0;        p-&gt;killed = 0;        																// modified	    *rutime = p-&gt;rutime;	    *sltime = p-&gt;sltime;	    *retime = p-&gt;retime;	    p-&gt;ctime=0;	    p-&gt;rutime=0;	    p-&gt;retime=0;	    p-&gt;sltime=0;        																// end        p-&gt;state = UNUSED;        release(&amp;ptable.lock);        return pid;      }    }    // No point waiting if we don't have any children.    if(!havekids || curproc-&gt;killed){      release(&amp;ptable.lock);      return -1;    }    // Wait for children to exit.  (See wakeup1 call in proc_exit.)    sleep(curproc, &amp;ptable.lock);  //DOC: wait-sleep  }} 现在的问题是，如何动态地维护进程的运行时间、等待时间和休眠时间呢？首先一个直观的思路是，在每一次进程状态变化的时候，例如由A→B，记录下这次状态转换的时间作为B状态的开始时间$t_{0}$, 等下一次再从B→C的时候，B的持续时间就加上这个时刻减去$t_{0}$. 但是这种方法实现起来非常复杂，首先进程状态转换可能在很多种情况下发生，每一次转换时，都要判断旧的状态和新的状态是什么，然后更新旧状态的持续时间，保存新状态的开始时间。这样代码会变得冗长而且容易出错，还需要为进程维护两倍的变量（对每一种状态，例如RUNNING, 至少需要维护最近一次RUNNING状态开始的时刻，以及累计运行时间）。另一种思路是，在每一次收到时钟中断时，判断进程处于何种状态，为这种状态的持续时间加一。这样的计算是一种很粗糙的近似。这样计算，是假设进程会将这种状态维持一个时间片，在中间不发生状态的变换，并假设这种状态大概在时钟中断发生时开始。然而，在两个时钟中断之间，进程是很有可能调用sleep或wait进行休眠的，也有可能在下一个时钟中断到来之前，调度器就已经调度了另一个进程并将该进程唤醒，状态变为RUNNABLE，这样，中间的sleep阶段就没有被我们的计算捕捉到。不过因为进程一般是运行和等待的时间占大多数，sleep占比很少，运行和等待之间的切换又一般是通过时钟中断引发的yield, 所以这种近似还是可以接受的。xv6 的时钟机制xv6的时钟在timer.c中实现，每过100ms，硬件就会产生一个时钟中断。每个cpu都可以独立地接收时钟中断，并陷入中断处理。在trap.c中定义了一个uint类型的变量ticks，每一次这个变量加一，代表系统的时钟往前走了一步。两个cpu收到时钟中断后，先后进入中断处理程序，为了让系统的时钟(ticks)能够真正在每次timer产生时钟中断的时候自增一，只在某一个固定的cpu收到中断时让ticks++, 因此xv6中每一次CPU0收到一个时钟中断，ticks就自增1：switch(tf-&gt;trapno){  case T_IRQ0 + IRQ_TIMER:     //收到了时钟中断    if(cpuid() == 0){          //如果是cpu0收到了这个时钟中断，才将tick增加1；这样对于不同的cpu，时钟都是保持同步的，所有的cpu都会共用这个tick变量。      acquire(&amp;tickslock);             ticks++;      wakeup(&amp;ticks);      release(&amp;tickslock);    }    lapiceoi();    cprintf("ashajh,time: %d, cpu: %d\n",ticks, cpuid());    break;编译这段代码，输出是：这段输出中，每一次cpu id 为0时，对应的时钟比上一次printf增加一，而对于cpu1则不一定是这样。如果我们要在每次时钟中断时判断进程的状态，也有两种思路。第一个是，在时钟中断的时候，调用myproc先判断cpu上是否有进程在运行，如果有，则判断它的状态并更新变量。这种思路首先因为进程会在两个cpu上调度，而在cpu1上调度时，两次中断之间ticks不一定有增加，所以要增加一个lasttick保存上一次中断时ticks的值，如果这个值有改变，才会进行判断。但即使是添加了这个逻辑，这种方法也是错误的，因为如果myproc()能返回进程的PCB， 进程一定是处于RUNNING状态（否则它就不会被cpu调度了），这样我们无法知道进程何时在休眠或等待。第二个思路是，在ticks++的时候，遍历整一个ptable表，对每个进程判断它是在何种状态，然后给变量+1.这样便可实现上面那种近似的算法。 在trap.c中作以下修改：  switch(tf-&gt;trapno){  case T_IRQ0 + IRQ_TIMER:    if(cpuid() == 0){      acquire(&amp;tickslock);      ticks++;      									//modified      update();                         //每一次ticks++时，更新进程状态变量      wakeup(&amp;ticks);      release(&amp;tickslock);    }    lapiceoi();    break;其中，判断进程状态并更新变量的函数在proc.c中实现：void update(){    struct proc *p;    acquire(&amp;ptable.lock);    //对进程变量进行修改，要先获得ptable锁。    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){    switch(p-&gt;state) {   //遍历整个表，判断进程状态      case RUNNING:      //运行中        p-&gt;rutime++;     //则运行时间增加一个时间片        break;      case SLEEPING:    //休眠中        p-&gt;sltime++;     //休眠时间增加一个时间片        break;      case RUNNABLE:    //等待中        p-&gt;retime++;    //等待时间增加一个时间片        break;      default: ;    // default，有ZOMBIE\EMBRYO\UNUSED，这些状态我们不关心    }  }  release(&amp;ptable.lock);  //释放锁}    我们按照上次实验的步骤将waitSch包装成系统调用。 实现过程中比较重要的是，xv6syscall.c中系统调用函数的统一格式是返回值int, 不允许带参数。如果需要加入参数，则参数会被压入栈中，通过argint和argptr从栈上取出参数进行解析。在sysproc.c中包装waitSch如下：intsys_waitsch(void){    int *rutime;    int *retime;     int *sltime;    int pid;    rutime=retime=sltime=0;    if(argptr(0, (char**)&amp;rutime, sizeof(int)) &lt; 0)        return -1;    if(argptr(1, (char**)&amp;retime, sizeof(int)) &lt; 0)        return -1;    if(argptr(2, (char**)&amp;sltime, sizeof(int)) &lt; 0)        return -1;    pid = waitSH(rutime, retime, sltime);    if(pid!=-1)    //如果有子进程退出，打印出它的pid，运行时间、等待时间和休眠时间。        cprintf("pid: %d, runtime:%d, ready time: %d, sleep time %d\n", pid, *rutime, *retime, *sltime);    return pid;} 在syscall.h以及系统调用表中，为waitSch增加系统调用号，并在user.h中定义接口。waitSch的实现已经完成。编写RRsta.c 统计RR调度情况编写RRsta.c， main函数接收一个命令行参数n, fork n个子进程进行相同的大规模计算，然后父进程调用waitSch，等待每个子进程的结束，并输出每个子进程的运行时间、等待时间和休眠时间。最后统计n个进程在RR（Round Robin）调度下的平均轮转时间。#include "param.h"#include "types.h"#include "stat.h"#include "user.h"#include "fs.h"#include "fcntl.h"#include "syscall.h"#include "traps.h"#include "memlayout.h"int stdout=1;int main(int argc, char* argv[]){    int forknumber=atoi(argv[1]);   //命令行参数，为fork 子进程的个数    int status;    for(int i=0;i&lt;forknumber;i++)        {        status=fork();        if(status==0){    //在子进程内status=pid=0            for(int count=0;count&lt;40; count++){  //执行相同的运算，主要是大规模乘、除法计算。                printf(1," ");                for(int k=0;k&lt;1000000;k++){                    int result=0;                    for( int j=0; j&lt;5000; j++){                        result+=j*j;                        result-=j*j;                        result= result/j;                        result=result*j;                        result = result+result/j;                    }		    	}            }                        exit();   //一定要注意exit，否则子进程也会执行上面的fork循环。        }    }    int rutime;   //运行时间    int retime;    int sltime;    int pid;      //子进程pid    int sum=0; //周转时间总和    while((pid = waitSch(&amp;rutime,&amp;retime,&amp;sltime))!=-1){        sum += rutime+retime+sltime;    }    printf(1,"average turn-around time of %d process is %d.\n", forknumber, sum/forknumber);    exit();}将RRsta.c编译到qemu中，在shell中运行make cleanmakemake qemu$ RRsta 10得到输出：（实际上，手动计算平均周转时间的话会得到5.8， 但是xv6的printf并没有使用C的标准输出库，不支持输出浮点数，只能将小数省去了。下一篇： Part 3： 优先级和动态多及反馈队列的实现、调度算法比较]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> xv6 </tag>
        
          <tag> scheduling </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[Notes]NPA:Neural News Recommendation with Personalized Attention]]></title>
      <url>/paper%20reading/2020/01/30/NPA/</url>
      <content type="text"><![CDATA[基于个性化注意力机制的新闻推荐论文链接：https://arxiv.org/pdf/1907.05559.pdf这篇论文是2019年SIGKDD的7篇精选论文之一。主要解决的是新闻推荐中个性化的问题。Introduction首先介绍了为什么需要新闻推荐（减少信息过载，高效获取信息）新闻推荐有什么样的难题（可以归结为如何表示新闻，以及如何表示用户兴趣）：这篇论文的作者之前已经做过一项工作，就是多视角学习的新闻推荐，所谓多视角，就是用标题、内容、类别和子类别分别去表示一篇新闻，结果是类别和子类别获得了最高的attention权重。但是在这一篇论文中，作者选择了用标题来表示新闻，原因可能是，相比内容标题更短代价更小，但是相比类别又具有更多潜在信息。但是，一个新闻标题中，并不是所有词汇都同样关键，例如That 和 Crazy的重要程度就显然不同，因此可以引入基于单词的注意力机制， 也就是给不同的单词不同的权重，来捕捉关键信息。对于第二个问题，也就是如何对用户兴趣建模，每个用户虽然点了很多篇新闻，但是，他们不是对这些新闻都同样地感兴趣，不同的新闻对用户兴趣的建模关键程度也不同。这样就可以引入一个基于新闻的注意力机制来解决。然后作者列举了已有的一些新闻推荐模型，这些模型有的也有使用attention机制来识别关键因素，但是这些attention网络都是静态的，都无法做到权重根据用户来调整。不同单词对新闻内容表示的重要程度，以及新闻对用户兴趣表示的重要程度，都是因人而异的。不同的用户看到一样的标题，他们的关注点可能不同，点击了同样的几篇文章，他们对同一篇文章的感兴趣程度也不同。因此，这两层的注意力机制还必须是个性化的，才能解决个性化推荐的问题。NPA模型模型分为三个部分：  编码新闻的news encoder  编码用户兴趣的 user encoder  预测点击新闻的概率的click predictornews encoder和user encoder中都使用了word-和new-两个层级的注意力网络，并且注意力网络的权重是个性化的， 相同单词、相同新闻对每个用户的权重可能不同。News Encoder根据新闻的标题来编码一篇新闻。分为3个子模块。  word embedding(word2vec算法)          NLP中的最小单位是单词，单词组成句子，句子再组成文章。而神经网络只能接受数值性输入，因此要将自然语言的单词转换成数值向量，就是word embedding的过程。      skip-gram方法：                  输入一个单词x，预测它的上下文y；x的表示形式用one hot encode，V是词典的大小，则输入的向量就是V维的。从x到输出f(X)是一个有一个隐层的神经网络，但是隐层是线性的，没有激活函数！ 输出也是V维的，输出是x的上下文是这V个单词的概率。          在训练完成之后，得到神经网络的权重（输入到隐层，隐层到输出层）分别是输入向量和输出向量，输入向量的维度则和隐藏层节点个数相同，因为x只有一个节点是1，其他都是0，所以输入到隐层节点的权重不会每一个都一样（否则输出就会一样），所以可以用输入向量（或者输出向量）唯一表示一个单词。这样相当于将V维的输入降维。                    论文里说用一个V*D维的word embedding矩阵来做word embedding，D的维数为300, 这里用的是已经预训练好的GloVe矩阵        [NLP] 秒懂词向量Word2vec的本质    CNN： 用来观察标题中的每一个局部，发掘出局部中隐藏的信息。这个卷积层的意义是，让每一个词的表示向量不仅包含这个单词本身的信息，还要包含窗口为2k+1的上下文信息。          这个卷积就只是一层\( N_{f} \)个卷积核，加偏置B（B的维度就是Nf维），然后用Relu激活）。      window size = 2k+1，\( \mathbf{e} \)是word embeddings连接起来得到的矩阵。输出是\( c_{1} \cdot \cdot \cdot c_{m} \)，是包含了上下文信息的词表示向量。        Word Level Attention Network： 这是这个模型的重要特点之一，这里体现了个性化的注意力机制。因为每个用户在标题中对每个单词的关注度是不同的，所以attention 网络就不能像传统的注意力网络一样，对每一个用户有相同的query vector。这里使用的方法是：          先将用户的ID进行embedding变成一个De维的向量\( e_{u} \)（ 这里的问题，user ID不是单词是各种符号的集合如何embedding?  )      再将用户IDembedding向量，经过一个单层的网络映射成preference query vector。这个网络有一个Relu激活： （fig3中的红色向量）                  每一个词表示向量的权重计算是：综合了词表示向量和query vector。 （fig3中的橙色向量就是权重向量α）                      最终，新闻表示就是所有词表示的加权和：          User Encoder  user encoder是对于一个用户，将所有他点击过的新闻（用该用户的ID embedding进行word attention 之后得到的新闻表示）作为输入，经过一个news level attention 网络，得到该用户的表示。      个性化attention网络的思路跟前面差不多，还是利用了user ID embedding，生成另一个query vector：            query vector qd 和 ID embedding 内积得到新闻表示的权重向量，用同样的方法：        最终，用户的表示向量 \( \mathbf{u} \) 是该用户点击过的新闻表示向量加权和。Click Predictor在新闻推荐中，因为用户会在看到的大量新闻中，只点击其中非常少数的新闻，所以在这个问题中，正负样本是非常不平衡的，如果直接在所有candidate新闻样本上预测，训练效果显然不好，并且训练过程要花费很多时间。因此，提出一种正负样本平衡的训练方法，即联合预测K+1个样本，其中有K个负样本，1个正样本，预测每一个样本的click score：预测值yi用 新闻的表示和用户表示向量的内积得到，得到的yi值 要在K+1个样本中softmax归一化。这样，整个predictor就可以看成是一个伪二分类问题，可以使用交叉熵loss函数来训练，loss function是：只考虑正样本的损失值这样，模型从predictor到最底层的attention、CNN的参数，都是可以用back propogation来调节的。这里使用Adam来优化。实验数据集数据在MSN新闻上采集，具体情况看table1，正负样本的比例是13. 这一小节还具体列举了模型一些超参数的设置。 测试集采用最近一个星期的数据，另外随机采样了10%的数据作为验证其中，user embedding（用user ID得到的）维度是De=50， 两个查询向量 的维度Dq和Dd的维度都是200.metrics：  AUC： AUC的涵义是ROC曲线下的面积。 TPrate是指，真值为1 的数据被模型预测为真的频率， FPrate是指，真值为0的数据被模型预测为真的频率。 ROC曲线的纵轴是TPrate，横轴是FPrate。 如果曲线是y=x，那么说明无论真值是0还是1，模型都以一样的概率预测为真或假，说明模型毫无辨别能力。 AUC则是ROC曲线下的面积，一般来说要在一定的FPrate下，TPrate越高越好，那么面积就要越大越好，最坏的情况就是0.5.  AUC的好处是同时考虑了模型对正例和负例的预测能力，可以规避正负样本很不平衡时带来的问题。如何理解机器学习和统计中的AUC？      MRR： 用来评价搜索算法的一种指标，如果第n个结果匹配，得分为 1/n ， 最后得分为总和。        DCGp： 是用来评价搜索算法排序好坏的一种指标，要人为地将结果分为几个等级，每个等级对应一个分数，然后分数根据排序位置衰减，最终DCG分数是p个结果得分的总和（论文里面有预测5个的也有预测10个的分数，10个分数肯定是更高的）        nDCG： 是相对DCG，是先将人工排好序的结果作为理想状态，计算此状态下的IDCG，然后用预测得到的结果除以IDCG,得到相对的nDCG。  实验结果  有使用神经网络的模型比使用矩阵分解的传统算法效果好  使用了negative mining的算法比不使用的算法效果更好  使用attention 机制的算法也普遍比没有使用的要好，因为新闻中的不同词以及不同新闻对于新闻本身以及用户兴趣的表现重要程度确实不同。  NPA算法在被比较的算法中表现最好。]]></content>
      <categories>
        
          <category> Paper Reading </category>
        
      </categories>
      <tags>
        
          <tag> recommend system </tag>
        
          <tag> attention </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
