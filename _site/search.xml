<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[[论文笔记]Knock Knock, Who's There? Membership Inference on Aggregate Location Data]]></title>
      <url>/paper%20reading/2020/04/25/membershipinference/</url>
      <content type="text"><![CDATA[论文：NDSS 2018的best paper award：Knock Knock, Who’s There? Membership Inference on Aggregate Location DataKnock Knock, Who’s There? Membership Inference on Aggregate Location Data在涉及数据公开的时候，为了保护用户的隐私，机构通常会公开统计数据，隐去可以识别个体的信息，在公开数据的时候，还要注意不能让人从统计数据中推断出个体的信息。现在有越来越多的应用，为了提供某些功能需要收集用户的位置序列，并发布聚集的位置信息，例如收集车辆的位置序列来显示一张动态的交通情况地图，估算某条路线的拥堵情况；或者聚集显示某个餐厅的人数，来估算等待时间。因为位置信息、特别是时间序列的位置很敏感，可以从中得到许多用户的个人隐私，所以这种位置的统计数据更要注意不能泄露个体信息。https://arxiv.org/abs/1703.00366这篇文章表明如果有目标用户移动轨迹的一些先验知识，就可以从聚集的数据中挖掘得到更多信息，甚至可以定位该用户； 有时用户的轨迹可以直接从聚集数据中推断，而不需要任何先验的知识。要从统计数据中推断个体，首先就必须要能推断出某个目标个体的数据是不是该聚集的一部分，这个叫做membership inference. 基于membership inference，membership inference attack最开始是在Membership Inference Attacks Against Machine Learning Models这篇论文中提出来的一种新的攻击方式，在MIA这篇论文中关注的是机器学习模型的隐私泄露问题，就是推断目标是否出现在某个模型的训练集中。 Membership inference 的方法也可以用在数据提供者要发布统计数据的时候，评估发布的危险性。作者将membership inference作为一个分类问题，即对一个target user，预测它是否是某个聚集数据的成员。在两个数据集上测试，结果预测的AUC非常高，在有先验知识的情况下，甚至在每一个聚集都包含9500个用户的时候AUC还能达到 1.0。 他们也测试了影响预测正确率的一些因素，例如，每组用户人数、timeframe、数据的颗粒度、稀疏程度。他们用基于差分隐私的方法来研究如何对抗成员推断攻击，发现这种对抗总体都是比较有效的，但是会以牺牲功能为代价，而且当攻击者使用噪声数据来训练的时候，这种保护的效果就不那么好了。本文的贡献：  提出一种聚集位置序列数据的成员推断方法  用该方法去量化raw aggregate的隐私泄露问题  阐述了这种方法可以如何用来研究defense mechanism的有效性问题定义假设一个应用收集U 个用户的数据，发布在一系列的时间区间内，处于某些感兴趣区域（ROI）的用户人数。一个攻击者使用发布的数据，以及可能的一些先验知识，来推断一个个体是否属于某一个聚集的数据。即，我们知道十点-十点十分，在中央公园的用户有100个人，要推断目标个体是不是这100个人中的一个。记号：所有用户的集合所有感兴趣区域的集合：聚集位置信息被收集的时间间隔：一个用户的位置时间序列可以被表示为一个0,1的|S| x |T| 的矩阵，其中在$s_{i}$ $t_{j}$ 为1表示该用户在时间区间$t_{j}$ 时正在位置$s_{i}$.所有用户的位置时间序列就可以被表示为一个|U|x |S|x|T| 的三维张量 L。 用矩阵Ax来表示用户集合X的聚集，其中Ax[s] [t] 表示在t区间内有多少个用户在位置s。攻击者对目标用户的先验知识标记为 P， 这个先验知识是他在某一段观察时间To ∈ T中观察得到的，先验知识可以用来推断， 要推断的聚集数据在时间Ti ∈ T时发布。作者把Membership inference任务表达为一个游戏，参数是用户集合U，每个聚集数据包括的成员个数m，以及发布多少个时间区间的数据 Ti如图，adv先随便取一个目标用户u，然后Ch随机取一个不包含u 的U的子集Y，大小为m-1. 然后随机一个bit b，b就表示了u是不是在聚集中，如果b=1，那就把u和Y中所有成员聚集，得到AUb。如果b=0，就随机取另外一个不在Y中的用户取代u。 把聚集的矩阵Aub返回给Adv。这时，Adv要根据Aub、先验知识P、u、m、Ti来猜测b‘， 目标是让b’ = b。这个猜测b‘的过程 可以看作一个函数d。目标是在多次推断中猜对的概率要大于1/2（瞎猜），越高越好。Methodology研究：1. 如何在To内构建先验知识P； 2. 如何构建函数d，或者如何初始化函数d。1. 构建先验知识P两种方法：      subset of locations：攻击者知道一个包含u*的用户子集在Ti 时间区间中的真实位置。即：        比如说某些可以取得一些用户真实位置数据的攻击者，如app开发者、通信服务提供商等。        participation in past groups： 攻击者知道在过去的时间To内，用户u是否包含在某些aggregated group内。即，对于若干个U的自己Wi，知道他们在To的聚集矩阵Awi，以及u是否在这些集合中        这些集合有两种可能：          和Ti 公布的聚集是重合的      和Ti 公布的聚集可能不重合        1适用于从时间To到Ti，用户和他附近的用户位置都比较固定，例如是在同一个街区的数据。2是攻击者只知道u*在过去一段时间是否属于某些group，但这些group是不太稳定的，所以和Ti收集到的并不一样，例如用户在某个时间点搬家了。  2. Distinguishing function这里distinguishing function就是一个以m、Ti、u*、P 和Aub为输入的二分类器，用监督学习的方式来训练。3. 量化隐私泄露作者用distinguishing function的性能来衡量隐私泄露的情况，即，函数d性能越好，说明隐私泄露得越多。 引入了一个privacy loss metric， 这个metric 基于AUC。adv做出一系列对b的猜测，然后通过b 和 b’来计算AUC， 其中b=0是positive， b=1是negative，以此计算TP\TN\FP\FN。得到ROC曲线和AUC。实验1. 数据集  TFL transport for london： 伦敦的公共交通， 2010三月。包含每一张Oyster Card的touch in、touch out time、station id的序列。把一张卡视为一个用户，station作为ROI, touch in touch out都算作用户处于该地点。时间粒度是1小时。  San Francisco Cab network： 三藩的出租车数据； 2008年5月-6月； 包含car id、经度维度、时间戳。 把城市分成10x10的网格，每一个网格是一个ROI，时间区间是1小时，每个id视作一个用户。TFL 比SFC更predictable，因为日常的出行轨迹有一些频繁模式， 但是比SFC更稀疏。因为SFC中每辆车报告位置比TFL中用户刷卡频繁得多。把两个数据集中的用户，按他们出现的ROI个数从高到低排序，分为三个等级，从每个等级中采样50个用户，得到150个用户作为攻击的目标。2. 实验设置对每一个攻击目标u*， adv要先在从先验知识P中产生的数据上训练，然后在预测阶段的时候给出分类预测。一次实验可以分成3个阶段：  Aggregate： 在这个阶段，构建数据集D。 重复以下步骤：          随机选取m个包括u*的用户，在时间区间Ti内进行聚集，得到对应的矩阵A作为D的一个样本，并且标注为in。      随机选取m个不包括u*的用户，标注为out，做同样的事。        重复上面两个步骤就是让数据集正负样本平衡。    Feature Extraction：对于D的每一个数据样本A，注意A是一个矩阵，每一行就是在一个ROI的聚集情况，对每一行进行一些统计指标的计算，例如每一行的平均值（意思就是某个地方，在时间Ti区间内，平均每个间隔有多少用户在），标准差、方差、最大最小值等等。这些统计指标会作为classifier的输入。  Classification： 把数据集D分成训练集和预测集，在训练集上训练之后，在预测集上进行distinguish ability game。 实验中采用了scikit-learn的几种分类器：logistic、knn、random forests、MLP3. 在Raw Aggregate数据集上评估membership inference的能力（raw aggregate就是原始聚集数据，没有采用差分隐私等方法）对不同的先验知识P，来进行评估。记得先验知识被分为subset of location 和participation in past groups两大类。      subset of location：    攻击者知道在Ti时间内，包括目标用户在内的一小部分用户的真实位置。那么产生数据集的时候就用这个先验知识， 假设Ti知道的这部分是U1： 1. 从U1产生训练数据集；2. 从U-U1 ∪ {u*} 产生测试集。对不同的每组用户个数m测试AOC：    结果：    fig2是在TFC的结果； fig4是在SFC的结果； fig3是privacy loss与m的关系。自然地，m越小，就越容易泄露隐私，当m与先验知识的大小差不多的时候，就难以推断出membership了。在fig3中，TFL的privacy loss 比SFC高（m&lt;100), 说明在越稀疏的数据集上越容易泄露隐私。        participation in past groups：                  same groups as released： 攻击者知道u在过去的To中，是否参与了某些groups的聚集，而且这些groups和放出的预测集是同一些group。所以数据集D就是先随机采样150个一半包含u，一半不包含u*的用户集，训练集是这些用户集在To时间内的聚集，测试集是它们在Ti时间内的聚集，也就是把D按时间划分。        结果：当m=9500的时候，MLP仍然可以得到0.99的AUC，说明当mobility patterns随时间很稳定的时候，如果攻击者有prior knowledge就可以成功地在未来推断u*，即便用户集非常大。但是在SFC数据集上效果就明显差了，当m很小的时候，AUC只有大概0.6-0.7，比起TFL的0.99-1.0差了很多，当m增加的时候，传统模型就和random guess差不多，MLP也掉到0.57. TFL是人们公交打卡的数据，这种会非常规律，而SFC是出租车数据，这种则比较随机，所以past knowledge就难以推断未来。这说明了regularity和membership inference的关系。                                    different groups than released： 先随机采样400个用户集，一半包含u*一半不包含，然后随机分成300个给训练集，100个给测试集，训练集在To时间内聚集，测试集在Ti时间内聚集。        结果：        TFL在m很小的时候，仍然可以得到很好的结果；当m增加到100-1000，所有分类器的性能都大幅下降，传统模型和random guess差不多，但是当m=9500的时候，因为训练集、测试集用到的user group有很多交集，所以效果就比较像same groups as released的情况，会好一点点。总的来说privacy loss 比之前小了很多。SFC也有相同的模式，但SFC数据更没规律，所以结果比TFL差。                          考虑Ti时长的影响：    设置Ti 为 8小时、24小时、168小时，在TFL\SFC上分别实验，设置为prior knowledge=2.1，m=1000和m=100，用各自最好的分类器，结果：    在TFL数据集上，Ti缩短AUC下降，因为时长不够模型发掘规律；而且weekday 和weekends也有显著的差别，在星期1 Ti=8时，效果明显比周六好，因为周一人们公交打卡是非常有规律的。在SFC上，则除了Ti=168以外，其他都没有明显区别，而且都接近0.5.      Take-away：当攻击者知道包括目标用户在内的用户群体真实位置、或者知道过去一段时间目标用户的成员情况时，membership inference是很容易实现的。它的效果和数据集本身的特征（稀疏性、规律性）有关，和聚集的用户数m有关，和timeframe以及预测的时间在周末/工作日 有关。evaluating DP defenses1. 差分隐私介绍差分隐私用来保护数据集中的差别导致的隐私泄露。定义相邻数据集D1， D2，他们有且仅有一个元素不一样：对于一个随机化算法M（也就是对于同一个输入不一定会产生同一个输出，产生何种输出符合某种分布）， 如果在D1和D2数据集中产生结果O的概率分布大致相同，就说这个算法达到了差分隐私的效果。形式化表达：为了衡量一条数据的差别对某个函数的输出有多大的影响， 定义 sensitivity的概念：常用的达到差分隐私的方法是在数据集或者输出上加随机噪声，常用的是拉普拉斯噪声。将M定义为：随机算法M达到ε-差分隐私，当还有一种较弱的方法，就是在时序聚合的数值加Lap(1/ε)的噪声，但只能保护单个location visit，所以在本文只作为一种baseline方法。高斯噪声： 比Laplacian弱的是在输出加高斯分布的噪声。FPA： 专门针对时序聚集数据的方法是使用离散傅里叶变换的FPA，一个时间序列用离散傅里叶变换进行压缩，然后保留前k个系数，用拉普拉斯噪声干扰，最后用0 padding到原来的长度，逆变换。EFPAG： enhanced fourier perturbation algorithm with gaussian noise： 按某种概率选择FPA中的k，使得原序列和经过干扰之后的序列root sum squared error最小，并且用高斯噪声代替拉普拉斯加入噪声会使数据变得不准确，当数据量小的时候，噪声的影响相对较大，作者也提到证明了数据量较小的时候，使用以上方法数据的可用性有很明显的下降。所以接下来的实验都是在m较大的情况下完成的。2. 实验为了评估差分隐私方法对抗membership inference 的有效性，实验假设了一种最坏的情况，就是攻击者拥有非常多的先验知识，即同时知道prior knowledge 1 和 2.1（准确位置和u*以往的membership）。有这些prior knowledge，分类器的AUC基本上是1.测试数据集现在聚集的时候用以上的差分隐私方法进行干扰。训练数据集则有两种产生策略：  row aggregation： 即分类器在没有进行干扰的聚集数据上训练  采用了和测试数据集一样的干扰方法的训练数据， 这个模仿的是一种比较active的攻击者，知道使用了什么样的保护策略，并在模仿这种保护策略的数据上进行训练metrics： 引入privacy gain的概念来衡量对抗方法的有效性。用row aggregate 和 加入噪声之后的聚集之间的mean relative error来衡量utility loss。效果：当保护比较严格（ε很小的时候），攻击者是否用了干扰的数据训练对于PG没有很明显的不同，但是当ε较大的时候，可以看到即使在SFC数据集上，PG也下降了很多。用拉普拉斯、高斯噪声干扰utility损失很多，而FPA等方法对utility的损害就比较小，但是以privacy loss 为代价，utility和privacy loss之间是trade off 的关系。最后我比较怀疑的是文章中这种membership inference的方法在现实中是不是真的能够实现……就如果攻击者可以知道包括target user和其他一些用户的位置的话，ta就可以用这些数据来以文中的方法产生训练数据，但既然已经知道target user的真实位置，怎么会还有必要去做membership inference呢？ 而如果攻击者可以知道target user过去是否属于某些group，那ta又是怎么知道这种信息的，是不是要比较了解该app聚集用户信息的方式，或者本来就已经知道该用户的活动范围了？然后是否有可能像文中一样，知道的这些信息中正负样本是1:1平衡的呢？样本不平衡的话分类器效果就没那么好了。不知道我的理解是不是正确的。]]></content>
      <categories>
        
          <category> Paper Reading </category>
        
      </categories>
      <tags>
        
          <tag> privacy </tag>
        
          <tag> location data </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[cs224w笔记]8.Graph Neural Network]]></title>
      <url>/social%20network/graph-based%20machine%20learning/2020/03/29/cs224w8/</url>
      <content type="text"><![CDATA[内容提要：  图卷积原理和基础版  graphSAGE  graph attention network回到node embedding问题，之前的DeepWalk以及node2vec等等，用的都是“shallow embedding”，他们的encoder函数都是相当于一个lookup table。这样的做法有什么局限性？  要学习的参数非常多， 整个表都是要学习的，n个节点，d维表示，那就是nd个参数，而n往往是非常大的。  inherently transductive： 它不能产生那些不在training过程中的节点的embedding，就是如果有一个新的网络/新的节点，同样的参数不能在新网络上使用，必须重新训练。  没有考虑节点本身的特征，只是考虑了网络结构。Graph Convolutional Networks在网络上的“卷积”，可以看成是中心节点从它的邻居聚集 信息一个直接的想法： 用adjacency matrix直接作为dnn的输入这个方法不行：  首先parameter会非常非常多， 只第一层输入层的参数就有n* feature dimension 那么多， 这个参数会远远多于训练数据  对于大小不同的图无法使用，因为输入必须是固定的大小，但网络的结构是经常变化的  not invariant to node ordering, 网络节点标号不同的顺序，得到的结果应该是一样的。Deep learning for graphsG， V， A， X是node feature matrix这个aggregate function，就是图中的方块，应该是节点顺序不变性的， 比如说求最大值、求平均值等等， 经验表明，summation的效果比较好。上图右边就是一个 两层的neural network， 一般来说， 在图上的dnn层数不会很深，因为社交网络的直径在6左右！所以一般3、4层就可以了。layer -0 的输入就是节点的特征向量x，然后每个节点会聚集邻居的特征向量x，直到最顶层， 最终得到节点的embedding要学习的参数是Wk和Bk （两个都是矩阵，和特征的维数应该是一样的。如何训练？可以是unsupervised training， 用random walks、graph factorizing来表示节点的similarity， 然后相似的节点应该有相近的embedding。也可以直接对具体的任务做supervised training、semi-supervised training。例如：  node classification因为W和B只是不同的层是不一样的， 而对每一个节点是一样的， 所以如果网络加进来了新的节点， 可以在原来那一部分训练好模型之后， 直接apply到新的节点上。GraphSAGE和上面相比， GraphSAGE的两个不同，一个在于可以用不同的Aggregation函数，另一个在于不是直接把邻居和本节点融合(+)，而是向量拼接。对不同的问题，可以采用不同的聚集函数：  mean  pool，可以取最小或者最大值  LSTM： 因为embedding是需要顺序不变性的，但LSTM输入是顺序的， 所以可以用随机顺序输入训练。其中， mean aggregation可以用矩阵相乘的方式非常快地实现：            D是度数矩阵， 相当于除以      N(v)      如果是有权重的图， 一种方式是在mean aggregation中， 用加权平均。embedding和GNN方面最新的论文：Graph attention network            上面的方法中，每一个邻居对某个节点的“贡献”是一样的，可以看成它们都有相同的权重 α=1/      N(v)      .      attention strategy 1：用某种方式，来计算出节点v的邻居u的attention coefficient：计算出所有邻居的attention coefficient后， 节点的权重 α可以用softmax归一：这样，每一层在计算embedding的时候，聚集函数，例如mean，就是每一个邻居上一层embedding的加权平均，权重=α.如何选择函数 a？  有看到一些具体的任务，a就是直接两个向量进行点积（类似于余弦相似度，和原节点越相似，那么这个邻居节点就越重要），这个应该可以根据任务的不同，自己定义一些重要性的函数，例如相似度、距离等等。  a可以是一个参数可以通过训练调节的简单神经网络。  如果图本来就有权重，可以直接将边权作为attention coefficient，也可以在a中加入这个因素。]]></content>
      <categories>
        
          <category> social network </category>
        
          <category> graph-based machine learning </category>
        
      </categories>
      <tags>
        
          <tag> notes </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[cs224w笔记]4.网络聚类之谱分析方法]]></title>
      <url>/social%20network/graph-based%20machine%20learning/2020/03/14/cs224w4/</url>
      <content type="text"><![CDATA[这是斯坦福cs224w的课程笔记，根据老师讲课内容、ppt和助教的笔记和自己的理解整理的。 cs224w的资源在b站和youtube都有，不过没有字幕，jure老师的口音挺可爱的，听一俩节课也习惯了，不过有时候还是配合助教的笔记看清楚一些。助教的笔记： https://snap-stanford.github.io/cs224w-notes/network-methods/spectral-clustering课程网站（有ppt和作业）：http://web.stanford.edu/class/cs224w/另外， 之前没有发现原来文章的排版那么难看！稍微改了一下行距，可能会好一些orz 如果有空我会把英文部分尽量都翻译过来的Spectral ClusteringHere we study the important class of spectral methods for understanding networks on a global level. By “spectral” we mean the spectrum, or eigenvalues, of matrices derived from graphs, which will give us insight into the structure of the graphs themselves. In particular, we will explore spectral clustering algorithms, which take advantage of these tools for clustering nodes in graphs.The spectral clustering algorithms we will explore generally consist of three basic stages.  Preprocessing: construct a matrix representation of a graph, such as the adjacency matrix (but we will explore other options)  Decomposition: compute the eigenvectors and eigenvalues of the matrix, and use these to create a low-dimensional representation space  Grouping: assign points to clusters based on their representation in this space*英文内容来自助教的笔记这一节讲的是用谱分析的方法来做网络聚类。 谱分析就是研究网络邻接矩阵的特征值和特征向量， 从而获得网络结构的一些信息。 这种方法大致分为3个步骤：  预处理： 构建表示图结构的矩阵，例如邻接矩阵以及laplacian矩阵。  分解： 计算特征值和特征向量。用它们来表示节点。  聚类： 将节点根据它们在representation 空间中的情况聚类。Graph PartitioningLet’s formalize the task we would like to solve. We start out with an undirected graph G(V,E). Our goal is to partition VV into two disjoint groups A,B (so A∩B=∅ and A∪B=V) in a way that maximizes the number of connections internal to the groups and minimizes the number of connections between the two groups.To further formalize the objective, let’s introduce some terminology: 首先对问题作出定义：      Cut: how much connection there is between two disjoint sets of nodes. cut(A,B)=∑i∈A,j∈Bwij where wij is the weight of the edge between nodes i and j. 即类A和类B之间的边的总和，如果是有权重的，就是总加权和。            Minimum cut:  使得两个部分之间联系最少的分割。      Since we want to minimize the number of connections between A and B, we might decide to make the minimum cut our objective. However, we find that we end up with very unintuitive clusters this way – we can often simply set A to be a single node with very few outgoing connections, and B to be the rest of the network, to get a very small cut. What we need is a measure that also considers internal cluster connectivity.但是仅仅考虑两个分割之间的边数是不够的，例如有一个悬挂节点，那么它和网络其他部分的边数为1, 把它作为一个类，两个类就会非常不平衡。 所以，也要考虑到聚类的内部联系。用 conductance 来衡量聚类的好坏，conductance越小，越好。Enter the conductance, which balances between-group and within-group connectivity concerns. We define wherethe total (weighted) degree of the nodes in A. We can roughly think of conductance as analogous to a surface area to volume ratio: the numerator is the area of the shared surface between A and B, and the denominator measures volume while trying to ensure A and B have similar volumes. Because of this nuanced measure, picking A and B to minimize the conductance results in more balanced partitions than minimizing the cut. The challenge then becomes to efficiently find a good partition, since minimizing conductance is NP-hard.最小化conductance是一个NP-Hard问题。spectral graph partitioning      首先定义图的邻接矩阵A。 然后假设有一个vector x， x是所有顶点的label。 那么Ax = y ，yi的意义就是：    节点i 的所有邻居的label的总和。        特征值和特征向量的意义？ Ax = λx，意味着，有一种特殊的节点的labeling 方式，使得 我们把一个节点i的所有邻居的label加起来，作为这个节点的新label，这个label就只是原来分配的label的λ倍， 且这个倍数是对所有的节点都一样的。  Enter spectral graph partitioning, a method that will allow us to pin down the conductance using eigenvectors. We’ll start by introducing some basic techniques in spectral graph theory.The goal of spectral graph theory is to analyze the “spectrum” of matrices representing graphs. By spectrum we mean the set Λ={λ1,…,λn} of eigenvalues λiλi of a matrix representing a graph, in order of their magnitudes, along with their corresponding eigenvalues. For example, the largest eigenvector/eigenvalue pair for the adjacency matrix of a d-regular graph is the all-ones vector x=(1,1,…,1), with eigenvalue λ=d.可以证明， d-regular 的图中， d就是邻接矩阵最大的特征值。（1,1,…1)是它对应的唯一的特征向量。Exercise: what are some eigenvectors for a disconnected graph with two components, each component d-regular? Note that by the spectral theorem, the adjacency matrix (which is real and symmetric) has a complete spectrum of orthogonal eigenvectors.然后，如果是有两个连通部分，每个都是d-regular的，那么我们把一边标记为0，一边标为1， x=(0,0,..0,1,1,..1), Ax=lambdax， λ就是d。同样，也可以把一边标为1一边标为0.所以当有两个联通部分的时候， λn=λn-1. multiplicity=2（也就是λ的出现次数）。 当有k个联通部分，λn的multiplicity就是k。What kinds of matrices can we analyze using spectral graph theory?      The adjacency matrix: this matrix is a good starting point due to its direct relation to graph structure. It also has the important property of being symmetric, which means that it has a complete spectrum of real-valued, orthogonal eigenvectors.A的特征向量是正交的！！！所以，xn · xn-1=0, 如果图是d-regular连通图，那么xn=(1,1,1,…), 那么xn-1的元素和必须为0.    那么 xn-1中的元素会有一些&gt;0, 有一些小于0！ 它就把点集分成了两半！              Laplacian matrix ：            性质：          L 的对角线是 i的度数， 而在i，j 相连的地方，Lij=-1.      x=（1，1, …1） 那么 Lx=0.  所以， λ=λ1=0 最小的那个特征值是0.      那么，所有特征值都是非负的。      L也是实对称阵， 那么特征向量也都是正交的，并且有n个特征值。        In particular, λ2, the second smallest eigenvalue of L, is already fascinating and studying it will let us make big strides in understanding graph clustering. By the theory of Rayleigh quotients, we have that        where w1is the eigenvector corresponding to eigenvalue λ1; in other words, we minimize the objective in the subspace of vectors orthogonal to the first eigenvector in order to find the second eigenvector (remember that L is symmetric and thus has an orthogonal basis of eigenvalues).        就会变成：        这个目标的意义就是， xi - xj可以看作是节点i和节点j的label的距离，而当ijlabel 一个为正一个为负的时候，他们的距离平方就会大，如果是同一个分组的，平方和就会小。 那么这个式子就是在将节点分成两半，然后要横跨两个部分的边越少越好。（同一边的越多越好）  how to find optimal cut （Fiedler）            这个yi 只能取1或负1，是节点的label。这个目标实际上和λ2的目标很相似，只不过λ2那个式子里，xlabel是可以取任意实值的。但是，这个没办法求出精确解， 所以，有了下面的约束，并且让y可以取任意实值：因为和y的大小没有关系，所以可以约束      y      =1， 而                  就是让      A      ==      B      的一个约束。      所以， λ2 = min  f(y)而， x = arg miny f(y)  y的解就是λ2对应的特征向量。Now that we have a link between an eigenvalue of L and graph partitioning, let’s push the connection further and see if we can get rid of the hard |A|=|B| constraint – maybe there is a link between the more flexible conductance measure and λ2. Let’s rephrase conductance here in the following way: if a graph Gis partitioned into A and B where |A|≤|B| , then the conductance of the cut is defined as β=cut(A,B)/|A| . A result called the Cheeger inequality links β to λ2: in particular, where kmax is the maximum node degree in the graph. The upper bound on λ2 is most useful to us for graph partitioning, since we are trying to minimize the conductance; it says that λ2 gives us a good estimate of the conductance – we never overestimate it more than by a factor of 2! The corresponding eigenvector x is defined by xi=−1/a if i∈A and xj=1/b if i∈B ; the signs of the entries of x give us the partition assignments of each node.   λ2可以作为分割的metrics conductance 的一个良好近似，因为λ2≤ 2β， 所以估计值比真实值大最多一倍。Spectral Clustering 实做  算出图的Laplacian matrix  L  计算出L 的特征值λ和特征矩阵x  ， 第二小的那个λ2 对应的特征向量x2， 对应了每个节点的label  grouping，决定x2 从哪个值分成两个partition，一般是0K 个partition怎么办  recursively cut into 2 partitions  cluster multiple eigenvectors， 不仅是看x2， 也看x3， x4……，一个节点就会由p个值来label， 然后可以把这些在p维空间聚类。如何选择有多少个聚类？ 如何确定k？      eigengap： 两个相邻的特征值的绝对值差。    Most stable clustering is generally given by the value k that maximizes eigengap Δk  （这里的特征值是从大到小排序的，取从大到小扫描，两个相邻值相差最大的k）      Motif-Based Spectral ClusteringWhat if we want to cluster by higher-level patterns than raw edges? We can instead cluster graph motifs into “modules”. We can do everything in an analogous way. Let’s start by proposing analogous definitions for cut, volume and conductance:  motifs cut：cutM(S): 是一些节点在一个partition，而另一些节点在另一个partition中的motifs的数量。  volm(S): 是S中motif m的节点的数量。      motif conductance：      找到optimal cut 也一样是np-hard问题算法：      preprocessing： 把原来的图转变为 一个 weighted graph，边的权重= 这条边参与motif的个数。        apply spectral cluster： 根据转化后的加权图，算出它的邻接矩阵A和laplacian矩阵L。 并且求L的特征值和特征向量 λ2和x2        grouping： 对节点按x2中的值从小到大排序， 然后分别从第i个节点处分割， 每次分割算出在第i个节点分割的motif conductance， conductance 最小的那个就是近似最好的分割。  这样比直接从0分成两半要好。  这个近似也是有可证明的保障的：Again, we can prove a motif version of the Cheeger inequality to show that the motif conductance found by our algorithm is bounded above by , where is the optimal conductance.]]></content>
      <categories>
        
          <category> social network </category>
        
          <category> graph-based machine learning </category>
        
      </categories>
      <tags>
        
          <tag> notes </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[论文阅读笔记]Please Forget Where I Was Last Summer:The Privacy Risks of Public Location (Meta)Data 社交应用位置元数据的隐私暴露问题]]></title>
      <url>/paper%20reading/2020/03/01/privacy/</url>
      <content type="text"><![CDATA[论文题目：Please Forget Where I Was Last Summer:The Privacy Risks of Public Location (Meta)Data链接：Please Forget Where I Was Last Summer:The Privacy Risks of Public Location (Meta)DataAbstract这篇文章研究用户在社交网站，例如推特上的位置数据，如何以及多大程度上泄露了他们的隐私（例如敏感信息，身体状况等）。他们制作了一个LPAuditor的工具，这个工具可以解释性地评价社交位置元数据（location metadata）造成的隐私泄露。这个系统首先可以以最细的粒度发现用户的关键位置（通过鉴别他们真实的邮件地址(postal addresses). 然后，他们可以自动地进行隐私信息推断， 比如发现用户去过的和健康相关的位置、宗教场所以及夜店等。作者们还进一步探索了用户活动和暴露出来的信息失配的情况。他们发现老版本的推特app，会给用户的geo-tagged的推文加上精确的GPS 坐标（在元数据里），这种做法是会侵犯用户隐私的。而当用户对自己的数据有更多选择，例如可以选择他们要发表什么粒度的location data时，这种细粒度带GPS坐标的推文94%都不会被发表，说明用户对自己精确的位置信息是十分在意的。所以，这个LPAuditor可以给用户作为一种审核工具，告诉用户在发表这个位置信息的时候，他们的什么隐私可能会被暴露，从而让用户对自己的数据有更多选择。（location metadata指的是用户发布的推文的位置属性（并不是用户自己打上的位置tag，而是保留在系统后台的属性数据，这种数据是对后台或者API可见的。）Introduction首先讲社交网络上发布位置信息可能带来什么后果： stalking、cybercasing， cybercasing是指从用户发布的位置信息中推断出家庭住址并且推断出什么时候用户不在家，从而实施抢劫。之前的研究present了如何从用户的数据推断出用户的关键地址（家庭、工作地址等），但是却没有指出用户具体推文中的位置信息会导致的隐私泄露程度，也没有进一步探索这些推文可以进一步（自动地）推断出什么其他的敏感信息（这些研究有探索能进一步推断出的消息，但并不是用算法去自动推断，也没有实现出来）。作者present 了 LPAuditor工具。最初是一种鉴别用户家庭和工作地址（postal address的粒度）的技术，是用两级的聚类来做的，把推文进行聚类，然后把这些聚类映射到某个postal address上，这样做，对于那些用户移动带来的空间移位以及GPS错误的情况会比较鲁棒。 然后是对某个用户推文的时空特征进行分析，推断关键位置。在这个任务中， 数据集真值是手动构建的。然后他们研究那些会暴露用户在某些敏感地点的推文。这里的敏感位置指的是和三个方面有关的位置： 医疗健康、宗教、性/夜生活。 他们发现，有71%的用户在敏感地点发布过推文， 27.5%的推文可以从内容中推断出他们处于敏感位置。 当用户的推文中包含了更多的情境时，位置元信息就会泄露更多的隐私，例如用户推文中提出她在见医生而位置元信息显示在一个堕胎诊所。 不仅是推文内容，单单从分析时空特征的模型，也可以发现29.5%的用户正在敏感位置。注意到这种隐私侵犯对用户是不可见的，因为GPS坐标这种元数据只能在推特后台或者用推特API 得到，而且历史数据会一直对API可见，当用户的关键位置被成功精准发现时，这种隐私政策带来的隐私暴露会增加15倍。（这篇文章的写作真是长句好多……看起来太费劲了……）SYSTEM OVERVIEW1. Data labeling and Clustering1.1 labeling tweets：首先要把每一条推文的GPS坐标与一个postal address对应起来。这个可以用公共的API，使用ArcGIS和Google Maps Geocoding因为数据集很大，调用API是比较不经济的。所以提出了一种caching机制，就是每当要label一个坐标时，查找附近有没有已经label好的，如果两个坐标距离小于二米，就判定他们属于两个地址。（这样会不会引入误差呀？例如在街上两个店铺有可能距离会小于两米的（聚类的时候有矫正误差））这种方法将调用次数减少到42.5%但是有一些地方API是给不出地址的。这些地方主要是大学校园、机场或者是偏远山区等。把这些label为unknown address，并不影响判断的精度。1.2 initial clustering首先是把属于同一个postal address的推文聚到一类，计算这个聚类的中心坐标，作为该postal address的中心坐标。但是，caching机制会引入一些误差，所以作者比较了Google API返回的中心坐标和聚类得到的，如果不一样，那就采用google的作为中心坐标。但这种矫正只是对每个用户最大的那10个聚类做而已，其他的不重要。对于那些标记为unknown的，使用DBSCAN算法。DBSCAN算法：转自：DBSCAN密度聚类算法一种基于密度的聚类算法，基于密度的聚类算法一般假定类别可以通过样本分布的紧密程度决定，如果是同一类别的样本，他们之间是紧密相连的，就是说在一个类别里的样本，周围不远处一定有同类样本存在。假设我的样本集是D=(x1,x2,…,xm)则DBSCAN具体的密度描述定义如下：            1） ϵ-邻域：对于xj∈D，其ϵ-邻域包含样本集D中与xj的距离不大于ϵ的子样本集，即Nϵ(xj)={xi∈D      distance(xi,xj)≤ϵ} 这个子样本集的个数记为      Nϵ(xj)                  2) 核心对象：对于任一样本xj∈Dxj∈D，如果其ϵϵ-邻域对应的Nϵ(xj)Nϵ(xj)至少包含MinPts个样本，即如果      Nϵ(xj)      ≥MinPts，则xj是核心对象。      3）密度直达：如果xi位于xj的ϵ-邻域中，且xjxj是核心对象，则称xi由xj密度直达。注意反之不一定成立，即此时不能说xj由xi密度直达, 除非且xi也是核心对象。4）密度可达：对于xixi和xjxj,如果存在样本样本序列p1,p2,…,pT满足p1=xi,pT=xj 且pt+1由pt密度直达，则称xj由xi密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本p1,p2,…,pT−1均为核心对象，因为只有核心对象才能使其他样本密度直达。注意密度可达也不满足对称性，这个可以由密度直达的不对称性得出。5）密度相连：对于xi和xj,如果存在核心对象样本xk，使xi和xj均由xk密度可达，则称xi和xj密度相连。注意密度相连关系是满足对称性的。DBSCAN算法是由密度可达关系导出最大密度相连的样本集合，就是一个聚类。这个DBSCAN的簇里面可以有一个或者多个核心对象。如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的ϵ-邻域里；如果有多个核心对象，则簇里的任意一个核心对象的ϵ-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的ϵ-邻域里所有的样本的集合组成的一个DBSCAN聚类簇。算法过程：  确定好参数ε和MinPts，初始化核心对象集合为空集，初始化聚类簇数k=0，为访问样本集等于全集，簇划分C=空集  对于每一个样本点：          通过规定的距离公式，找到样本点的ε-领域子样本集      如果子样本集中个数满足MinPts限制，它就是核心对象样本，加入核心对象样本集合。        如果核心对象集合是空集，那算法就结束了，所有的点都是噪点或者单独一个聚类  在核心对象集合中，随机选择一个O作为初始，当前簇的核心对象队列Ωcur={o} ,初始化类别序号k=k+1， 初始化当前簇样本集合Ck={O}， 未访问样本集合更新。5）如果当前簇核心对象队列Ωcur=∅，则当前聚类簇Ck生成完毕, 更新簇划分C={C1,C2,…,Ck}, 更新核心对象集合Ω=Ω−Ck， 转入步骤3。否则更新核心对象集合Ω=Ω−Ck。6）在当前簇核心对象队列Ωcur中取出一个核心对象o′,通过邻域距离阈值ϵ找出所有的ϵ-邻域子样本集Nϵ(o′)，令Δ=Nϵ(o′)∩Γ 更新当前簇样本集合Ck=Ck∪Δ, 更新未访问样本集合Γ=Γ−ΔΓ=Γ−Δ, 更新Ωcur=Ωcur∪(Δ∩Ω)−o′，转入步骤5.输出结果为： 簇划分C={C1,C2,…,Ck}简单总结就是，先找出所有核心对象，然后初始随机选择一个，它的邻域中的样本全部加入这个聚类，其中的核心样本加入队列，然后每次从队列中取出一个样本，重复上述过程，直到队列为空，这样就得到一个簇聚。 得到之后，再随机选择下一个核心对象作为另一个聚类的初始样本，再继续。在这篇文章中，作者将距离阈值设置为30米。Second-level clustering第一次聚类之后，就会发现在同一个地点范围之内，会出现不止一个聚类，通常都是一个大的聚类，然后旁边有一些非常小的，而且非常靠近的聚类。作者把这些坐标在地图上标出来，但也很难确定他们属于哪个地址，这是由多种因素造成的： 1. GPS读数不精确；2. API也有一定误差； 3. 与用户在发送推文时候的位置有关，例如用户正在离开或到达那个地方，或者在后院或隔壁。使用改进的DBSCAN方法来把这些小的聚类和大聚类合并，原则是距离不能超过50m。因为DBSCAN是会把密度相连的样本作为一类，也就是级联效应（cascading effect），改进就是在加入聚类的时候都要check这个聚类和大聚类的距离是不是50m以内。（这个2次聚类是说对第一次的DBSCAN聚出来那些unknown address 的类再聚类，还是那些精确的地址也要加进来呢？）2. Identifying Key User Locations在鉴别哪些聚类是用户的关键位置时，没有用到推文中的内容和语境信息，而是单纯用时序特征， 而这是基于人类社会活动的一般规则，例如8小时上班制，但对于那些上班时间很不规律的地区就不在研究范围之内了。 而之所以不用推文内容，是要强调隐私泄露的风险： 即使你很小心，在推文内容中避免了关键位置信息的透露，你的GPS坐标也会出卖你。Home基于两个inituitions：  在这个鉴定为home address 的聚类中，要有比较多的推文，因为用户每天都会在家一段时间。  在别的地方可能会有非常有规律的时序模式，但是在家发推文的时间就会比较分散，从很早到很晚都可能。而在工作日的时候，这些特征可能会和其他地点的活动混淆，只在weekends观察会更鲁棒。他们的方法是首先找到5个周末里面推文最多的聚类，然后估计了time frame 和active hours， 选择其中time frame最长的。work在工作地点发推特在时间上比较有规律。首先也是找5个推文最多的聚类，去掉已经标记为home的，然后对每个聚类， 看发推文超过一条的那些天，将第一条和最后一条之间的间隔作为time frame， 然后把这些time frame叠加起来，如果某个time frame在这个聚类超过一半的日期里都有表现，就将这个time frame作为prominent time frame。 这样可以消去一些噪音，例如用户某天加班了，也可以考虑到一些几班倒的用户。然后，将dominant time frame以外的推文删掉；并且去掉time frame经常超过10小时的聚类（这是根据US法定工作时间和欧盟一周48小时限制）。不过考虑到有些人就是会加班，所以只是把超过20%time frame大于10小时的聚类去掉。可以看到，这个算法比起传统的固定工作时间的算法要灵活许多。3. Identifying Highly-Sensitive Places任务： 鉴别用户的Potentially Sensitive Clusters（PSCs），这些聚类附近有比较敏感的场所，并且确定用户是否去过这些场所。关于聚类附近是否有敏感场所，使用Foursquare‘s venue API来获取聚类附近场所的信息，附近指的是聚类中心坐标25米范围内。Content-based：聚类离敏感场所近不代表用户去过那些敏感场所，需要推文内容佐证。佐证使用的是一个手动构建的wordlist，包括医疗、宗教、sex/nightlife的关键词。首先对推文进行预处理：去掉emoji、mentions、停顿词、url，tokanization、lemmatization。然后使用tf-idf方法（term frequency - inverse document frequency）来统计关键词频率。tf-idf：一个挺简单的算法：机器学习：生动理解TF-IDF算法在这里，作者是将这个聚类的所有推文作为一个document，而将用户的所有聚类作为corpus。tf-idf会输出对于一个文档的关键词排名，所以只要检查这个聚类分数最高的前几个单词是否在wordlist中，可以决定推文context是否与sensitive venue有关。Duration-Based corroboration：用户可能因为地点的敏感性不会在推文中发关键词，所以还用了停留时间作为依据。如果在一个聚类中，相当一段时间内用户发了多条推文，说明用户在该地点花了一段时间，就有可能访问了敏感地点。 但这种判断不confidence。4. Implementation Details这套LPAuditor工具全用Python实现，用到的数据放在Mongo db中，这一部分讲的是作者具体都用到什么工具和包，这些工具对社交分析都挺有用的，感兴趣的话可以阅读原论文。]]></content>
      <categories>
        
          <category> Paper Reading </category>
        
      </categories>
      <tags>
        
          <tag> privacy </tag>
        
          <tag> LSBN </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[论文阅读笔记]假新闻检测 dEFEND: Explainable Fake News Detection]]></title>
      <url>/paper%20reading/2020/02/19/defend/</url>
      <content type="text"><![CDATA[论文： dEFEND: Explainable Fake News DetectionKDD2019：            [KDD 2019      dEFEND: Explainable Fake News Detection](https://www.kdd.org/kdd2019/accepted-papers/view/defend-explainable-fake-news-detection)      ABSTRACT本文研究的问题是假新闻检测，假新闻检测可以看作一个分类问题，它和谣言分类、fact check、垃圾内容挖掘等比较相似，都属于内容质量检测的领域。假新闻检测的研究方向主要有三个：  数据集： 假新闻标准数据集需要人工去构建。  特征方面的工作： 假新闻检测的方法可以分为基于内容和基于社交网络，这些方法都涉及到新闻文本的embedding、用户的embedding、用户之间社交网络的embedding如何获取的问题。  模型研究。本文提出，虽然之前的工作在假新闻检测上获得了比较准确的结果，但是这些模型都不是可解释的，不能告诉我们为什么判定这篇新闻是假新闻。他们提出了一个融合模型，根据新闻内容和用户评论来检测，同时能够筛选出Top-k 用户评论来解释为什么这条新闻是假的。作者在真实数据集上做实验，结果比7个模型f1-score高出5.33%，精确度大幅提升。Introduction这一部分主要讲了为什么假新闻检测很重要，现在新闻的传播有什么样的特点， 假新闻检测的困难在哪里，作者做出了什么样的贡献。首先随着社交网络的用户大幅增长，人们获取新闻的渠道改变，2018年已经有68%的美国成年人从社交网络上获取新闻。但社交媒体上的创作形式和传播速度让用户每天接触到大量的错误信息和虚假信息，广泛传播的假新闻会损害公众对政府、媒体的信任，改变人们对真新闻的看法，还可能对现实世界造成伤害。假新闻检测的困难在于：  因为假新闻本来就是故意写来误导读者的，它在内容上可能逻辑自洽，语气客观，仅仅基于内容很难判定它的真伪。  不能只基于内容，要结合用户评论（或者有一些模型也研究了传播特性（网络结构），以及多条新闻下多个评论用户之间的社交关系、用户的特征等）， 但社交网络数据非常庞大，有很多用户是匿名的，有很多机器人账号，这导致评论和转发中有较多的噪声。近年来比较优秀的一篇是natali他们提出来的，他们综合了新闻内容，用户评论，回复的用户的特征和关系网络来检测假新闻。也有用attention机制的，在大量的用户回复中筛选出那些重要的回复。作者从另一个角度提出研究课题，即在检测的同时还要解释为什么它是假新闻。这么做的好处首先是对特征研究有贡献；然后这么做有利于从噪声中分离对判断真假有用的信息，从而提高判断的准确度。什么叫做解释为什么？ 作者是从两个角度去解决的，解释可以从新闻内容中得到，也可以从用户评论中得到。  新闻内容中有一些信息是可以验证真伪的，例如涉及科学知识、人文历史等等，记者可以到一些第三方的事实鉴定网站去鉴定，但对于新的事件知识库里还没有信息，就没法检查。  用户评论含有大量信息，如立场、情感、观点，可以帮助检测  新闻内容和用户评论会有联动，用户评论可能针对新闻中某句话/某个观点提出质疑，也可以解释一条新闻为什么被判定真假。从这两个角度去挖掘原因，作者提出的框架包括3个部分：  编码新闻内容的模块， 具体是采用一个层级注意力网络。  编码用户评论的模块，用的是word-level注意力子网络。  sentence-comment co-attention component，用来捕捉新闻内容和用户评论之间的关系，以及选出能够解释检测结果的top-k条用户评论和新闻句子。这篇文章强调了以下的挑战：  如何同时提高检测准确度和可解释性  如何提取解释性的句子和评论，在没有真值的情况下  如何对新闻内容和用户评论之间的关系建模。总结他们的贡献，有三个方面。一是他们提出了基于社交网络的假新闻检测领域的新问题，即如何解释检测结果；二是针对这个问题提出了一个模型，三是在真实数据集上测量模型的准确性和可解释性。Related work关于假新闻检测：一文看懂虚假新闻检测（附数据集 &amp; 论文推荐）新闻检测分为基于新闻内容的和基于社交环境的。基于新闻内容的模型可以从文本和视觉元素提取特征。文本特征可以包括写作风格、情感等，来判断新闻写作是否中立客观；视觉特征从配图、视频中提取。基于社交环境的方法，可以包括用户、推文、社交网络三个方面的特征，用户特征是指从用户的资料来描述用户，推文是指根据用户以往的评论、推文来检测他们的立场、可信度。网络特征指的是对该新闻的传播模式、用户之间的互动关系等进行建模。可解释的机器学习机器学习模型的可解释性也是研究的热点。可以分为两种： intrinsic 和post-hoc explainability。前者是把可解释性融合进模型结构本身，例如用权重的大小来寻找重要特征；后者是创建另外一个模型来对一个已有的模型进行解释，比如说用一个决策树，训练到能和一个神经网络产生一样的结果。这篇paper用层级注意力机制就是intrinsic的方法。问题定义A 是一篇文章， 有N个句子si每个句子有Mi个单词，记为 w1，……wMiC={ c1……cT} 是T条评论，每条评论有w1……wQj 个单词。新闻检测问题看作是二分类问题，要学习的是分类，以及该新闻所有的T条评论的一个排序，和该新闻所有N个句子的一个排序。排序的一句是解释能力。新闻句子的解释能力指的是这个句子的信息有多check-worthy（我理解的check-worthy指的是检查有多容易以及这个句子信息的真假对模型  新闻内容编码  用户评论编码  sentence-comment co-attention  predictionNews contents encoding使用的是word-sentence level的注意力机制。最近的研究喜欢用层级注意力网络来表示文本，就像上一篇新闻推荐的论文，这种网络可以根据目标的不同更好地选择特征。      word encoder： encoder用的是双向GRU RNN。每一个单词用相应正向和反向两个单元的隐状态共同表示，包含了这个单词的上下文信息（而不是只有上文）。在这之上就是一个常规的attention 网络，权重α描述了每一个单词对该句子表示的贡献。一个句子向量vi就是单词表示的加权和。                sentence encoder： 也是用双向 GRU RNN。将该新闻的N个句子向量作为输入，每个句子的表示si用正向和反向的隐状态连接。      User Comments Encoding因为评论比较短所以就直接用word attention了。这个embedding首先是直接将每个词映射到一个embedding 矩阵的一个向量，embedding的维度是D维。然后将评论的所有单词embedding输入一个双向GRU RNN, 然后还是用相似的方法， 把隐状态加权和得到comment vector cj。Sentence-Comment Co-attention  affinity matrix： also called similarity matrix。矩阵来衡量x和y轴数据点的相似度，常用的有余弦距离。  attention map： 是一维的attention 机制的扩展。二维的attention map是一个数值矩阵，每一个数值衡量该坐标数据点对特点目标的重要程度：attention map: a scalar matrix representing the relative importance of layer activations at different 2D spatial locations with respect to the target task。  首先构建相似度矩阵：        用相似度矩阵来进一步构建二维attention 网络的权重。        其中，as是所有N个句子的权重， ac是所有T条评论的权重。最后，句子加权和和评论加权和分别作为最终该新闻句子的表示和评论的表示。        没看懂这个模块的数学含义，comment和sentence在事实上的相似度如何影响attention权重的？F矩阵中有w参数需要训练，但这个训练过程怎么能让F真的就代表comment和sentence之间的相似度呢？还有看起来F越接近0应该是相似度越低的，这样Hs直接=tanh（WsS）而与C无关。我们的目标函数真的能让F的参数往这个方向去优化吗？  Prediction      预测函数：        这里y head是个二维向量，表示y预测为0或1的概率。        目标函数：      模型使用RMSprop作为优化器。实验实验探究3个问题：  defend模型能否提高假新闻检测的表现？  能提高分类表现，那么新闻内容和用户评论，分别对提高的贡献有多大？  defend能找到可以解释分类结果的句子和用户评论吗？数据集PolitiFact用了一个新闻检测benchmark: FakeNewsNew. 里面的数据是从两个fact-checking平台 GossipCop和PoilitiFact获取的，PolitiFact是一个非营利性的事实鉴定平台，由用户上传statement，由调查记者调查并给出statement鉴定和调查报告。鉴定不是简单的真或者假，而是分了好几个级别。作者过滤了不足3条评论的新闻。比较方法对比了以往七种方法，其中前两种对新闻内容的语言学特征提取和建模，还利用了心理学的知识（没具体看），HAN用层级注意力网络，但是只基于新闻内容；text-CNN用CNN去编码新闻内容。TCNN-URG用卷积网络学习新闻内容特征和用户评论特征。HPA不是基于新闻内容，反而从评论用户去获取该新闻的表示，也用了层级注意力网络。CSI则是混合方法，基于新闻内容、用户评论文本、用户特征来检测，是2017年提出之后很受关注的方法。这个实验针对问题选取了三种不同的比较组，选取的逻辑非常清晰。DEFEND分类表现  用常规accuracy precision recall 和 f1来衡量。  RST\LIWC\HAN： 这三个都是只基于新闻内容的传统方法，HAN各项评分都高很多，说明层级注意力网络编码特征的能力最强； LIWC优于RST，说明语言特征对假新闻检测很有帮助。  融合用户评论的方法比单纯使用新闻内容或单纯使用评论的方法性能好。说明用户评论的确包含了新闻真伪的补充信息。  只用用户信息的方法比只用新闻内容的方法稍微好一些。量化模型中user comment/新闻内容对检测的影响有几个defend变种：  dEDEND\C： 去掉用户评论，编码新闻内容之后，直接pooling和softmax  defend、N： 不考虑新闻内容。  defend\Co: 不用co-attention，而是在新闻内容和用户评论上分别做sentence级别的self-attention。结果：作者的结论是： co=attention、新闻内容、用户评论都会有贡献。然而，他们没有提到 新闻内容和用户评论在两个数据集上的作用很不一样，且po上F1比Accuracy高，Gossip上相反，这是为什么。他们也没有对这两个数据集做基本的介绍。解释性和case study对比的方法是HAN 和 HPA。  sentence： 使用工具ClaimBuster，这个工具是一个对claim的check-worthy程度进行打分的模型，是用竞选辩论数据集去训练的，label是人们手工标注的。和HAN对比，使用MAP（mean average precisionk）。 （这里对准确对应的ground-truth，应该是把新闻输入claim buster去标注的）。 结果是defend好于HAN，好于Random  comments： comments解释性的评价就更厉害了……他们选了50篇文章（去掉不足50个词的，超过500个单词就截短），然后雇佣了AMT众包工人来评估每篇文章选出来的评论TOPK LIST。第一个任务是在DEFEND 和 HPA 得到的两个rank list（用attention 权重从大到小排列选出来的）之间进行ABtest。这个任务用三个角度来评估：  rc1和rc2投票人数比  rc1和rc2获胜的新闻条数比。  每条新闻，保证会被3个worker投票，每条新闻的比分结果是rc1票数远比rc2多，在新闻比分上，3：0和2：1加起来大于60%。第二个任务，是让工人对list里面的每一条评论进行0-4解释性打分。评价标准是NDCG和Precision，这里Precision指的是在rank list里面的条目是相关条目的比例。Case Study里面确实highlight了非常有价值的用户评论，且和文本有很强对应；有用的评论也比无关信息、有太强主观性的评论得分高。Future work  结合fact-checking 网站上列出来的调查结论，可能可以更好地highlight新闻句子和用户评论  结合更多的用户信息，例如评论获得点赞数，也可能更好帮助筛选重要评论]]></content>
      <categories>
        
          <category> Paper Reading </category>
        
      </categories>
      <tags>
        
          <tag> news </tag>
        
          <tag> attention </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MIT 6.828 JOS操作系统实验6-文件系统实现详解]]></title>
      <url>/lab%20report/2020/02/03/OS6filesystem/</url>
      <content type="text"><![CDATA[Lab6：文件系统实现JOS操作系统提供了一个简单的文件系统，它可以满足基本功能：创建，读取，写入，删除和以分层目录结构来组织文件。但并未提供文件所有权、用户权限的概念，不支持硬链接、符号链接、时间戳或特殊设备文件等，它提供的保护仅能捕获错误。磁盘上的文件系统结构文件储存在磁盘上，磁盘最小的存储单位为扇区，JOS采用的扇区大小为512字节。为了提高数据交换的效率，与磁盘的一次数据交换以块为单位，不同的操作系统定义不同的块大小，具体而言，更大地块管理的开销越小，但是可能出现的内部碎片越多。JOS采用的块大小为4096bytes，与内存的页大小相同。块和扇区的大小定义在inc/fs.h和fs/fs.h中：#define BLKSIZE		PGSIZE#define BLKBITSIZE	(BLKSIZE * 8)#define SECTSIZE	512			// bytes per disk sector#define BLKSECTS	(BLKSIZE / SECTSIZE)	// sectors per block大多数UNIX文件系统采用索引结构来组织文件的块，为每一个文件分配一个索引节点（inode），索引节点上保存了文件的重要元数据，包括文件名称，大小，创建者，创建日期，权限，以及文件的数据块存放的物理位置。inode号码而非文件名唯一地标识了系统中的一个文件。一般情况下，文件全名与inode号码是意义对应的关系，但UNIX系统允许硬链接，也就是多个文件名指向同一个inode，可以用不同的文件名访问同一个文件，不同的文件名没有依赖关系。当删除一个文件名时，不会影响到其他文件名对该inode的访问，只有在链接数为0的时候，该inode才会被真正释放。由于JOS不实现硬链接或者符号链接，因此目录之间是没办法共享文件的，所以不需要采用inode结构，直接将文件的元数据存放在目录条目中。超级块在一个UNIX系统中，一般会存在许多种文件系统，例如ext4，NFS, tmpfs等等，他们有着自己的组织方法，权限设置，文件数据块大小定义等，因此文件的创建、打开、删除等具体操作都是不同的。为了让用户透明地处理文件，操作系统引入了虚拟文件系统，封装不同文件系统的文件操作，为用户提供统一的文件操作接口。用户访问文件系统的过程：这样，不同的文件系统必须提供描述该系统的元数据，例如块大小，总磁盘大小，根目录，块设备驱动程序描述符指针，上次安装文件系统的时间，上次检查文件系统错误的时间等等，这些元数据存放在超级块 中。超级块存放在磁盘的最开始或者结尾，并且为了可靠性会在不同区域中备份。JOS的超级块数据结构定义在inc\fs.h中：struct Super {	uint32_t s_magic;		// Magic number: FS_MAGIC	uint32_t s_nblocks;		// Total number of blocks on disk	struct File s_root;		// Root directory node};包括文件系统魔术字、总共有多少块和根目录节点。文件元数据struct File描述了文件的元数据：struct File {	char f_name[MAXNAMELEN];	// filename	off_t f_size;			// file size in bytes	uint32_t f_type;		// file type	// Block pointers.	// A block is allocated iff its value is != 0.	uint32_t f_direct[NDIRECT];	// direct blocks	uint32_t f_indirect;		// indirect block	// Pad out to 256 bytes; must do arithmetic in case we're compiling	// fsformat on a 64-bit machine.	uint8_t f_pad[256 - MAXNAMELEN - 8 - 4*NDIRECT - 4];} __attribute__((packed));	// required only on some 64-bit machines  f_name: 文件名，MAXNAMELEN定义为128字节。  f_size: 以字节为单位的文件大小  f_type： 文件类型（用来区分是普通文件还是目录）  f_direct:  直接索引块，NDIRECT被定义为10。  f_indirect: 间接索引块，只有1块。  f_pad:  这是元数据结构的padding，填充使得File结构的大小为256字节。元数据存储在磁盘上的目录条目中。文件数据块使用索引来组织，每一条索引存放着对应文件数据块的物理地址（块号）。直接索引块有10个，意味着大小不超过10 *4096=40KB的文件可以直接映射。间接索引块只有一个，可以存放4096/4 = 1024个额外的直接索引块，因此可以提供1024*4096 =4MB的空间。文件系统允许的文件大小为4MB+40KB，1034块。目录文件目录实际上也是一个文件，只是其中存放的是每个文件的元数据（File结构体），描述其中的文件和子目录。根目录的元数据保存在超级块中。根目录文件存放了一系列子目录和文件的File元数据，虚拟文件系统可以根据超级块找到根目录的元数据，从而访问到根目录文件的每一块，进而再通过元数据找到具体文件。磁盘空间管理如上面的结构图所示，JOS用bitmap来管理磁盘的空闲块。在创建文件或文件内容需要扩展时，JOS根据bitmap找到空闲的磁盘块并分配给它；当文件空间释放时，文件系统将bitmap中对应的块置为1，表示该块已经空闲。Bitmap的管理相对于空闲块链表来说更加高效，但是也会占据较大空间。因为文件系统最多只能管理3GB的磁盘，则最多有786，432块，需要98，304字节的bitmap，即24个bitmap块。访问磁盘文件系统的实现需要访问磁盘。一般的UNIX系统中，因为有许多异质的IO设备，因此要将磁盘驱动程序安装为内核的一部分，并提供系统调用使得文件系统可以访问磁盘。在JOS中，为了方便，直接将磁盘驱动程序实现在文件系统的用户级环境中，使得文件系统可以直接访问磁盘。JOS中程序与磁盘数据交换的方式是轮询， 即基于编程IO的磁盘访问。本次实验中，我们要实现与探究的问题有：  文件系统中，文件逻辑块是如何映射到物理块上的  文件创建或扩展时，如何分配磁盘块  需要访问文件时，如何将文件数据从磁盘上读取到内存缓冲区中  如何把内存中的文件数据写回到磁盘  如何为用户提供读、写、创建、删除的接口TODO1:  i386_init通过将ENV_TYPE_FS类型传递给环境创建函数env_create来标识文件系统环境。在env.c中修改env_create，使其授予文件系统环境I/O权限，但不要将该权限授予其他环境。JOS中，环境实际上与用户进程类似， 一个环境包括环境的状态（RUNNING \RUNNABLE等），环境的地址空间，环境的父级id，中断帧等，与进程相似。可以通过系统调用fork来创建用户环境，当发生中断时，保存用户环境的运行状态到中断帧中，然后进入内核处理中断。环境描述还包括环境的类型，只有两种——ENV_TYPE_USER和ENV_TYPE_FS，前者为普通的用户进程，后者为文件系统环境。系统开启时，在init.c的i386_init()中会创建文件系统进程：	...	boot_aps();		//将初始化代码拷贝到MPENTRY_PADDR处，然后依次启动所有AP	// Start fs.	ENV_CREATE(fs_fs, ENV_TYPE_FS);x86处理器使用EFLAGS寄存器中的IOPL位来确定是否允许保护模式代码执行特殊的设备I / O指令，例如IN和OUT指令。JOS的磁盘寄存器位于x86的IO空间中，因此我们需要赋予文件系统环境IO权限，但不允许其他任何环境，包括内核访问磁盘。一个进程只能使用POPF指令来更改IOPL，但是这条指令的执行也需要内核级特权，文件系统环境并不具有内核级特权。所以，需要在运行于内核模式的env_create中，判断环境类型是否为文件系统，然后手动修改该环境下的EFLAGS寄存器。代码为：voidenv_create(uint8_t *binary, enum EnvType type){	// LAB 3: Your code here.	struct Env *e;	int r;	if ((r = env_alloc(&amp;e, 0) != 0)) {		panic("create env failed\n");	}	// If this is the file server (type == ENV_TYPE_FS) give it I/O privileges.	// LAB 5: Your code here.	if(type == ENV_TYPE_FS)		e-&gt;env_tf.tf_eflags |= FL_IOPL_MASK;	load_icode(e, binary);	e-&gt;env_type = type;}其中，FL_IOPL_MASK在mmu.h中定义。eflags寄存器中的第12和13位为IO特权级别位（IOPL），从0-3分别对应4种模式：内核、驱动程序、驱动程序、应用程序。当前的进程只有当权限位&lt;=IOPL时，才可以获得访问IO的权限， 否则就会引发保护异常。e-&gt;env_tf.tf_flags|=FL_IOPL_MASK将EFLAGS寄存器置为0x3000，即IOPL=3，也就是该进程的用户模式可访问IO, 于是文件系统便获得IO权限，而对于其他用户环境，eflags中的IOPL位仍为0. 因为每一个进程都有自己独立的标志寄存器，所以用户环境与文件系统环境具有不同的IOPL。  Env 数据结构为环境描述符，类似于PCB，在inc\env.h中定义：  struct Env {	struct Trapframe env_tf;	// Saved registers	struct Env *env_link;		// Next free Env	envid_t env_id;			// Unique environment identifier	envid_t env_parent_id;		// env_id of this env's parent	enum EnvType env_type;		// Indicates special system environments	unsigned env_status;		// Status of the environment	uint32_t env_runs;		// Number of times environment has run	int env_cpunum;			// The CPU that the env is running on	// Address space	pde_t *env_pgdir;		// Kernel virtual address of page dir	// Exception handling	void *env_pgfault_upcall;	// Page fault upcall entry point	// Lab 4 IPC	bool env_ipc_recving;		// Env is blocked receiving	void *env_ipc_dstva;		// VA at which to map received page	uint32_t env_ipc_value;		// Data value sent to us	envid_t env_ipc_from;		// envid of the sender	int env_ipc_perm;		// Perm of page mapping received};    其中 env_tf为中断帧，中断帧数据结构Trapframe与之前的实验类似，tf_eflags为进程的EFLAGS寄存器。###问题：在环境切换的时候是否需要其他操作来确保IOPL正确设置和还原？不需要。因为在环境切换的时候，操作系统会保存旧的中断帧在内核栈上，其中就包括了EFLAGS寄存器。在环境切换回来的时候，会从内核栈上加载中断帧，恢复原来的EFLAGS寄存器值。检查运行./grade-lab5进行检查：fs i/o结果为OK, 说明文件系统已有IO权限。TODO2:  ​    在fs / bc.c中实现bc_pgfault和flush_block函数。 bc_pgfault是一个页面错误处理程序，bc_pgfault的工作是从磁盘加载页面，flush_block函数应将一个块写出到磁盘。为了解决磁盘与CPU处理数据速度的不同与数据大小的不同，通常要在两者之间加入缓冲区。缓冲区实际上是内存上的一块特定区域。JOS的缓冲区只有一个块， 代码在fs\bc.c中。bc_pgfault()文件系统在JOS中本质上是一个进程，它如何提供其他进程访问文件的接口呢？内核将磁盘的物理地址映射到文件系统进程的高虚拟地址空间。在fs.h中，磁盘映射的虚拟地址和磁盘大小的定义是：/* Disk block n, when in memory, is mapped into the file system * server's address space at DISKMAP + (n*BLKSIZE). */#define DISKMAP		0x10000000/* Maximum disk size we can handle (3GB) */#define DISKSIZE	0xC0000000意味着该文件系统只能处理最多3GB的磁盘。磁盘地址被映射到文件系统进程的高地址3GB区域，从0x10000000开始。这样，磁盘地址映射到文件系统虚拟地址，而虚拟地址又在被访问时映射到内存的物理地址，当其他进程向文件系统请求读取文件时，文件系统访问虚拟地址空间来读取相应的块。对于文件系统，磁盘块的读取和普通进程访问代码和数据时按需调页的策略类似，是按需加载的。只有当某个磁盘块被需要时，才要将其加载进内存。当文件系统进程访问某个虚拟地址时，如果对应的磁盘块还不在内存中，就会发生缺页故障，这时应该将虚拟地址对应的磁盘块读入内存，这与普通用户的页错误处理不同，需要自定义页错误处理程序。bc_pgfault函数将磁盘块读取到对应的内存，才可以重新执行该访问。  正常情况下，用户环境运行在JOS分配给用户的正常栈上。但当用户模式下发生页错误时，JOS内核将从正常用户栈切换到用户异常栈，来运行用户级页面错误处理程序。在异常栈上，有UTrapframe结构体，为异常中断帧，它保存了引起页错误的虚拟地址。  处理用户态页面异常的过程是：      发生异常之前，用户已经向内核注册自定义的页面处理程序在env_pgfault_upcall中。    用户态发生页错误，陷入内核态，保存正常的中断帧Trapframe，切换到内核栈，进入trap（）    根据中断号发现是缺页故障，调用page_fault_handler进行处理    检测trapframe上保存的cs寄存器，发现是用户态    判断是否有用户自定义页面异常处理程序，若没有则销毁进程。    如果有，压入Utrapframe，并设置tf-&gt;tf_eip即下一条指令为用户自定义的缺页处理程序，返回用户态    用户态会从自定义处理程序开始执行。    文件系统进程自定义了它的缺页处理程序为bc_pgfaultbc_pgfault首先对访问的虚拟地址进行了合法性检查：static voidbc_pgfault(struct UTrapframe *utf){	void *addr = (void *) utf-&gt;utf_fault_va;	uint32_t blockno = ((uint32_t)addr - DISKMAP) / BLKSIZE;	int r;	// Check that the fault was within the block cache region	if (addr &lt; (void*)DISKMAP || addr &gt;= (void*)(DISKMAP + DISKSIZE))		panic("page fault in FS: eip %08x, va %08x, err %04x",		      utf-&gt;utf_eip, addr, utf-&gt;utf_err);	// Sanity check the block number.	if (super &amp;&amp; blockno &gt;= super-&gt;s_nblocks)		panic("reading non-existent block %08x\n", blockno);虚拟地址必须在磁盘映射的虚拟地址范围之内，并且由于文件系统所占有的磁盘空间可能比3GB要小，所以要检查对应的磁盘块号是否在文件系统的范围内（blockno &lt; super-&gt;s_nblocks).接下来，由于addr不一定与页大小对齐，所以要先将它与PGSIZE对齐：addr = ROUNDDOWN(addr, PGSIZE);然后将磁盘块读入内存，则首先要在内存中分配一个页。JOS的syscall.c中实现了分配内存页的系统调用sys_page_alloc, 它的接口是：static intsys_page_alloc(envid_t envid, void *va, int perm)它为id为envid的环境在内存中分配一个页，并将页首地址映射到虚拟地址va, 设置这个页的权限为perm。（与上次实验一样，PTE_U表示用户可读，PTE_W表示可写，PTE_P表示存在内存中）。我们知道init.c中，文件系统环境是第一个被create出来的进程：	// Starting non-boot CPUs	boot_aps();		//将初始化代码拷贝到MPENTRY_PADDR处，然后依次启动所有AP	// Start fs.	ENV_CREATE(fs_fs, ENV_TYPE_FS);#if defined(TEST)	// Don't touch -- used by grading script!	ENV_CREATE(TEST, ENV_TYPE_USER);它的envid为0.所以可以用：sys_page_alloc(0, addr, PTE_W|PTE_U|PTE_P)来分配页。文件系统对这个页的权限当然是可读可写。如果内存大小不够的话，sys_page_alloc会返回E_NO_MEM错误，否则返回0. 这里加以判断，如果内存不足抛出panic。ide.c磁盘驱动程序提供了读取磁盘的接口：intide_read(uint32_t secno, void *dst, size_t nsecs){	int r;	assert(nsecs &lt;= 256);	ide_wait_ready(0);	outb(0x1F2, nsecs);	outb(0x1F3, secno &amp; 0xFF);	outb(0x1F4, (secno &gt;&gt; 8) &amp; 0xFF);	outb(0x1F5, (secno &gt;&gt; 16) &amp; 0xFF);	outb(0x1F6, 0xE0 | ((diskno&amp;1)&lt;&lt;4) | ((secno&gt;&gt;24)&amp;0x0F));	outb(0x1F7, 0x20);	// CMD 0x20 means read sector	for (; nsecs &gt; 0; nsecs--, dst += SECTSIZE) {		if ((r = ide_wait_ready(1)) &lt; 0)			return r;		insl(0x1F0, dst, SECTSIZE/4);	}	return 0;}磁盘驱动器中读取磁盘的单位是一个扇区。提供的接口中，secno表示开始扇区号，dst表示数据输出的地址，nsecs表示读取扇区数目。BLKSECTS定义在fs.h中，为BLKSIZE/SECSIZE,表示一个块包含的扇区数，则这里secno应该为 blockno * BLKSECTS, 目的地址为addr， 读取扇区数为BLKSECTS.bc_pgfault()的实现：static voidbc_pgfault(struct UTrapframe *utf){	void *addr = (void *) utf-&gt;utf_fault_va;	uint32_t blockno = ((uint32_t)addr - DISKMAP) / BLKSIZE;	int r;	// Check that the fault was within the block cache region	if (addr &lt; (void*)DISKMAP || addr &gt;= (void*)(DISKMAP + DISKSIZE))		panic("page fault in FS: eip %08x, va %08x, err %04x",		      utf-&gt;utf_eip, addr, utf-&gt;utf_err);	// Sanity check the block number.	if (super &amp;&amp; blockno &gt;= super-&gt;s_nblocks)		panic("reading non-existent block %08x\n", blockno);	// Allocate a page in the disk map region, read the contents	// of the block from the disk into that page.	// Hint: first round addr to page boundary. fs/ide.c has code to read	// the disk.	//	// LAB 5: you code here:	addr = ROUNDDOWN(addr, PGSIZE);    	if((r=sys_page_alloc(0, addr, PTE_W|PTE_U|PTE_P))&lt;0)		panic("failed to alloc page: %e",r);    	if ((r = ide_read(blockno*BLKSECTS, addr, BLKSECTS)) &lt; 0){        	panic("in bc_pgfault,ide_read: %e", r);    	}	// Clear the dirty bit for the disk block page since we just read the	// block from disk	if ((r = sys_page_map(0, addr, 0, addr, uvpt[PGNUM(addr)] &amp; PTE_SYSCALL)) &lt; 0)		panic("in bc_pgfault, sys_page_map: %e", r);	// Check that the block we read was allocated. (exercise for	// the reader: why do we do this *after* reading the block	// in?)	if (bitmap &amp;&amp; block_is_free(blockno))		panic("reading free block %08x\n", blockno);}flush_blockflush_block函数必要时将内存中的一个磁盘块的数据写回磁盘, 然后将该块的脏位置为0. 这里的必要，指的是当该块在内存中并且脏位为1的时候。假如数据没有发生改变，那么就没必要写磁盘。形参addr是文件系统进程中的一个虚拟地址，用blockno=((uint32_t)addr - DISKMAP) / BLKSIZE;将它转为磁盘块号。同样，addr需要对齐到页大小。根据提示，首先用va_is_mapped判断该虚拟地址是否绑定到内存物理地址上，如果没有，说明该块不在内存中，不需要写回。用va_is_dirty判断该虚拟地址对应页是否被修改过，如果没有也直接返回。否则用磁盘驱动器的ide_write(uint32_t secno, const void *src, size_t nsecs)接口来写回块。同样以扇区为单位操作，src是数据块的源虚拟地址。写回之后，把内存上该块的脏位（PTE_D）置为0. 可以用sys_page_map来实现。sys_page_map的接口是：static intsys_page_map(envid_t srcenvid, void *srcva,	     envid_t dstenvid, void *dstva, int perm)它将进程 srcenvid的虚拟地址srcva映射到进程dstenvid的dstva处，并将权限设置为perm。可以通过将源和目的虚拟地址都设置为文件系统的虚拟地址addr, 并将perm设置为 uvpt[PGNUM(addr)]&amp; PTE_SYSCALL来把PTE_D清空, 并保留addr页面原来的权限设置。PTE_SYSCALL是只有用户程序进行系统调用时才会设置的，它的值是：#define PTE_SYSCALL	(PTE_AVAIL | PTE_P | PTE_W | PTE_U), PTE_AVAIL表示可为用户使用。加上错误处理，flush_block()实现便完成：// Flush the contents of the block containing VA out to disk if// necessary, then clear the PTE_D bit using sys_page_map.// If the block is not in the block cache or is not dirty, does// nothing.// Hint: Use va_is_mapped, va_is_dirty, and ide_write.// Hint: Use the PTE_SYSCALL constant when calling sys_page_map.// Hint: Don't forget to round addr down.voidflush_block(void *addr){	uint32_t blockno = ((uint32_t)addr - DISKMAP) / BLKSIZE;	if (addr &lt; (void*)DISKMAP || addr &gt;= (void*)(DISKMAP + DISKSIZE))		panic("flush_block of bad va %08x", addr);	// LAB 5: Your code here.    int r;    addr = ROUNDDOWN(addr, PGSIZE);    if(!va_is_mapped(addr)|| ! va_is_dirty(addr))        return;    if((r = ide_write(blockno * BLKSECTS, addr, BLKSECTS))&lt;0)        panic("in flush_block, ide_write(): %e",r);    if((r = sys_page_map(0,addr, 0,addr, uvpt[PGNUM(addr)]&amp;PTE_SYSCALL))&lt;0)        panic("in flush_block, sys_page_map: %e",r);}  检查make grade后：check_bc,check_super和check_bitmapOK，表示两个函数实现正确。TODO3:  使用free_block作为模板在fs / fs.c中实现alloc_block，后者应在bitmap中找到可用的磁盘块，将其标记为已使用，然后返回该块的编号。 分配该块时，应立即使用flush_block将更改后的bitmap块刷新到磁盘，以帮助文件系统保持一致。alloc_block的过程是：  从第一个块号开始，调用block_is_free判断它是否空闲，找到第一个空闲的块。block_is_free就是从判断bitmap中该块对应的bit是否为1（free），如果是则返回真。  找到第一个空闲块后，将bitmap中对应的位置为0.  使用flush_block()将该块对应的bitmap块写回磁盘，保持同步。因为flush_block()接收的是虚拟地址，所以还要使用diskaddr将块号转化为虚拟地址。bitmap数据结构在fs.h中定义，是一个类型为uint32_t的数组，那么数组的一个元素（32位）就可以表示32个块。要判断一个块是否空闲，应该用bitmap[blockno/32] &amp;= 1&lt;&lt;blockno%32是否为1来判断。则要将某一位置为0，可以用bitmap[blockno/32] &amp;= ~(1&lt;&lt;blockno%32)。在JOS中，磁盘的第一个块（blockno=0）是boot sector，第二个块（blockno=1）是超级块，第三个块（blockno=2）开始才是bitmap块。因此给定一个块号blockno，它的bitmap所在的块为： 2+ (blockno/32) / (BLKSIZE /4 ).  一个bitmap元素的大小是4个字节，一个块大小是BLKSIZE，所以一个块总共有BLKSIZE/4个bitmap元素， blockno/32为该块对应的bitmap索引号，除以每块能存放的元素数就是相对的块偏移。diskaddr定义在bc.c中，它接收一个磁盘块号，返回在文件系统进程中对应的虚拟地址。alloc_block的实现如下：intalloc_block(void){	// The bitmap consists of one or more blocks.  A single bitmap block	// contains the in-use bits for BLKBITSIZE blocks.  There are	// super-&gt;s_nblocks blocks in the disk altogether.	// LAB 5: Your code here.	for(uint32_t blockno=0; blockno&lt; super-&gt;s_nblocks; blockno++){		if(block_is_free(blockno){			bitmap[blockno/32] &amp;= ~(1&lt;&lt;(blockno%32)); //将bitmap上对应的位置为0，表示占用。			flush_block(diskaddr(2 + (blockno / 32) / (BLKSIZE/4))); //bitmap的第一块磁盘块号是2, blockno/32是该blockno在bitmap中的索引号。			return blockno;		}	}	return -E_NO_DISK;}检查make grade:alloc_blockOK, 代码正确。TODO4  实现file_block_walk和file_get_block。 file_block_walk从文件中的块偏移量映射到struct File或间接块中的指针，非常类似于pgdir_walk对页表所做的操作。 file_get_block更进一步，并映射到实际的磁盘块，并在必要时分配一个新的磁盘块。阅读fs. c中的代码fs.c中实现了文件系统的各种操作，包括初始化、从根目录开始遍历文件系统以找到某个绝对路径标识的文件、在目录上分配一个文件的条目、获取一个文件的条目、获取文件的某个块等基本操作，以及对文件进行创建、打开、删除、修改、读取、写回磁盘、扩展、截短等操作。JOS中，有两个磁盘映像。如果磁盘1可用的话，磁盘0只用来装载内核，磁盘1上才实现了文件系统。fs_init()函数检查可用磁盘，并设置超级块super为该磁盘的第二块。fs.c中所有的函数以及对应的功能为：  bitmap操作：          block_is_free(blockno): 接收磁盘块号，根据bitmap判断块是否空闲      free_block(blockno): 接收磁盘块号，释放该块，将bitmap中对应位置1.      alloc_block(): 根据bitmap找到第一个空闲块，将它分配出去，返回块号        文件系统结构构建和维护：          fs_init()： 初始化文件系统。找到可用磁盘，设置磁盘驱动，读取超级块并保存指针到super。      file_block_walk(struct File *f, uint32_t filebno, uint32_t **ppdiskbno, bool alloc): 查找文件f的第filebno个块的磁盘地址（通过该文件元数据中的索引来查找），将该索引的地址存放到ppdiskbno中；由于直接索引只有10个，则当块号fileno大于9时，而间接索引还没有分配的时候，如果alloc，就分配一个间接索引页，然后将ppdiskbno置为该间接索引页上对应的索引地址。      file_get_block(struct File *f, uint32_t filebno, char **blk) : 查找文件f的第filebno块在文件系统进程中对应的虚拟地址，并将其保存在blk中       dir_lookup(struct File *dir, const char *name, struct File **file) : 在DIR指定的目录下寻找名字为name的文件的元数据，并将元数据的地址保存到file中。       dir_alloc_file(struct File *dir, struct File **file) : 在dir目录下寻找一个没有被使用的File结构，把它分配给一个新文件使用（调用者负责填充这块元数据），将它的地址存放到file中。       walk_path(const char *path, struct File **pdir, struct File **pf, char *lastelem) : 解析path中的文件路径，如果成功，将文件的元数据地址存放在pf中，将文件所在目录的元数据地址存放在pdir中。如果不成功，pdir还是存放最后匹配的目录元数据地址，而lastelem存放最后无法匹配的路径字符串。例如目录/aaa/bbb/下不存在/aaa/bbb/c.c，则pdir指向/aaa/bbb的元数据，lastelem=’c.c’        文件操作：          file_create(const char *path, struct File **pf): 创建path文件/目录，如果成功，将该文件或目录的元数据指针放在pf中。      file_open(const char *path, struct File **pf): 打开path， 如果成功，将该文件元数据地址赋给pf.      file_read(struct File *f, void *buf, size_t count, off_t offset): 从文件f的offset位置（字节为单位）开始， 读取count个字节到buf中，返回实际读取字节数。      file_write(struct File *f, const void *buf, size_t count, off_t offset): 向文件f从offset位置开始，写入count个字节，数据的来源是buf。 如果写的时候文件的大小超过了它已分配的块就要扩展文件。      file_free_block(struct File *f, uint32_t filebno): 删除文件f的第filebno个块。      file_truncate_blocks(struct File *f, off_t newsize): 将文件f截短到newsize的大小。具体操作是，计算原来文件具有的块数和新的块数，将超过新块数的部分释放掉。如果新的大小已经小于10块，则不需要间接索引了，释放掉间接索引块      file_set_size(struct File *f, off_t newsize): 设置文件f的大小为newsize,自动截短或扩展。      file_flush(struct File *f): 将文件f的数据和元数据写回磁盘，遍历文件的所有块，将所有的脏块写回磁盘。      file_block_walk（）查找文件f的第filebno个块的磁盘地址（通过该文件元数据中的索引来查找），将该索引的地址存放到ppdiskbno中。如果alloc置位，必要时分配间接索引页，否则返回错误信息。函数的过程：  判断filebno，是否在直接索引支持的范围内（0-9），如果是，就直接将ppdiskbno赋为直接索引上对应的条目的地址，即&amp; (f-&gt;f_direct[filebno]).  如果filebno大于9，但是小于1034, 即文件系统可以支持的文件最大块数          如果间接索引块还没有分配，要检查alloc，如果alloc为0，返回-E_NOT_FOUND表示找不到索引条目。      如果alloc为1：                  调用alloc_block为间接索引分配一个块， 如果分配错误，返回-E_NO_DISK。          分配成功，则将间接索引链接到这个块上，即f-&gt;f_indirect = blockno，这样查询10号以后的块时，首先查找f-&gt;f_indirect得到索引块的地址，然后在索引块上找到对应的直接索引条目，根据该条目的值找到物理块号。          分配好之后，还要将索引块初始化为全0，表示这些文件块还没有分配和映射。                    无论是新分配的，还是原来就存在的，都要将间接索引块上对应的索引条目地址存放到ppdiskbno中。 如何获得索引块上对应的条目地址呢？ 首先，f-&gt;f_indirect给出索引块的物理地址，用diskaddr(f-&gt;f_indirect)就可以知道它的虚拟地址，也就是这个块上第一个索引条目的地址。这个虚拟地址再加上filebno在索引块上的偏移量就可以得到索引条目的虚拟地址，代码上有两种实现方式：                  (uintptr_t*) diskaddr(f-&gt;f_indirect)将索引块的虚拟地址转为JOS的指针类型，指针可以用[i]操作符来取得第i个数据，这里i应该是filebno-10, 因为是索引块上的第filebno-10条索引，最后用&amp;取址。          首先用 diskaddr(f-&gt;f_indirect)获得第一个索引条目的虚拟地址，然后加上(filebno-10)*4就得到第filebno-10的地址，因为每个索引条目大小是4个字节。                      如果filebno超过1034，是无效的，返回-E_INVAL，否则返回0，表示成功。static intfile_block_walk(struct File *f, uint32_t filebno, uint32_t **ppdiskbno, bool alloc){       // LAB 5: Your code here.	uint32_t blockno;	if(filebno &lt;=9){		*ppdiskbno = &amp;(f-&gt;f_direct[filebno]);	}	else if(filebno&lt;1034){		if(!f-&gt;f_indirect){			if(!alloc)				return -E_NOT_FOUND;						else{				if((blockno=alloc_block())&lt;0)					return -E_NO_DISK;				f-&gt;f_indirect = blockno;				memset(diskaddr(blockno), 0, BLKSIZE);			}		}		*ppdiskbno = &amp;((uintptr_t*) diskaddr(f-&gt;f_indirect))[filebno-10];               //或者 *ppdiskbno = diskaddr(f-&gt;f_indirect)+ (filebno-10)*4;	}	else return -E_INVAL;	return 0;		}file_get_block()file_get_block在file_block_walk的基础上，更进一步地，获取文件块对应的物理块号，并将物理块在文件系统进程中对应的虚拟地址保存在blk中，过程是：  调用file_block_walk获取f的第filebno个块的索引条目地址，保存在slot中，要把alloc设置为1，表示如果需要用到间接索引但索引块还未分配时要自动分配。  如果flie_block_walk返回值小于0，表示发生了错误，返回对应的错误码。  如果slot的值为0，即该文件逻辑块还没有分配到具体的物理块，可能是文件被创建或者被扩展等，要为该文件块分配一个物理块，使用alloc_block()          如果分配错误应返回-E_NO_DISK      分配成功，则令对应的索引条目值为返回的物理块号，这样文件块就被映射到物理块上。        无论文件块原来是否已经分配，将物理块对应的虚拟地址保存在blk中。*slot是对应的物理块号，调用diskaddr(*slot)就可以找到虚拟地址，将它保存在blk中，成功返回0.intfile_get_block(struct File *f, uint32_t filebno, char **blk){       // LAB 5: Your code here.	uint32_t *slot=NULL;    //slot = 索引条目的地址，*slot = 文件块对应的物理块号码	uint32_t errorcode=file_block_walk(f, filebno, &amp;slot, 1);	uint32_t blockno;	if(errorcode&lt;0)		return errorcode;	if(!*slot){		if((blockno = alloc_block())&lt;0)			return -E_NO_DISK;		*slot = blockno;	}	*blk = (char *)diskaddr(*slot);	return 0;}检查make grade:file_get_blockOK，代码正确。TODO5:  •在fs/serv.c中实现serve_read 。  •serve_read的大量操作将由fs / fs.c中已经实现的file_read来完成。 serve_read只需提供RPC接口即可读取文件。 查看serve_set_size中的注释和代码，以大致了解服务器功能的结构。请求文件系统服务的过程文件系统进程内部实现了对文件的各种操作，但是用户进程无法直接调用这些函数，因为它们是在文件系统进程的内存空间内。这时，需要采用进程间通信（IPC机制）来在进程之间交换数据或者方法。JOS中，使用主从式架构来进行文件系统和其他进程之间的通信。通常，使用IPC进行数据交换的两个进程可以被分为服务端和用户端，客户端向服务器发出请求，服务器进行处理之后回应请求。JOS用建立在IPC基础上的远程过程调用（RPC）来进行通信。RPC在两个应用之间建立TCP连接，然后客户端应用将过程调用的参数序列化成二进制数据之后发送给服务端，服务端反序列化之后进行过程调用，返回值再序列化后发送回客户端。JOS的文件系统调用过程是：底层实际上是进程间通信。文件系统的相关数据结构有：  文件描述符Fd： 文件所在的设备id（因为一个文件系统可以跨多个设备），是否打开, 以及fd_offset文件的光标等。  设备描述符Dev： 设备上各种操作调用的入口，设备名字和设备id。  打开文件描述符列表OpenFile结构，由内核维护的opentab数组，保存了文件元数据的地址，文件打开状态，文件id以及磁盘上文件描述符Fd结构的指针  File结构（文件元数据）：维护文件重要信息，完成逻辑块到物理块的映射等。文件描述符表映射在磁盘的0xD0000000处，文件描述符结构中包含了设备id，当进程调用fd.c中的read时，将传入文件描述符号，通过文件描述符号找到对应地Fd结构体，进而查找设备，设备用Dev结构来描述，结构体中包含了dev_read, dev_write等设备读、写函数的指针。devfile_read函数定义在file.c中，它将调用fsipc()向服务器进程发起请求。fsipc()函数专门负责与文件系统进程进行通信，它建立在ipc机制上。ipc.c封装了两个函数——ipc_send( envid_t to_env, uint32_t val, void *pg, int perm )和ipc_recv( envid_t *from_env_store, void *pg, int *perm_store )，允许与某一个环境进行数据交换，交换的消息包含两个部分：  1个32位的值  可选的页映射关系其中，send中的val是交换的数值，pg参数表示发送进程希望与接收进程共享pg对应的物理页，并且在接收进程中，对该页的权限是perm。文件系统实现了一个Fspic数据结构：union Fsipc {	struct Fsreq_open {		char req_path[MAXPATHLEN];		int req_omode;	} open;	struct Fsreq_set_size {		int req_fileid;		off_t req_size;	} set_size;	struct Fsreq_read {		int req_fileid;		size_t req_n;	} read;	struct Fsret_read {		char ret_buf[PGSIZE];	} readRet;	struct Fsreq_write {		int req_fileid;		size_t req_n;		char req_buf[PGSIZE - (sizeof(int) + sizeof(size_t))];	} write;	struct Fsreq_stat {		int req_fileid;	} stat;	struct Fsret_stat {		char ret_name[MAXNAMELEN];		off_t ret_size;		int ret_isdir;	} statRet;	struct Fsreq_flush {		int req_fileid;	} flush;	struct Fsreq_remove {		char req_path[MAXPATHLEN];	} remove;	// Ensure Fsipc is one page	char _pad[PGSIZE];};它是一个联合，其中的各种结构体保存了对应的文件操作需要的参数，例如fsipc.read就有要读取的文件的id以及读取的大小req_n. 所以，在进程与文件系统通信的时候，两者可以共享一个保存了Fsipc结构的页面，从而实现参数的传递。 在file.c中，这个Fspic结构的变量名为fspicbuf.devfile_read函数接收文件id以及读取大小，设置fspicbuf中对应的字段，然后调用fsipc()。fsipc接收两个参数，一个是文件操作的类型type，另一个是虚拟地址，这里应该传入fsipcbuf, 然后它会调用ipc_send()，其中要交换的32位值就是type，对于文件读取，type置为FSREQ_READ。然后继续调用ipc_recv等待文件系统响应。当文件系统响应后，把结果依次返回给用户进程。在服务端，文件系统中的serve()会循环调用ipc_recv()监听请求，接收到请求之后，它会解析请求中的type参数，然后具体分发到对应的handler。type和handler的对应关系是：fshandler handlers[] = {	// Open is handled specially because it passes pages	/* [FSREQ_OPEN] =	(fshandler)serve_open, */	[FSREQ_READ] =		serve_read,	[FSREQ_STAT] =		serve_stat,	[FSREQ_FLUSH] =		(fshandler)serve_flush,	[FSREQ_WRITE] =		(fshandler)serve_write,	[FSREQ_SET_SIZE] =	(fshandler)serve_set_size,	[FSREQ_SYNC] =		serve_sync};调用ipc_recv()时，serve会传入一个参数fsreq，它表示文件系统接收请求进程的共享页，要将共享页映射到虚拟内存的什么位置，fsreq的定义是：// Virtual address at which to receive page mappings containing client requests.union Fsipc *fsreq = (union Fsipc *)0x0ffff000;也是一个Fsipc类型的指针。通过Fsipc，文件系统可以接收来自客户进程的参数。从对应的handler返回后，serve()函数也负责调用ipc_send()将结果返回给fsipc。其中，交换的32位数据就是各个handler的返回状态，例如0表示成功，-E_NO_DISK表示磁盘空间不足等。在handler中，因为文件系统与客户进程共享fsreq数据结构，那么它可以将读取出来的文件数据放在fsreq.readRet中，这相当于一个块的缓冲区，让用户去读取。 另外，如果调用的类型是FSREQ_OPEN， 还会将文件描述符Fd结构所在的页地址放到pg中，与用户进程共享。serve_set_size()阅读这个函数是将fs.c中的file_set_size()函数封装成一个handler，所有的handler调用入口都是serve函数。除了serve_open以外，所有的handler都只接受两个参数，一个是服务请求者的进程ID（envid或者whom），另一个是与请求者共享的fsreq页，上面保存了调用的参数。voidserve(void){	uint32_t req, whom;	int perm, r;	void *pg;	while (1) {		perm = 0;		req = ipc_recv((int32_t *) &amp;whom, fsreq, &amp;perm);		if (debug)			cprintf("fs req %d from %08x [page %08x: %s]\n",				req, whom, uvpt[PGNUM(fsreq)], fsreq);		// All requests must contain an argument page		if (!(perm &amp; PTE_P)) {			cprintf("Invalid request from %08x: no argument page\n",				whom);			continue; // just leave it hanging...		}		pg = NULL;		if (req == FSREQ_OPEN) {			r = serve_open(whom, (struct Fsreq_open*)fsreq, &amp;pg, &amp;perm);		} else if (req &lt; ARRAY_SIZE(handlers) &amp;&amp; handlers[req]) {			r = handlers[req](whom, fsreq);		} else {			cprintf("Invalid request code %d from %08x\n", req, whom);			r = -E_INVAL;		}		ipc_send(whom, r, pg, perm);		sys_page_unmap(0, fsreq);	}}serve_set_size中，fsrequnion中只包含了set_size结构体，可以通过req-&gt;req_fileid获取文件id，req-&gt;req_size获取要设置的文件大小。首先调用openfile_lookup来判断该进程是否打开了该文件。内核维护了系统中所有被打开的文件在opentab数组中，而每一个进程也维护了自己打开文件的描述符。如果进程未打开文件，不可以调用set_file_size()，返回错误码。否则，openfile_lookup会将该文件的描述符保存在o中，通过o-&gt;o_file就可以获取该文件的元数据指针。直接调用file_set_size并返回返回值即可。set_size不需要为用户进程提供文件数据，只需要交换一个操作成功与否的状态信息。// Set the size of req-&gt;req_fileid to req-&gt;req_size bytes, truncating// or extending the file as necessary.intserve_set_size(envid_t envid, struct Fsreq_set_size *req){	struct OpenFile *o;	int r;	if (debug)		cprintf("serve_set_size %08x %08x %08x\n", envid, req-&gt;req_fileid, req-&gt;req_size);	// Every file system IPC call has the same general structure.	// Here's how it goes.	// First, use openfile_lookup to find the relevant open file.	// On failure, return the error code to the client with ipc_send.	if ((r = openfile_lookup(envid, req-&gt;req_fileid, &amp;o)) &lt; 0)		return r;	// Second, call the relevant file system function (from fs/fs.c).	// On failure, return the error code to the client.	return file_set_size(o-&gt;o_file, req-&gt;req_size);}serve_read()serve_read处理读取文件的请求。它基于fs.c中实现的file_read函数，接收进程的id以及调用参数结构Fsipc，从文件的当前光标处读取文件的req_n个字节，到缓冲区readRet.ret_buf中，并更新文件光标的位置。如果成功，返回实际读取的字节数，否则返回错误号码。它的实现过程是：      首先从与客户端共享的调用参数页ipc中，获取调用参数。因为是read操作，所以fsipc中保存的是REQ_READ结构体：                  struct Fsreq_read {		int req_fileid;		size_t req_n;	} read;                          返回的时候，要填充fspic中的readRet结构体中的ret_buf，而因为缓冲区的大小只有一页，所以一次devfile_read调用读取的字节数不可以超过4096字节。readRet结构体的定义：                  struct Fsret_read {		char ret_buf[PGSIZE];	} readRet;                          调用openfile_lookup找到该打开文件的openFile结构体。如果返回值小于0，说明发生错误，返回错误码。        OpenFile结构体中的o_file保存了文件的元数据指针，o_fd指向磁盘上的文件描述符结构Fd，而Fd保存了fd_offset,表示文件当前的光标位置。        调用file_read()，它需要四个参数：文件元数据指针，读取数据的缓冲区，读取数据字节数，读取开始的offset。read()调用隐式地从文件光标处读取n个字节，所以offset参数可以通过o-&gt;o_fd-&gt;fd_offset获得。缓冲区是ret-&gt;ret_buf。        读取失败的话返回错误码，否则将文件光标移动到读取完成的位置。        返回实际读取的字节数。  intserve_read(envid_t envid, union Fsipc *ipc){	struct Fsreq_read *req = &amp;ipc-&gt;read;	struct Fsret_read *ret = &amp;ipc-&gt;readRet;	struct OpenFile *o;	int r;	if (debug)		cprintf("serve_read %08x %08x %08x\n", envid, req-&gt;req_fileid, req-&gt;req_n);	// Lab 5: Your code here:	if((r = openfile_lookup(envid, req-&gt;req_fileid, &amp;o))&lt;0)		return r;		if((r = file_read(o-&gt;o_file, ret-&gt;ret_buf, req-&gt;req_n, o-&gt;o_fd-&gt;fd_offset))&lt;0)		return r;	o-&gt;o_fd-&gt;fd_offset+=r;		 	return r;}检查file_readOK， 实现正确。TODO6:  ​     在fs/serv.c中实现serve-write，在lib/file.c中实现devfile-writeserve_write()根据注释，serve_write()的功能应该是接收进程的idenvid和写请求的参数：struct Fsreq_write {		int req_fileid;		size_t req_n;		char req_buf[PGSIZE - (sizeof(int) + sizeof(size_t))];	} write;将req_buf中的req_n个字节的内容写到对应文件的光标开始处。req_buf的大小是PGSIZE-(sizeof(int) + sizeof(size_t))， 这是为了保证fsipc结构可以与页的大小对齐。req_n的大小只能小于或等于req_buf的固定大小。也就是一次请求不会超过一个块。serve_write可以通过file_write()来实现，file_write没有大小的限制，传给file_write的count大小只要不要超过文件最大大小的范围即可。file_write也已经提供了自动扩展文件大小的功能。根据函数的语义，serve_write()的实现为：intserve_write(envid_t envid, struct Fsreq_write *req){	int r;	struct OpenFile *o;	if (debug)		cprintf("serve_write %08x %08x %08x\n", envid, req-&gt;req_fileid, req-&gt;req_n);	// LAB 5: Your code here.	if((r = openfile_lookup(envid, req-&gt;req_fileid, &amp;o))&lt;0)		return r;	if((r = file_write(o-&gt;o_file, req-&gt;req_buf, req-&gt;req_n, o-&gt;o_fd-&gt;fd_offset))&lt;0)		return r;	o-&gt;o_fd-&gt;fd_offset += r;	return r; }devfile_write()devfile_write()处理用户程序发出的写请求，req_n不能超过req_buf大小的条件在这个函数中检查。它将文件的id，请求写字节数和缓冲区保存到fsipcbuf的write结构中，调用fsipc()函数发送ipc请求，并共享fsipcbuf页传递参数，把fsipc的返回值直接返回给用户进程。decfile_write的实现过程为：  判断请求的字节数n是否比缓冲区大小（PGSIZE - (sizeof(int) + sizeof(size_t))大，如果超出，设置为缓冲区大小。  将fsipcbuf.write.req_fileid保存为用户进程请求的文件id。由于用户进程传给devfile_write的是文件的描述符指针，所以通过fd-&gt;fd_file.id来获得该文件id。  将fsipcbuf.write.req_n设置为请求字节数n（不超过缓冲区大小）  将用户传递进来的buf中的前n个字节，复制到fsipcbuf.write.req_buf中，这个复制使用memmove。  调用fsipc()发起请求并返回。 这个调用的类型为写操作FSREQ_WRITE， 操作类型将由文件系统的serve()接收，分发到对应的serve_write函数。它不需要从文件系统那里共享页（因为不需要读取文件的内容），所以共享页虚拟地址为NULL。（fsipc会在ipc_send的时候与文件系统共享fsipcbuf页）。static ssize_tdevfile_write(struct Fd *fd, const void *buf, size_t n){	// Make an FSREQ_WRITE request to the file system server.  Be	// careful: fsipcbuf.write.req_buf is only so large, but	// remember that write is always allowed to write *fewer*	// bytes than requested.	// LAB 5: Your code here        uint32_t max_n = PGSIZE - (sizeof(int) + sizeof(size_t));        n = n &gt; max_n ? max_n : n;    	fsipcbuf.write.req_fileid = fd-&gt;fd_file.id;    	fsipcbuf.write.req_n = n;    	memmove(fsipcbuf.write.req_buf, buf, n);    	return fsipc(FSREQ_WRITE, NULL);}检查本次实验所有实现的函数检查都OK.总结文件系统负责文件的管理，它通过索引、链表、连续分配等方式组织文件，将文件内的逻辑地址映射到磁盘上的物理块地址；它通过bitmap、空闲块链表等数据结构维护磁盘上的空闲块，实现对磁盘空闲空间的管理；它直接与设备驱动程序（JOS）或者间接与设备驱动程序（UNIX系统中，设备驱动程序的操作封装为系统调用）交互，并通过逻辑地址到物理地址的转换，为用户提供了方便简单且统一的接口，以及一个直观的文件系统界面。JOS的文件系统通过bitmap来管理空闲空间，当文件系统的大小为3GB时，bitmap的大小为24块。通过间接索引结构来组织文件，文件元数据中保存索引，来完成文件逻辑地址到磁盘物理地址的转换。文件系统中的每一个文件都有一个id标识，保存在文件描述符中，文件描述符表位于磁盘上，描述了文件的id、是否打开、设备以及偏移位置等。内核维护打开的文件描述符表，该结构可以通过打开的文件id找到对应的文件描述符，并保存了文件的元数据指针等变量。文件块的读取是按需读取的，只有当一个块被用户请求时，文件系统引用该块对应的虚拟地址，会发生页错误，然后才将该块从磁盘上读取到内存。文件系统拥有IO权限，所以可以访问设备驱动程序，调用它的读和写函数。 文件的修改用了 延迟写回的策略， 修改暂时修改在内存中，只有在必要时，会将PTE_D标记为脏的内存页面写回到磁盘上。JOS中的文件系统是一个特殊的用户进程，它将磁盘地址映射到自己的虚拟地址空间上进行方便的管理，并通过自定义页错误处理程序完成磁盘块到内存的数据转移。通过IPC进程间通信机制，使得用户进程可以调用定义在文件系统进程内的过程，从而为用户进程提供系统调用的接口。]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> JOS </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MIT 6.828 JOS 内存管理详解 操作系统实验5]]></title>
      <url>/lab%20report/2020/02/01/OS5memory/</url>
      <content type="text"><![CDATA[本文详解JOS操作系统虚拟内存的结构，以及内存管理单元的结构与实现。在这次实验中，将探索以下问题：  计算机启动时如何开启虚拟内存  程序虚拟地址空间的结构  如何将内核物理地址与虚拟地址映射  访问一个虚拟地址的时候，会发生什么  如何管理内存空闲空间  如何保护特定的代码和数据  ……操作系统lab5： 内存管理在lab1中，我们看到JOS 物理内存的结构：最初物理内存只有1MB， 之后扩展到了4GB，这时物理内存的640KB 到1MB之间就成为IO hole，是不可用的，用来分配给外部IO设备，如上图，640KB 到1MB之间被分配给了VGA Display、BIOS ROM以及其他的外部设备，称为IO hole。在JOS中，从0x0到640KB这部分称为 basemem，是可用的， 1MB以上的空间称为extented memory，也是可用的。为了更有效地管理和使用内存空间，JOS使用了虚拟内存，虚拟内存通过对程序存储地址与真实内存物理地址的解耦，有效解决了内存大小相对于大量用户程序所需空间不足的问题。引入虚拟内存之后，需要解决如何将多个程序分配到物理内存上，以及程序的虚拟地址如何与物理地址映射的问题。JOS通过分页的方式来管理内存和虚拟地址空间 ，将程序地址空间分为固定大小的页，将内存分为同样大小的页框，以页为单位将程序分配到内存物理空间上。页表记录了一个虚拟页对应的物理页框，以及这些页的相关信息，当程序执行中访问一个虚拟地址时，首先要访问它的页表，然后从页表中找到对应的真实地址，再访问真实的物理地址。在操作系统中，页表的管理、从虚拟地址到物理地址的转换、页面的分配回收以及缓存的管理等等，都是由内存管理单元(MMU)来完成的。内存管理与虚拟内存对用户是不可见的。在这次实验中，将探索以下问题：  计算机启动时如何开启虚拟内存  程序虚拟地址空间的结构  如何将内核物理地址与虚拟地址映射  访问一个虚拟地址的时候，会发生什么  如何管理内存空闲空间  如何保护特定的代码和数据  ……JOS的虚拟地址空间布局在lab1中，我们追踪了开机时bootloader加载内核的过程，加载完成后，物理内存的布局为：（图片来自https://blog-1253119293.cos.ap-beijing.myqcloud.com/图片来自https://blog-1253119293.cos.ap-beijing.myqcloud.com/&lt;/img&gt;JOS用了手写的内存映射，将物理地址0x00000000-0x00400000之间4MB的空间映射到了虚拟地址0xf0000000-0xf0400000处。0xf0000000即为在虚拟地址空间中内核部分的起始。真正开启虚拟内存之后，对于内核和用户程序来说，虚拟地址的布局在memlayout.h中定义：/* * Virtual memory map:                                Permissions *                                                    kernel/user * *    4 Gig --------&gt;  +------------------------------+ *                     |                              | RW/-- *                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ *                     :              .               : *                     :              .               : *                     :              .               : *                     |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| RW/-- *                     |                              | RW/-- *                     |   Remapped Physical Memory   | RW/-- *                     |                              | RW/-- *    KERNBASE, ----&gt;  +------------------------------+ 0xf0000000      --+ *    KSTACKTOP        |     CPU0's Kernel Stack      | RW/--  KSTKSIZE   | *                     | - - - - - - - - - - - - - - -|                   | *                     |      Invalid Memory (*)      | --/--  KSTKGAP    | *                     +------------------------------+                   | *                     |     CPU1's Kernel Stack      | RW/--  KSTKSIZE   | *                     | - - - - - - - - - - - - - - -|                 PTSIZE *                     |      Invalid Memory (*)      | --/--  KSTKGAP    | *                     +------------------------------+                   | *                     :              .               :                   | *                     :              .               :                   | *    MMIOLIM ------&gt;  +------------------------------+ 0xefc00000      --+ *                     |       Memory-mapped I/O      | RW/--  PTSIZE * ULIM, MMIOBASE --&gt;  +------------------------------+ 0xef800000 *                     |  Cur. Page Table (User R-)   | R-/R-  PTSIZE *    UVPT      ----&gt;  +------------------------------+ 0xef400000 *                     |          RO PAGES            | R-/R-  PTSIZE *    UPAGES    ----&gt;  +------------------------------+ 0xef000000 *                     |           RO ENVS            | R-/R-  PTSIZE * UTOP,UENVS ------&gt;  +------------------------------+ 0xeec00000 * UXSTACKTOP -/       |     User Exception Stack     | RW/RW  PGSIZE *                     +------------------------------+ 0xeebff000 *                     |       Empty Memory (*)       | --/--  PGSIZE *    USTACKTOP  ---&gt;  +------------------------------+ 0xeebfe000 *                     |      Normal User Stack       | RW/RW  PGSIZE *                     +------------------------------+ 0xeebfd000 *                     |                              | *                     |                              | *                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ *                     .                              . *                     .                              . *                     .                              . *                     |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| *                     |     Program Data &amp; Heap      | *    UTEXT --------&gt;  +------------------------------+ 0x00800000 *    PFTEMP -------&gt;  |       Empty Memory (*)       |        PTSIZE *                     |                              | *    UTEMP --------&gt;  +------------------------------+ 0x00400000      --+ *                     |       Empty Memory (*)       |                   | *                     | - - - - - - - - - - - - - - -|                   | *                     |  User STAB Data (optional)   |                 PTSIZE *    USTABDATA ----&gt;  +------------------------------+ 0x00200000        | *                     |       Empty Memory (*)       |                   | *    0 ------------&gt;  +------------------------------+                 --+ *[KERNBASE, 4Gig ] :  这部分映射到物理内存上中断向量表、引导扇区代码、IOhole以及内核代码、数据。在这部分，一个虚拟地址 - KERNBASE就是它的物理地址。内核部分会被同样地映射到每个进程的高地址空间，用户是没有权限访问的。KERNBASE往下是进程的地址空间，如之前报告中所述，进程地址空间的高处是内核栈，这部分地址是用户模式下不可访问的。[MMIOBASE, MMIOLIM ] : 这部分空间属于内存映射的IO设备，与IO设备通信要陷入内核完成，因此用户模式也不可访问。[UVPT, ULIM ] : 从ULIM往下直到UTOP是用户模式下只读的地址。UVPT到ULIM的这部分是当前的页目录，用户可以读取页表知道一个虚拟地址所在的物理页面，但不可操纵页表。[UPAGES, UVPT] ：这部分对应着pages数组在物理内存中存放的位置，用户也可以通过uvpt[n].pp_ref来知道某个物理页框是否已经被占用，但也不可操纵。[0, UTOP] : 这部分才真正是用户模式下可以读写的地址空间，它包括了用户程序的代码段、数据段、堆栈等。JOS中的三种地址JOS中有三种地址： 逻辑地址(virtual address)， 线性地址(linear address),  物理地址(physical address). 逻辑地址是程序编译链接之后变量的符号，实际上，逻辑地址是变量的段内偏移。 线性地址是逻辑地址经过保护模式的段地址变换之后的虚拟地址，线性地址=段首地址+逻辑地址。物理地址则是内存存储单元的编址，它会被直接送到内存的地址线上进行读写。逻辑地址到线性地址的变换在保护模式下自动完成。如果没有开启页式地址转换（Paging），那么线性地址就是物理地址，如同我们在lab 1中，mov %eax, cr3之前看到的一样。如果开启分页，线性地址就会按查询页表的方式转换成物理地址。后面的实验内容中，我们直接将线性地址称为虚拟地址。JOS的页表结构页表记录了从虚拟地址到真实物理地址之间的映射，JOS的页表结构、虚拟地址组成定义在mmu.h中，它使用的是一个两级页表： A linear address 'la' has a three-part structure as follows:// +--------10------+-------10-------+---------12----------+// | Page Directory |   Page Table   | Offset within Page  |// |      Index     |      Index     |                     |// +----------------+----------------+---------------------+//  \--- PDX(la) --/ \--- PTX(la) --/ \---- PGOFF(la) ----///  \---------- PGNUM(la) ----------/地址最高10位表示页表目录，中间10位表示页表索引，最后12位表示在一个页面内的偏移，因此，页面总数为$2^{10}$ *$2^{10}$=1024x1024，页面大小为2^12=4096字节。JOS使用两级页表，将全部的地址空间分为了一个页表目录和1024个页表，由于页表有1024个条目，每个条目的长度是4字节，则每个页表刚好就占一个页面，因此页表的地址只需要20位来区分。所以，在页面目录中，我们只需要20位来存放索引对应页表所在的物理地址，剩下的12位用来存放各种标志。页面目录中也含有1024条目，所以页面目录也只占一个页面。所以在用户的虚拟地址中，只需要存放一份页面目录的镜像，就可以让用户程序访问到页表，而不需要将所有1024个页表都映射到用户的虚拟地址空间。一个页表目录条目（或者页表条目，一样的）的结构为：一个目录条目的前20位记录了一个页表的物理地址。访问一个虚拟地址时，首先根据前10位目录索引从page directory上找到相应的条目，取出前20位作为页表的物理地址，然后访问该页表，根据10位的页表索引找到页表上对应的物理地址（也是前20位，与PGSIZE对齐），这个20位的物理地址加上offset就得到了物理地址。如图：条目剩下的低12位用来存放各种标志，来表示一个页表/页面的状态，所有的状态在mmu.h中定义：// Page table/directory entry flags.#define PTE_P		0x001	// Present#define PTE_W		0x002	// Writeable#define PTE_U		0x004	// User#define PTE_PWT		0x008	// Write-Through#define PTE_PCD		0x010	// Cache-Disable#define PTE_A		0x020	// Accessed#define PTE_D		0x040	// Dirty#define PTE_PS		0x080	// Page Size#define PTE_G		0x100	// Global其中，Present位是用来判断对应的页表或者条目是否存在物理内存中，如果存在则为1. 在后面的代码中，我们判断一个虚拟页是否与一个物理页框映射，即是否驻留在内存时，就可以通过 entry &amp; PTE_P来判断。TODO 1: Physical Page Management 代码阅读mem_init()mem_init()在内核刚启动时调用，它的任务是在开机之后，设置好分页系统，并完成内核部分虚拟地址与物理地址的映射。目前只完成了一部分，它需要初始化的变量如下：// These variables are set in mem_init()pde_t *kern_pgdir;		// Kernel's initial page directorystruct PageInfo *pages;		// Physical page state arraystatic struct PageInfo *page_free_list;	// Free list of physical pageskern_pgdir是页表目录。PageInfo是一个用来描述物理页框的结构体，它定义在memlayout.h中, 由一个指向下一个节点的指针，和引用位构成。每一个物理页框都对应着一个PageInfo结构，引用位表示该页框是否已经被占用。pages数组记录了所有物理页框（总共npages个）的信息，而为了分配页面时更快地找到一个空的页框，JOS还维护了page_free_list链表，动态地保存所有空闲的页框。当需要分配页面时，从page_free_list的头部指针获取第一个空闲页框，然后将头部指针后移；当有新的空闲页面时，将这个新页面的指针添加到page_free_list中。struct PageInfo {	// Next page on the free list.	struct PageInfo *pp_link;	// pp_ref is the count of pointers (usually in page table entries)	// to this page, for pages allocated using page_alloc.	// Pages allocated at boot time using pmap.c's	// boot_alloc do not have valid reference count fields.	uint16_t pp_ref;};mem_init具体实现如下：// Set up a two-level page table://    kern_pgdir is its linear (virtual) address of the root//// This function only sets up the kernel part of the address space// (ie. addresses &gt;= UTOP).  The user part of the address space// will be set up later.//// From UTOP to ULIM, the user is allowed to read but not write.// Above ULIM the user cannot read or write.voidmem_init(void){	uint32_t cr0;	size_t n;	// Find out how much memory the machine has (npages &amp; npages_basemem).	i386_detect_memory();     //检测机器有多少物理内存。	// Remove this line when you're ready to test this function.	panic("mem_init: This function is not finished\n");	//////////////////////////////////////////////////////////////////////	// create initial page directory.	kern_pgdir = (pde_t *) boot_alloc(PGSIZE);	memset(kern_pgdir, 0, PGSIZE);	//////////////////////////////////////////////////////////////////////	// Recursively insert PD in itself as a page table, to form	// a virtual page table at virtual address UVPT.	// (For now, you don't have understand the greater purpose of the	// following line.)	// Permissions: kernel R, user R	kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P;	//////////////////////////////////////////////////////////////////////	// Allocate an array of npages 'struct PageInfo's and store it in 'pages'.	// The kernel uses this array to keep track of physical pages: for	// each physical page, there is a corresponding struct PageInfo in this	// array.  'npages' is the number of physical pages in memory.  Use memset	// to initialize all fields of each struct PageInfo to 0.	pages = (struct PageInfo *) boot_alloc(npages * sizeof(struct PageInfo));	memset(pages, 0, npages * sizeof(struct PageInfo));	//////////////////////////////////////////////////////////////////////	// Now that we've allocated the initial kernel data structures, we set	// up the list of free physical pages. Once we've done so, all further	// memory management will go through the page_* functions. In	// particular, we can now map memory using boot_map_region	// or page_insert	page_init();	check_page_free_list(1);	check_page_alloc();	check_page();在JOS开机的时候，我们会看到一句输出：给出了物理内存的可用空间，base是底部的basemem的大小（640K），extended是extended memory的大小，是1MB以上的可用空间。检测是在函数i386_detect_memory()中完成的：static voidi386_detect_memory(void){	size_t basemem, extmem, ext16mem, totalmem;	// Use CMOS calls to measure available base &amp; extended memory.	// (CMOS calls return results in kilobytes.)	basemem = nvram_read(NVRAM_BASELO);	extmem = nvram_read(NVRAM_EXTLO);	ext16mem = nvram_read(NVRAM_EXT16LO) * 64;	// Calculate the number of physical pages available in both base	// and extended memory.	if (ext16mem)		totalmem = 16 * 1024 + ext16mem;	else if (extmem)		totalmem = 1 * 1024 + extmem;	else		totalmem = basemem;	npages = totalmem / (PGSIZE / 1024);	npages_basemem = basemem / (PGSIZE / 1024);	cprintf("Physical memory: %uK available, base = %uK, extended = %uK\n",		totalmem, basemem, totalmem - basemem);}注意到读取basemem、extmem和ext16mem的大小使用了函数nvram_read。nvram_read实际上又调用了mc146818_read函数，这个函数通过IO端口0x70与0x71从实时时钟RTC中读取数据。RTC使用芯片mc146818，在系统电源关闭时，RTC仍保持工作，维护系统的日期和时间，当系统启动时，就从RTC中读取日期时间的基准值。时钟和这里的物理内存其实没有关系，但mc146818芯片中带有一个非易失性的RAM，也就是non-volatile-ram（nvram），系统的物理内存basemem和extmem的大小，都存放在这个芯片上，这样可以保证系统电源关闭时，这些信息不会被擦除。i386_detect_memory通过nvram_read从mc146818芯片中读取出basemem和extmem大小（以KB为单位），然后根据它们计算出内存总的可用空间以及总的页面数npages,npages_basemem。PGSIZE定义在mmu.h中，为4096字节。检测出可用内存大小之后，mem_init开始设置内核的页表。首先调用boot_alloc在物理内存中分配内核的页表。boot_alloc()boot_alloc()只会在JOS初始化虚拟内存之前被调用一次，之后分配页面的时候都只会使用page_allocator(). 之所以要写一个单独的boot_alloc是因为： 在启动时需要将内核的物理地址映射到虚拟地址，这种映射需要通过访问内核的页表来实现，创建页表涉及到分配页表所在的页面，可是分配页面又是在虚拟内存设置好才可以做到。所以，JOS使用了一个单独的boot_alloc，将需要分配的页面映射到一些固定的虚拟地址，并返回所分配的内容的起始虚拟地址。static void *boot_alloc(uint32_t n)   //n表示需要分配的字节数{	static char *nextfree;	// virtual address of next byte of free memory	char *result;    //result 用来保存分配的一片虚拟地址的起始地址	// Initialize nextfree if this is the first time.	// 'end' is a magic symbol automatically generated by the linker,	// which points to the end of the kernel's bss segment:	// the first virtual address that the linker did *not* assign	// to any kernel code or global variables.	if (!nextfree) {                     		extern char end[];		nextfree = ROUNDUP((char *) end, PGSIZE);	}	// Allocate a chunk large enough to hold 'n' bytes, then update	// nextfree.  Make sure nextfree is kept aligned	// to a multiple of PGSIZE.	result = nextfree;	nextfree = ROUNDUP(nextfree+n, PGSIZE);	if((uint32_t)nextfree - KERNBASE &gt; (npages*PGSIZE))		panic("Out of memory!\n");	return result;}nextfree表示下一个未用的虚拟地址, 是一个静态变量。当mem_init调用kern_pgdir = (pde_t *) boot_alloc(PGSIZE)时，nextfree还未初始化，它会被初始化在内核.bss段的结束，并与页面大小4096B 对齐。这里使用了函数ROUNDUP(char* a, uint32_t n)，它同ROUNDDOWN一起在type.h中定义，分别是求 a/n的向上和向下取整，因此，ROUNDUP可以用来将地址a与n对齐。用result保存nextfree作为起始地址后，将nextfree向后移动n个字节（也要和PGSIZE对齐），作为下次分配的起始地址。在分配时，还要检查是否是一个合法的虚拟地址。 从上面JOS虚拟地址空间布局，我们知道nextfree-KERNBASE实际上就是nextfree的物理地址，这个物理地址不可超过内存可用的物理空间大小(页框总数npages*页面大小PGSIZE)，否则抛出错误。回到mem_init中，	kern_pgdir = (pde_t *) boot_alloc(PGSIZE);	memset(kern_pgdir, 0, PGSIZE);将内核页表分配在虚拟地址空间中内核.bss段的后面，然后用memset将页表初始化为全0.kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P;在虚拟地址布局中，我们看到从[UVPT, ULIM]（大小为一个页表大小）这一段是用户和内核都可读的页表目录的复制，那么就要将虚拟地址UVPT映射到kern_pgdir的真实物理地址上去， 而要完成这种映射，就是要在kern_pgdir页表目录中，对应虚拟地址UVPT的条目中，将页表地址改为kern_pgdir的物理地址。 这样，当用户或内核访问UVPT与ULIM之间的虚拟地址时，就要首先访问kern_pgdir，查找uvpt对应的物理地址，然后发现该物理地址就是kern_pgdir所在的物理地址。PDX(la)在mmu.h中定义，计算la对应的页表目录索引。PADDR是将传入的虚拟地址减去KERNBASE，得到物理地址。 PTE_U表示用户有权限（则内核也有权限），PTE_P表示物理地址存在。这个语句将页表目录中UVPT起始的页面对应的条目置为页表目录的物理页面地址， 并设置用户可读。接下来，初始化pages数组，调用boot_alloc将它分配在内核页表目录kern_pgdir之后，并用memset初始化为全0.	pages = (struct PageInfo *) boot_alloc(npages * sizeof(struct PageInfo));	memset(pages, 0, npages * sizeof(struct PageInfo));pages数组用来一对一地记录每一个物理页框是否被占用，可以通过pages[i].pp_ref来判断。之后，mem_init调用page_init().page_init()我们已经初始化了页表目录和pages数组，则page_init()的任务就是初始化page_free_list，记录哪一些物理页框是空闲的，并设置pages中每一个页框的结构。page_free_list实际上只是一个PageInfo结构体，此结构体中包含了指向下一个的指针，也就是下一个空闲的页框。所以，我们可以遍历pages数组，将那些已经分配出去的页框pp_ref置为1，将空闲的页框pp_ref置为0，并让page_free_list指向这个页框，从而将它插入空闲页框链表。根据注释提示，第一个物理页框已经分配给中断向量表和其他的BIOS结构，basemem中剩下的部分([PGSIZE, npages_basemem*PGSIZE])还是空闲的。extmem中，我们刚才已经分配了一部分给内核，要知道分配了多少，我们可以调用boot_alloc(0)来获取分配完pages之后，下一个可用的虚拟地址，将它减去KERNBASE得到物理地址，再除以PGSIZE就得到分配出去的页框数目。IOhole部分，也就是从640KB到1MB之间的96个页面，都分配给了外部IO设备。IOhole和extmem是连续的，因此page_init的实现如下：voidpage_init(void){	//  1) Mark physical page 0 as in use.	//     This way we preserve the real-mode IDT and BIOS structures	//     in case we ever need them.  (Currently we don't, but...)	//  2) The rest of base memory, [PGSIZE, npages_basemem * PGSIZE)	//     is free.	//  3) Then comes the IO hole [IOPHYSMEM, EXTPHYSMEM), which must	//     never be allocated.	//  4) Then extended memory [EXTPHYSMEM, ...).	//     Some of it is in use, some is free. Where is the kernel	//     in physical memory?  Which pages are already in use for	//     page tables and other data structures?	//	size_t i;	page_free_list = NULL;	//num_alloc：在extmem区域已经被占用的页的个数	int num_alloc = ((uint32_t)boot_alloc(0) - KERNBASE) / PGSIZE;	//num_iohole：在io hole区域占用的页数	int num_iohole = 96;	for(i=0; i&lt;npages; i++)	{	    if(i==0)	    {	        pages[i].pp_ref = 1;    //第一个页面已经被占用	    }    	    else if(i &gt;= npages_basemem &amp;&amp; i &lt; npages_basemem + num_iohole + num_alloc)	    {   //从IOhole到extmem中已分配的部分	        pages[i].pp_ref = 1;	    }	    else	    {  //页面空闲，将pp_ref=0，将pp_link置为下一个空闲页面的指针，这样，当该页面被分配出去的时候，我们可以让page_free_list指向pp_link来将它从链表中移出。	        pages[i].pp_ref = 0;	        pages[i].pp_link = page_free_list;	        page_free_list = &amp;pages[i];   //将pages[i]插入到page_free_list的头部。	    }	}}page_alloc()在pages设置好之后，分配页面就不可以再调用boot_alloc了，必须调用page_alloc通过在page_free_link查找空页框的方式来分配页面。page_alloc分配一个页面，返回该页面的PageInfo指针。它同时接收一个参数alloc_flags, 如果它的值为1（ALLOC_ZERO), 就将分配到的物理页面设置为全0。如果没有可用页框，则返回空指针NULL。所以该函数的步骤为：  从page_free_list中取出一个空闲页框的PageInfo结构体。  将这个页框从page_free_list中移去，并将链表头指针指向下一个空闲页框。  修改取出的PageInfo相关信息，如果有ALLOC_ZERO, 修改该内存页。struct PageInfo *page_alloc(int alloc_flags){    struct PageInfo *result;       if (page_free_list == NULL)     //没有空闲页框，返回NULL。        return NULL;      result= page_free_list;       //将第一个空闲页框分配出去      page_free_list = result-&gt;pp_link;    //让page_free_list指向该页框的pp_link,也就是链表上的下一个空闲页框，从而将队首pop出去。      result-&gt;pp_link = NULL;           if (alloc_flags &amp; ALLOC_ZERO)        memset(page2kva(result), 0, PGSIZE);  //用memset将该页面设置为全0.      return result;}其中，page2kva()函数是将传进去的result加上KERNBASE, 得到result的物理地址。 memset将result对应的物理地址开始，一个页面大小的物理内存设置为0.page_free()这个函数将一个被分配的页框归还，只有当该页框的引用位pp_ref为0时，才可以调用：voidpage_free(struct PageInfo *pp){    // Fill this function in    // Hint: You may want to panic if pp-&gt;pp_ref is nonzero or    // pp-&gt;pp_link is not NULL.      assert(pp-&gt;pp_ref == 0);      assert(pp-&gt;pp_link == NULL);      pp-&gt;pp_link = page_free_list;      page_free_list = pp;}assert是断言，用来判断条件是否满足，否则发出panic错误。当页框在page_alloc中被分配出去时，会将pp_link设置为NULL，而页框不再被使用时，pp_ref会置回0，只有这两个条件满足才可以调用page_free.page_free只要简单地将页框插入到page_free_list的表头即可，为此，将pp_link指向现在的链表头部：page_free_list, 然后将头部指针指向该页框（pp).TODO 2:  Virtual Memory参考pgdir_walk, boot_map_region 和page_insert函数，实现page_lookup和page_remove。首先阅读这三个函数的代码，为了方便解释代码，先看一下mmu.h中提供的一些宏，以及pmap.h中提供的功能函数。宏与功能函数在types.h中定义了与内存管理相关的类型：typedef uint32_t uintptr_t;    //表示虚拟地址typedef uint32_t physaddr_t;    //表示物理地址// Page numbers are 32 bits long.typedef uint32_t ppn_t;   //表示页面编号的类型typedef uint32_t pte_t;   //表示一个页表条目的类型typedef uint32_t pde_t;   //表示一个页目录中的条目的类型对于一个虚拟地址va，如果它在KERNBASE以上，说明它是一个内核的虚拟地址，而内核部分是始终驻留在内存中的，我们可以使用pmap.h中定义的PADDR(va)直接将其减去KERNBASE得到物理地址。如果它不在KERNBASE上，那么就要通过MMU访问页表来将它转换成物理地址。相应的，如果是一个内核的物理地址pa, 才可以使用KADDR将它加上KERNBASE得到虚拟地址。在mmu.h中，定义了一些宏，方便从一个虚拟地址获得对应的页目录、页表条目信息以及物理地址：  PGNUM(la): 表示一个虚拟地址的页编号，因为每个页是4096字节，又编号是从0开始顺序编号的，只要将la右移12位。  PDX(la)：对应页目录索引  PTX(la): 对应页表索引  PGOFF(la): 在页面内的偏移  PGADDR(d,t,o): 从已知的页目录索引、页表索引和页内偏移还原一个虚拟地址。  PTE_ADDR(pte): 从一个页目录条目或者一个页表条目中取出它的高20位物理地址部分。在pmap.h中，函数page2pa()实现了给出一个页面，获取这个页面开始处的物理地址； 函数pa2page()实现了给出一个与页大小对齐的物理地址，返回它所在的页面的PageInfo。pgdir_walk()这个函数的功能是，给出一个虚拟地址va， 访问两级页表，找到它对应的页表条目，返回页表条目的指针。但是，由于页表不是一直都整个驻留在内存中的，所以va对应的条目所在的页表页可能还不在内存中，这时，如果create为False，就返回空指针，否则就要使用page_alloc()函数，分配这个页表页。这个过程是：  从虚拟地址va得到它的页目录索引  在页目录上根据索引找到对应条目  判断该条目的present位是否为1， 如果置位，说明对应的页表在内存中，否则不在          如果create置位，要在内存中为这个页表分配一个页框，使用page_alloc().                  如果分配不成功，只能返回NULL。          分配成功，要将这个页表页的引用数pp_ref 加上1， 因为我们现在正要从页表上查va对应的条目。并且，还要在页目录中，记录这个页表的物理地址，设置present为1，并设置权限为用户可读写。                    create为0，返回NULL。        找到了页表后，计算va对应的页表索引  获取页表上该条目，返回条目的地址。（这里所说的地址是该条目的虚拟地址）pte_t * pgdir_walk(pde_t *pgdir, const void * va, int create){      unsigned int page_off;    //页偏移      pte_t * page_base = NULL;    //页表页的基址（虚拟地址）      struct PageInfo* new_page = NULL;    //可能需要分配页表页            unsigned int dic_off = PDX(va);    //用PDX获取va的页目录索引      pde_t * dic_entry_ptr = pgdir + dic_off;    //页目录中对应的条目的虚拟地址，等于页目录的起始地址+条目在页目录中的索引号      if(!(*dic_entry_ptr &amp; PTE_P))    //*dic_entry_ptr获取目录条目中的内容，与PTE_P 判断present是否置位，也就是该页表是否在内存中，如果不在：      {            if(create)   //在内存中分配该页表页            {                   new_page = page_alloc(1);   //分配一个页表页，参数为1，表示该片内存被初始化为全0（之后会将该页表从磁盘读取到这片内存，这不是MMU的工作）                   if(new_page == NULL) return NULL;  //分配失败                   new_page-&gt;pp_ref++;      //分配成功，则该页面的引用数要增加1.                   *dic_entry_ptr = (page2pa(new_page) | PTE_P | PTE_W | PTE_U);// 使用page2pa(newpage)获得该页的起始物理地址，将它存放在对应的页目录条目中，并且置present为1， 更改权限为用户可以读写。            }           else               return NULL;            }       //用PTX(va)宏，获取va对应的页表索引，这就是要返回的条目在页表中的偏移。      page_off = PTX(va);    //page_base用来表示该页表页所在的虚拟地址。要获得此虚拟地址，首先要从页目录表上获得该页表页的物理地址，再用KADDR转换成虚拟地址。      page_base = KADDR(PTE_ADDR(*dic_entry_ptr));      return &amp;page_base[page_off];}最后三行代码最为关键，经过上面的判断和分配页表页，现在该页表页的物理地址已经存放在页目录对应的条目中了， 用PTE_ADDR(*dic_entry_ptr)就可以从条目中取出该页表页的物理地址。用KADDR可以将物理地址转换为页目录的虚拟地址，这时，其实将page_base+page_off就可以得到该条目的虚拟地址了。因此，最后return &amp;page_base[page_off]也可以替换为return page_base+page_off。boot_map_region()boot_map_region的功能是，将虚拟地址[va, va+size]映射到物理地址[pa, pa+size]上，意思就是在页表中[va, va+size]对应的条目中设置物理地址为[pa, pa+size]。 这里va, pa,size都是保证与页面大小对齐的，size的单位是页。perm参数给出了这块内存空间的权限。这个函数是用来“静态“地映射UTOP以上的用户只读空间的。过程是：  遍历从[va, va+size]的每一个虚拟页，使用pgdir_walk找到它在页表中的对应条目                              将该条目的内容设置为 [pa , pa+size ]          perm          PTE_P. 表示present置位，这些页面存在于内存中，并设置了权限。                    static voidboot_map_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm){    int nadd;    pte_t *entry = NULL;    for(nadd = 0; nadd &lt; size; nadd += PGSIZE)    {        entry = pgdir_walk(pgdir,(void *)va, 1);    //Get the table entry of this page.        *entry = (pa | perm | PTE_P);                            pa += PGSIZE;        va += PGSIZE;            }}page_insert()这个函数是将一个物理页框pp映射到虚拟地址va上。要考虑两种情况，一是该va已经映射到了其他的物理页框上，这时就要接触这个映射关系； 另一种是该va本来就映射到pp上了。过程是：  调用pgdir_walk得到va在页表上的条目。  如果找不到该条目，说明内存不足，返回错误码 -E_NO_MEM。  要先让pp-&gt;pp_ref++，之后解释原因。  如果该条目已经存在，说明va本来已经映射到一个物理页框上，          tlb_invalidate使该va对应的页表条目失效，这样，才不会使进程在这个过程中访问到不正确的物理地址。这是因为进程会缓存它用到的页表，快速访问页表时，它先访问缓存中是否有该页表，如果没有，才从页目录去找。      page_remove解除va和原来物理页框的映射。        无论之前该条目是否存在，现在va已经不与任何物理页框绑定，将条目内容设置为pp的物理地址，设置present为1，并设置权限为permintpage_insert(pde_t *pgdir, struct PageInfo *pp, void *va, int perm){      pte_t *entry = NULL;    entry =  pgdir_walk(pgdir, va, 1);    //Get the mapping page of this address va.    if(entry == NULL) return -E_NO_MEM;    pp-&gt;pp_ref++;    if((*entry) &amp; PTE_P)             //If this virtual address is already mapped.    {        tlb_invalidate(pgdir, va);        page_remove(pgdir, va);    }    *entry = (page2pa(pp) | perm | PTE_P);    pgdir[PDX(va)] |= perm;                  //Remember this step!            return 0;}在这个函数中，只用(*entry) &amp; PTE_P判断页表条目是否存在，来判断va是否已经有映射关系，但没有区分va是否与pp映射。如果我们将pp-&gt;pp_ref++移到if块后面，那么当va已经与pp映射时，pp原来的引用数有可能为0，在if中，我们会不加判断地调用page_remove，然后因为引用数为0，直接调用page_free将它释放掉。 既然释放了，它就会处在空闲链表page_free_list中，pp_ref应该保持为0. 我们之后再用pp-&gt;pp_ref++时，就会让空闲链表管理出错，下一次分配页框时，可能在空闲链表中找到这个pp，但它是不可用的。接下来我们实现page_lookup和page_remove函数。page_lookup()这个函数的功能是给出虚拟地址va，找到它映射到的物理页框。如果传入的pte_store不为NULL的话，就将该虚拟地址对应的页表条目指针存放到pte_store中。实现过程是：  调用pgdir_walk找到va对应的条目，这里create应该设置为0，即若va所在的页表页不在内存，我们也不分配它。  如果返回的是NULL，表示va所在的页表页不在内存，即va现在没有映射到物理页框，返回NULL。  如果va对应的页表页在内存中，但是条目的present位为0，说明va没有映射到物理页框:          判断pte_store，如果不为NULL，将条目存放在pte_store中      返回NULL        用PTE_ADDR从条目上获得va对应的物理地址  用pa2page获取物理地址对应页框的PageInfo  如果pte_store非空，将pte保存，最后返回PageInfostruct PageInfo *page_lookup(pde_t *pgdir, void *va, pte_t **pte_store){	pte_t *pte = NULL;	struct PageInfo *page;    pte = pgdir_walk(pgdir, va, 0);     //不存在，不分配    if(pte==NULL)            //页表页不存在        return NULL;        if(pte_store)              *pte_store = pte;    if(!(*pte &amp; PTE_P)) //页表页存在，但是va并不与一个物理页框映射        return NULL;    	page = pa2page(PTE_ADDR(*pte));	return page;}page_remove()这个函数的功能是解除va与它对应的物理页框之间的映射关系。这不一定说明该物理页框已经空闲，可以回到空闲链表中被分配。因为该页框的引用数并不一定为0，例如在共享内存时，有可能不同进程会共享一部分物理内存，不同的虚拟地址会映射到同一个物理页框上。因此，这个函数的实现过程应该是：  调用page_lookup查找va对应的物理页，并保存其页表条目的地址在&amp;pte中。  如果该va有映射到某个物理页框，解除映射：          调用page_decref，这里面做的是，将pp_ref减去1，如果等于0，可以调用page_free释放空间，将页框归还。      因为使用快速页表访问时，进程可能缓存了最近使用过的页表条目，所以要调用tlb_invalidate让这条va的缓存条目无效，否则进程会优先访问缓存中的条目，进而访问到非法的物理地址。      将va对应的条目内容清0.      voidpage_remove(pde_t *pgdir, void *va){    pte_t *pte;	struct PageInfo *page = page_lookup(pgdir, va, &amp;pte);		if(page){		page_decref(page);		tlb_invalidate(pgdir, va);   //使TLB中可能缓存的这条页表条目无效		*pte=0;	}}检查pmap.c中实现了几个函数对代码进行了检查。这些检查函数在mem_init中被调用，其中check_page()是检查分页的基础功能是否已经实现好，包括page_alloc(), page_insert(), page_remove()，page_lookup(), pgdir_walk() 以及page_free()。重新编译并启动qemu，看到控制台输出：” check_page() succeeded!” 表明实现是正确的。TODO 3: Kernel Address Space###完成mem_init()上面的函数已经完成了分页机制，页表也已经创建好。现在，我们就可以通过修改页表上的条目，完成UTOP以上用户不可操作的空间与物理地址的映射。首先，将[UPAGES, UVPT]这部分虚拟地址映射到pages数组上，权限设置为内核与用户只读，相当于为用户保留了物理页框信息的拷贝。这样，虚拟地址空间上实际有两份pages，一部分在KERNBASE以上，用户不可见，内核可读写，这一份就是在mem_init刚开始分配的pages；另一份是为用户准备的只读拷贝，两者通过页表映射到同一片物理内存上，但这份拷贝设置的权限是只读，所以用户不能对这部分虚拟地址的内容进行操作；又因为用户不可访问KERNBASE上面的虚拟地址（在用户访问虚拟地址时，内核会判断虚拟地址是否超出ULIM），所以用户不可读写pages。这样就实现了对pages数组的保护。这个映射用boot_map_region(kern_pgdir, UPAGES, PTSIZE, PADDR(pages), PTE_U);来实现。然后要完成内核栈的映射。从[KSTACKTOP - PTSIZE, KSTACKTOP]这部分都属于内核栈， 但被分为两个部分，上面[KSTACKTOP-KSTKSIZE, KSTACKTOP]是真正的内核栈，与某些物理页框映射，而[KSTACKTOP - PTSIZE, KSTACKTOP-KSTKSIZE ]不与物理地址映射，只是用来防止内核栈向下增长的时候发生溢出，然后覆盖了Memory-mapped IO部分，称为保护页。如果内核栈溢出，它就会发现物理地址不存在，抛出错误。 *    KERNBASE, ----&gt;  +------------------------------+ 0xf0000000      --+ *    KSTACKTOP        |     CPU0's Kernel Stack      | RW/--  KSTKSIZE   | *                     | - - - - - - - - - - - - - - -|                   | *                     |      Invalid Memory (*)      | --/--  KSTKGAP    | *                     +------------------------------+                   | *                     |     CPU1's Kernel Stack      | RW/--  KSTKSIZE   | *                     | - - - - - - - - - - - - - - -|                 PTSIZE *                     |      Invalid Memory (*)      | --/--  KSTKGAP    | *                     +------------------------------+                   | *                     :              .               :                   | *                     :              .               :                   | *    MMIOLIM ------&gt;  +------------------------------+ 0xefc00000      --+ *                     |       Memory-mapped I/O      | RW/--  PTSIZE * ULIM, MMIOBASE --&gt;  +------------------------------+ 0xef800000内核栈是内核可读写，但用户不可见的，这些页面的权限要被设置为PTE_W。调用boot_map_region, 将KSTACKTOP-KSTKIZE开始到KSTACKTOP的虚拟地址映射到bootstack的物理地址上，大小如上面结构所示，为KSTKSIZE，但要注意使用ROUNDUP与页面大小对齐：boot_map_region(kern_pgdir, KSTACKTOP-KSTKSIZE, ROUNDUP(KSTKSIZE, PGSIZE), PADDR(bootstack), PTE_W);将整一个[KERNBASE, 2^32）的整个内核地址空间映射到内存的[0, 2^32-KERNBASE)，权限是内核可修改但用户不可见。我们知道KERNBASE的虚拟地址是0xf0000000, 而整个虚拟空间的大小是2^32也就是4G，所以内核的大小总共是256MB=0X10000000，这已经是与页面大小对齐的。 内存将一直有256MB的空间被内核占用。调用boot_map_region实现如下：boot_map_region(kern_pgdir, KERNBASE, 0x10000000, 0, PTE_W);检验重新编译，启动QEMU，所有的检查都已经通过，说明分页机制与内核的分配已经正确实现：总结分页机制建立和开启全过程：完整地阅读mem_init，voidmem_init(void){	uint32_t cr0;	size_t n;	i386_detect_memory();	kern_pgdir = (pde_t *) boot_alloc(PGSIZE);	memset(kern_pgdir, 0, PGSIZE);    	kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P;	pages = (struct PageInfo *) boot_alloc(npages * sizeof(struct PageInfo));	memset(pages, 0, npages * sizeof(struct PageInfo));	page_init();	boot_map_region(kern_pgdir, UPAGES, PTSIZE, PADDR(pages), PTE_U);	boot_map_region(kern_pgdir, KSTACKTOP-KSTKSIZE, ROUNDUP(KSTKSIZE, PGSIZE), PADDR(bootstack), PTE_W);    	boot_map_region(kern_pgdir, KERNBASE, 0x10000000, 0, PTE_W);	lcr3(PADDR(kern_pgdir));	cr0 = rcr0();	cr0 |= CR0_PE|CR0_PG|CR0_AM|CR0_WP|CR0_NE|CR0_MP;	cr0 &amp;= ~(CR0_TS|CR0_EM);	lcr0(cr0);}内核部分虚拟地址空间的初始化全过程是：      检测总共可用的物理内存大小，由basemem+extmem构成，记录总页数npages和低地址页数npages_basemem        要初始化内核虚拟地址，必须通过页表来映射，但一开始，页表还不存在，页表的虚拟地址也还没有映射。因此，首先要分配页表目录的虚拟地址。这个分配无法通过分页机制来完成，只能通过静态映射：kern_pgdir = boot_alloc(PGSIZE). 页目录分配好后，首先初始化为全0.        因为用户进程在访问虚拟地址时，需要访问页表，因此我们需要在ULIM以下为用户准备一份页表的只读拷贝。然而，总共1024份页表的开销较大，其实只要能够访问到页目录，就可以通过页目录访问到页表。 而我们甚至不用真的在内存中存放两份页目录，只需要将拷贝的虚拟地址也指向kern_pgdir的物理地址即可。 所以，我们在页目录上让UVPT条目指向页目录本身，并设置用户只读：kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P;, 结构如图：                这里CR3寄存器存放的是进程页目录的虚拟地址。操作系统中有内核页表和进程页表两种页表，进程页表是每个进程独自有一份的，页目录的虚拟地址存放在cr3寄存器中，当进程切换时，会加载页目录虚拟地址到cr3寄存器。进程页表既包含了用户态，也包含了内核态的虚拟地址，内核态的虚拟地址是所有进程都一样的，就是内核页表的拷贝，它的虚拟地址就在UVPT，大小为一个页面。            拷贝好内核页目录后，因为接下来涉及到管理物理内存，需要记录每个页框的信息，这些信息保存在pages数组中。同样，需要用boot_alloc将pages静态映射到一片虚拟地址。如果打印出pages和kern_pgdir的虚拟地址，可以看到他们的虚拟地址分别是f0119000和f0118000， 是在f0400000之内的，这两个数据结构已经在物理内存中了。        将pages初始化为0，然后用page_init将已经分配出去的物理页框引用位置为1，并将空闲物理页框添加到page_free_list链表中，接下来，内核就可以使用page_free_list和pages两个数据结构管理物理页框。        允许用户读取pages来知道内存的占用情况，但为了保护该数据结构，同样要在ULIM以下为它保留一份拷贝，这份拷贝在UPAGES到UVPT之间，用boot_map_region来进行映射。        boot 的时候， 已经为内核初始化了一个栈，栈顶的界限是bootstack，用boot_map_region将虚拟地址的KSTACK部分映射到bootstack所在的物理内存上，进程进入内核态，并进入公共部分时，实际上运行在这个内核栈上。        最后，把整个高地址部分的内核虚拟空间映射到物理内存0地址开始处，实际上是内核部分一直驻留在内存中，且内核的虚拟地址空间被拷贝到每个进程的高地址部分。        lcr3(PADDR(kern_pgdir))是把内核页目录基址放在cr3寄存器中，之后如果开启分页，访问虚拟地址时，就会从cr3加载页目录地址，从而访问页目录。        将控制寄存器cr0置位为：CR0_PE|CR0_PG|CR0_AM|CR0_WP|CR0_NE|CR0_MP ，~(CR0_TS|CR0_EM)各个字段含义如下：                  #define CR0_PE		0x00000001	// Protection Enable#define CR0_MP		0x00000002	// Monitor coProcessor#define CR0_EM		0x00000004	// Emulation#define CR0_TS		0x00000008	// Task Switched#define CR0_ET		0x00000010	// Extension Type#define CR0_NE		0x00000020	// Numeric Errror#define CR0_WP		0x00010000	// Write Protect#define CR0_AM		0x00040000	// Alignment Mask#define CR0_NW		0x20000000	// Not Writethrough#define CR0_CD		0x40000000	// Cache Disable#define CR0_PG		0x80000000	// Paging                    cr0被设置为，开启保护模式，保护模式开启时只是开启了段级保护，没有开启分页，即逻辑地址转换成线性地址，线性地址直接等于物理地址, CR0_PG开启分页，CR0_AM 开启地址对齐检查， 开启写保护， 开启协处理器错误， 开启监控协处理器。TS任务已切换标志为0，EM为0，表示有协处理器，会将浮点指令交给协处理器用软件来模拟。            完成。  如何保护内核数据和代码：  通过虚拟地址空间的隔离。检查用户访问的虚拟地址与ULIM，可以防止用户访问高地址。  为ULIM以下的每个页面设置权限，并启用cr0中的非法写保护，可以防止无权限用户修改只读页面。如何管理内存空闲空间，以及管理的开销：  通过一对一地维护每个页框的信息，并动态维护一个空闲页框链表（头指针）来实现。  每个页框PageInfo的大小是8B，pages的大小是PTSIZE=4MB, 总共可存放512K个PageInfo，即可维护512K个物理页，总共512K*PGSIZE=2G物理内存。  如果全部2G的物理内存都分配出去，那么维护的开销是，pages大小+kern_pgdir大小+所有页表页大小=4MB+4K+2MB的额外内存。推荐阅读： OS操作系统实验 xv6调度算法实现]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> JOS </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OS操作系统实验：xv6调度(RR, 优先级调度, 优先级队列） 实现和分析 Part 1- xv6代码讲解]]></title>
      <url>/lab%20report/2020/01/31/OS4/</url>
      <content type="text"><![CDATA[Lab 4：调度调度任何操作系统中，都可能出现进程的个数大于处理器个数的情况，这就需要考虑如何分配处理器资源。一般进程的执行是CPU计算和IO操作的循环，当进程长时间等待某种资源时，为了更好地利用CPU资源，应选择其他准备好的进程来代替它；当进程完成所需的时间过长时，为了让其他已经在系统中的进程等待时间不要过长，也需要在某个适当的时间暂停当前的进程, 因此便需要多进程并发，也就需要调度。调度指的是决定一个进程在什么时候、是否要暂停，从一个等待队列中选择另一个进程来代替它，调度涉及调度策略的选择，也包含完成进程切换的动作。操作系统通过不断地调度，造成并发的效果，同时也为每个进程造成独占资源的假象。调度涉及以下的问题：  如何进行进程的切换？这是通过上下文的切换来实现的。上下文是一个进程运行状态的描述，包括程序计数器(%eip), 栈底指针(%ebp), 以及其他一些寄存器的值。在进程切换时，首先要保存旧进程的上下文在内核栈上，选择一个新进程，从该进程的内核栈上加载它的上下文，然后CPU就开始执行新进程%eip指向的指令。上下文的保存和加载使得程序可以从上次调度被暂停的地方接着进行，对进程来说，就好像切换从来没有发生过一样。  如果进程不是调用sleep主动放弃CPU，如何让进程的切换透明化呢？ xv6简单地使用时钟中断来完成。当时钟中断到来时，进程陷入中断处理程序，在内核中调用yield来进行上下文切换。  多个CPU同时在切换进程时，由于需要对进程表进行修改，可能会产生竞态条件，因此还要用锁来避免竞争。  进程结束时，需要释放资源。进程不能自己释放自己的所有资源，因此内核中还必须有一个角色负责监测进程的结束、释放资源。  当进程调用sleep进入睡眠时，调度也会发生。这时，要确保有其他进程可以唤起该进程，因此xv6需要提供一套进程间通信的机制，例如sleep和wake up。本次实验将详细研究整个调度的过程，看xv6如何解决上述问题，并实现优先级调度算法。在开始实验之前，需要了解以下事实： xv6永远不会从一个用户态进程切换到另一个用户态进程。在xv6中，调度发生在以下几种情况：1. 进程调用sleep进入休眠，主动放弃CPU，这会导致进程进入内核态，保存进程的上下文并加载调度器的上下文，当调度器返回时，该进程仍处于内核态；2. 进程收到时钟中断，已经运行完一个时间片，这也会导致进程进入内核态，并在yield中将控制权交给调度器；3. 进程调用exit结束。 在这些情况下，切换的过程都是 陷入内核→保存上下文→切换到调度器的上下文 → 切换到新进程的上下文（在内核态中） → 返回新进程的用户态。TODO1: 阅读proc.c中的函数我们先看调度发生的一般场景：进程运行完时间片，被迫放弃CPU，选择下一个进程调度。xv6的时钟每100毫秒就产生一个中断，以此实现进程时间分片。时钟中断是由lapic产生的，因此每个cpu可以独立地接收它的时钟中断。当接收到时钟中断时，进程会开启保护模式，陷入到内核态，来到中断处理程序的入口，然后在alltraps中保存中断帧，调用traps，traps根据中断号来判断应该执行哪种程序。在traps的最后，有可能调用yield使进程放弃CPU：  // Force process to give up CPU on clock tick.  // If interrupts were on while locks held, would need to check nlock.  if(myproc() &amp;&amp; myproc()-&gt;state == RUNNING &amp;&amp;     tf-&gt;trapno == T_IRQ0+IRQ_TIMER)   //当目前CPU上有正在运行的进程，且中断为时钟中断时，才会调用yield    yield();yieldyield函数在proc.c中实现:// Give up the CPU for one scheduling round.voidyield(void){  acquire(&amp;ptable.lock);  //DOC: yieldlock  myproc()-&gt;state = RUNNABLE;  sched();  release(&amp;ptable.lock);}yield是将目前进程的状态从RUNNING改为RUNNABLE，让进程进入等待队列，然后调用sched将控制权转移给调度器。由于进程PCB存放在进程表上，因此对状态进行修改之前要首先acquire(&amp;ptable.lock)获取进程表的锁，等进程再次被调度时，它会返回到yield中sched的下一行，释放进程表锁。这里要注意，在上一个实验中，我们知道xv6的内核中临界节内不允许中断，所以在进入sched之前，中断是已经关闭的状态。schedsched的任务是首先确保进程有放弃CPU，进行调度的条件，然后调用swtch进行上下文切换，转到cpu调度器scheduler上。// Enter scheduler.  Must hold only ptable.lock// and have changed proc-&gt;state. Saves and restores// intena because intena is a property of this// kernel thread, not this CPU. It should// be proc-&gt;intena and proc-&gt;ncli, but that would// break in the few places where a lock is held but// there's no process.voidsched(void){  int intena;  struct proc *p = myproc();  if(!holding(&amp;ptable.lock))    panic("sched ptable.lock");  if(mycpu()-&gt;ncli != 1){    panic("sched locks");  }  if(p-&gt;state == RUNNING)    panic("sched running");  if(readeflags()&amp;FL_IF)    panic("sched interruptible");  intena = mycpu()-&gt;intena;  swtch(&amp;p-&gt;context, mycpu()-&gt;scheduler);  mycpu()-&gt;intena = intena;}首先，holding(&amp;ptable.lock)判断该cpu是否已经持有了进程表的锁，因为多个CPU在调度过程中都需要访问进程表，假如这个cpu进入调度之前，没有先持有锁，那就有可能使其他cpu也同时进行调度，同时访问进程表，可能会出现两个cpu选择了同一个进程调度的情况。然后mycpu()-&gt;ncli!=1判断该cpu调用pushcli的次数是否恰好为1，否则会报错。在上个实验我们知道，每一次调用acquire获得锁，就会使ncli加一，释放锁后ncli减一。所以ncli为1就说明这里只允许进程持有一个锁，也就是说，进程被切换时，必须持有进程表的锁，并且必须释放其他所有的锁。持有进程表的锁是为了保证CPU的调度是互斥的，防止竞态条件，而释放其他所有锁是为了防止死锁的情况出现。如果p-&gt;state==RUNNING，进程的状态是仍在运行，不可以进入调度。这是操作系统中约定好的分工：sched应该只负责进入调度器，而不应该判断进程是因为什么原因而被暂停的，所以假如进程是终止了，应该由exit来将状态变为ZOMBIE，如果进程是被时钟中断了，应该由yield将状态变为RUNNABLE，休眠也同理。进入sched之前，进程的状态应该已经改变好。sched最后通过readeflags()FL_IF，检查标志寄存器中IF段的值，确保中断已经关闭，然后它将mycpu()-&gt;intena暂时保存起来，这个变量表示CPU在调用yield之前，中断是否被允许，因为之后在调度器中要调用sti开启中断，可能会破坏原来CPU的中断状态，所以暂存起来，等从调度器返回（进程被重新调度）的时候，再恢复这个值。sched调用swtch(&amp;p-&gt;context, mycpu()-&gt;scheduler)来切换上下文，swtch的汇编代码如下：# Context switch##   void swtch(struct context **old, struct context *new);# # Save the current registers on the stack, creating# a struct context, and save its address in *old.# Switch stacks to new and pop previously-saved registers..globl swtchswtch:  movl 4(%esp), %eax  movl 8(%esp), %edx              # swtch的第二个参数，即新的上下文  # Save old callee-saved registers  pushl %ebp                      # 保存旧进程内核栈的栈底指针  pushl %ebx                      # 保存旧进程%ebx寄存器  pushl %esi					# 保存旧进程%esi  pushl %edi					#               和%edi寄存器  # Switch stacks  movl %esp, (%eax)  movl %edx, %esp				#   # Load new callee-saved registers  popl %edi  popl %esi  popl %ebx  popl %ebp  ret首先，进程上下文中包含的信息有：struct context {  uint edi;  uint esi;  uint ebx;  uint ebp;  uint eip;};为什么只需要保存这些呢？假设进程在某个函数中，发生了上下文切换，那么首先不需要保存的是调用者保存的寄存器，如%eax，%ecx，%edx，因为该函数的调用者已经提前把它们保存在进程的栈上了。也不需要保存段寄存器，如%cs等，因为在指令地址发生改变的时候，这些寄存器也会同时改变。所以，要保存的有栈底指针、%ebx、程序计数器%eip、参数寄存器%edi、%esi。上下文保存在进程内核栈上：在进程从yield进入到sched再进入到swtch的这个时候，cpu首先是运行在旧进程的内核栈上的。在这里，swtch传入两个参数，第一个是旧进程上下文的指针的地址，第二个是该cpu调度器进程的上下文的指针，调度器的上下文也是调度器上一次调用swtch时保存的。我们逐条指令分析上下文切换的过程：swtch:  movl 4(%esp), %eax                  # 第一个参数  movl 8(%esp), %edx                  # 第二个参数这两条指令中，%esp指向旧进程内核栈现在的栈底，因为它调用了swtch，所以（%esp）上存放的是sched中swtch的返回地址。4(%esp)和8(%esp)分别是swtch的第一个和第二个参数，也就是旧进程上下文和新进程（调度器）上下文的指针。画出旧进程内核栈：                   …                  旧进程上下文指针      p-&gt;context              调度器上下文指针      mycpu()-&gt;scheduler              %esp→      mycpu()-&gt;intena = intena 对应指令的地址                            # Save old callee-saved registers  pushl %ebp                      # 保存旧进程内核栈的栈底指针  pushl %ebx                      # 保存旧进程%ebx寄存器  pushl %esi					# 保存旧进程%esi  pushl %edi					#               和%edi寄存器这四条指令是将旧进程的上下文保存到当前栈（旧进程的内核栈）上。                   …                  旧进程上下文指针      &amp;p-&gt;context              调度器上下文指针      mycpu()-&gt;scheduler                     mycpu()-&gt;intena = intena 对应指令的地址                     ebp                     ebx                     esi              %esp→      edi                            # Switch stacks  movl %esp, (%eax)       # 令p-&gt;context = %esp  movl %edx, %esp				然后交换栈。从第一、二条指令我们知道，现在（%eax）中是旧进程的上下文指针，令（%eax）=%esp，也就是让旧进程上下文指针重新指到现在它保存的地方。而%edx中是调度器上下文指针，把%edx赋给%esp，使栈底指针指向了调度器上下文所在的位置，这样，就从旧进程的内核栈切换到了调度器所在的栈（前者是代表用户进程的，是用户进程在内核态下运行时使用的栈，后者不代表任何用户进程，它是内核进程进行时使用的栈）。            (旧进程的内核栈)      …                  旧进程上下文指针      &amp;p-&gt;context              调度器上下文指针      mycpu()-&gt;scheduler              旧进程中swtch返回地址      mycpu()-&gt;intena = intena 对应指令的地址                     ebp                     ebx                     esi              p-&gt;context→      edi                                        （调度器所在的栈）      …                  调度器中swtch返回地址      ret              调度器的上下文      ebp                     ebx                     esi              %esp→      edi      栈切换之后，栈底指针指向调度器上下文所在的地址。现在，就可以从栈上pop出调度器的上下文了：# Load new callee-saved registers  popl %edi  popl %esi  popl %ebx  popl %ebp  ret最后，ret会返回到scheduler中，swtch的下一行位置（而不是返回sched)。这样，cpu控制权就从旧的进程转移到了调度器。从swtch指令中，我们没有看到对%eip的显式保存，这是因为旧进程的%eip在用call swtch调用swtch时就已经隐式地保存在了%ebp的前面，同样，ret指令也隐式地把调度器栈上的返回地址加载到了%eip中。schedulerscheduler就是上述的调度器。每一个进程最终都会将控制权返回到调度器，调度器会从等待队列中选择一个进程开始运行，它会调用swtch保存自己的上下文，然后切换到该进程的上下文开始运行。//PAGEBREAK: 42// Per-CPU process scheduler.// Each CPU calls scheduler() after setting itself up.// Scheduler never returns.  It loops, doing://  - choose a process to run//  - swtch to start running that process//  - eventually that process transfers control//      via swtch back to the scheduler.voidscheduler(void){  struct proc *p;  struct cpu *c = mycpu();  c-&gt;proc = 0;    for(;;){    // Enable interrupts on this processor.    sti();     //每一次从sched进入调度器，都会开启中断    // Loop over process table looking for process to run.    acquire(&amp;ptable.lock);    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;state != RUNNABLE)        continue;      // Switch to chosen process.  It is the process's job	    ........    }    release(&amp;ptable.lock);  }}调度器是一个两层的for循环。外层循环是无限循环，这意味着调度器永远不会返回。内层循环中，调度器遍历进程表，选择进程运行。在CPU开始的时候，它就会调用scheduler.scheduler每一次从内层循环退出，进入外层循环，都要显示地执行sti指令允许中断，并且要将ptable锁释放之后再重新获取。这两个步骤都是很有必要的，因为从内层循环退出，意味着调度器可能检查了一遍ptable，没有找到可以运行的进程，这时有可能所有的进程都在等待IO中断，如果不执行sti开启中断的话，IO中断永远也不能到达进程。而如果一个闲置CPU的调度器一直不释放锁，那么其他CPU也不能访问ptable，进行上下文或者系统调用了，所以就没有CPU能够将一个进程的状态改为RUNNABLE，这个CPU也无法跳出循环。scheduler内层循环遍历进程表，寻找下一个RUNNABLE的进程并切换到进程的上下文中运行。for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;state != RUNNABLE)        continue;      // Switch to chosen process.  It is the process's job      // to release ptable.lock and then reacquire it      // before jumping back to us.      // 找到了一个RUNNABLE的进程      c-&gt;proc = p;      //该cpu上现在运行的进程为*p      switchuvm(p);     // 加载该进程的地址空间      p-&gt;state = RUNNING;   //将进程状态变为RUNNING      swtch(&amp;(c-&gt;scheduler), p-&gt;context);      switchkvm();      // Process is done running for now.      // It should have changed its p-&gt;state before coming back.      c-&gt;proc = 0;    }当调度器找到了一个RUNNABLE的进程，就将cpu-&gt;proc 设置为它的PCB指针，然后调用switchuvm将该进程的内存空间、页表加载进内存中，并将进程状态设置为RUNNING.然后，调用swtch进行上下文的切换。现在，第一个参数（旧的上下文）是调度器的上下文指针的地址&amp;(c-&gt;scheduler), 第二个参数是新进程上下文的指针。这个swtch将保存调度器的上下文，并将c-&gt;scheduler指向保存的位置，然后从调度器的栈换到新进程的内核栈，从栈上加载新进程的上下文，然后转到新进程sched中swtch返回处运行。新进程将马上释放ptable 锁。有的时候，swtch也不一定是返回到新进程的sched处。如果该新进程是刚刚被fork产生，这是它第一次被调度，那么swtch就会返回到forkret处，在forkret中释放ptable锁，再从forkret返回到trapret，退出内核态。当调度器从swtch返回，意味着有某个进程调用了sched把控制权返还给它，它首先调用swtchkvm转换到内核的页表，然后将c-&gt;proc置为0，表示暂时没有进程在该cpu上运行。然后，如果该进程不是位于ptable的最后一个槽，调度器就会继续查找下一个RUNNABLE的进程，重复以上步骤。否则，它将释放ptable锁，并开启中断。睡眠与唤醒睡眠和唤醒提供了进程间通信的机制，它们可以让一个进程暂时休眠，等待某个特定事件的发生，然后当特定事件发生时，另一个进程会唤醒该进程。睡眠与唤醒通常被称为顺序合作或者有条件同步机制。 睡眠是调度发生的另一种情况，当进程调用sleep进入休眠时，它会调用sched把控制权交给调度器。sleep有两个参数，第一个参数chan是休眠被唤醒的信号，这个信号使得进程可以互相通信，一个进程调用sleep(chan)进入休眠，另一个进程用同样的chan调用wakeup(chan)就可以把它唤醒。第二个参数lk是调用休眠时必须持有的一个锁，这个锁主要是为了防止“遗失的唤醒”问题。一般，进程如果需要休眠，它需要循环判断某个条件是否成立（例如磁盘是否已经准备好），如果还不成立，就会调用sleep进入休眠。例如：while(r = something() == 0){    sleep();}之所以需要while循环判断，是因为如果某次事件成立了，进程从休眠中唤醒，但它被唤醒之后可能不是马上就被调度、马上就开始执行后面的代码，所以在这中间，有可能条件又不成立了，所以需要唤醒之后马上继续判断。而如果没有上面所说的lk锁，就可能发生，while判断条件不成立，进程准备进入休眠；但是这时候发生调度，另一个进程使得条件成立，想要唤醒进程，但这时候因为它还没休眠，所以找不到进程可以唤醒。再切换回原来的进程时，这个进程不知道条件已经成立了，它会进入休眠，并且之后再没有办法唤醒它。 因此，必须确保在条件判断和调用sleep是不会被wakeup打断的，即wakeup不可以在进程真正进入sleep之前被调用。这可以用锁来实现。// Atomically release lock and sleep on chan.// Reacquires lock when awakened.voidsleep(void *chan, struct spinlock *lk){struct proc *p = myproc();//调用sleep的进程if(p == 0)panic("sleep");if(lk == 0)panic("sleep without lk");//必须持有lk锁// Must acquire ptable.lock in order to// change p-&gt;state and then call sched.// Once we hold ptable.lock, we can be// guaranteed that we won't miss any wakeup// (wakeup runs with ptable.lock locked),// so it's okay to release lk.// 下面sleep必须修改ptable，把这个进程的状态改为SLEEPING,并把p-&gt;chan也就是休眠的等待事件改为//第一个参数chan，因此要比较传进来的第二个参数lk是不是ptable，如果是ptable，那进程就应该一直持有它，//不应该释放。if(lk != &amp;ptable.lock){ //DOC: sleeplock0acquire(&amp;ptable.lock); //DOC: sleeplock1release(lk);}sleep首先判断是否持有lk锁，否则报错。 然后，sleep需要释放lk锁并获得ptable-&gt;lock。这里有两种情况：  lk不是ptable-&gt;lock，则因为wakeup函数也要求获得ptable-&gt;lock，所以释放lk是没问题的，ptable-&gt;lock代替了lk，保证不会出现“遗失的唤醒”问题。必须在释放lk之前，先获取ptable.lock，否则有可能在释放lk之后，获取ptable.lock之前，发生调度，另一个进程此时便可以调用wakeup。而之所以要获取ptable-&gt;lock是因为之后要对进程表进行访问、修改，并且要调用sched进入调度器。  lk就是ptable-&gt;lock，则不需要任何操作。不可以先释放ptable.lock然后再重新获取，原因跟上面lk释放与ptable.lock获取的顺序不能调换的原因是一样的。然后进程就要进入休眠。// Go to sleep.p-&gt;chan = chan;         //休眠等待信号，wakeup函数中在ptable中查找p-&gt;chan=chan并且状态为休眠的进程去唤醒。p-&gt;state = SLEEPING;    //修改进程状态sched();              //将控制权交给调度器p-&gt;chan = 0;// Reacquire original lock.if(lk != &amp;ptable.lock){ //DOC: sleeplock2release(&amp;ptable.lock);acquire(lk);}调用sched之后，该进程就被暂时挂起了，sched会将上下文切换到调度器，再从调度器切换到另外的进程。当这里的sched返回的时候，表明进程已经被唤醒。它要将等待条件清为0，然后重新获得lk锁。重新获得锁时，则不必先获得lk,再释放ptable.lock，因为此时进程已经从休眠中唤醒了，它不会担心在释放和重新获得之间，其他进程调用wakeup，其他进程是否在这中间调用wakeup对该进程是没有影响的。对应地，wakup函数从ptable中找到状态为SLEEPING并且在chan上休眠的进程（可能多个），将它的状态设置为RUNNABLE，这样它就可以被CPU的调度器调度。// Wake up all processes sleeping on chan.// The ptable lock must be held.static voidwakeup1(void *chan){  struct proc *p;  for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++)    if(p-&gt;state == SLEEPING &amp;&amp; p-&gt;chan == chan)      p-&gt;state = RUNNABLE;}// Wake up all processes sleeping on chan.voidwakeup(void *chan){  acquire(&amp;ptable.lock);  wakeup1(chan);  release(&amp;ptable.lock);}wakeup是wakeup1的加锁版本。wakeup因为要访问并修改ptable，所以需要持有ptable.lock。wakeup作为系统调用让用户去调用时，操作系统而非用户要负责获取ptable.lock锁； 但在系统中，例如exit等函数也会调用wakeup，但在这之前它就已经获得了ptable.lock锁，所以也为内核提供了不加锁的wakeup1版本。waitwait让父进程等待子进程结束，并回收子进程的资源。它返回退出的子进程的pid，如果没有子进程或其他错误情况，则返回-1.// Wait for a child process to exit and return its pid.// Return -1 if this process has no children.intwait(void){	struct proc *p;	int havekids, pid;	struct proc *curproc = myproc();	acquire(&amp;ptable.lock);    //要遍历ptable中的所有进程，找到调用进程的一个子进程    //如果能够找到，需要修改该子进程的状态，因此要对ptable做出修改    //不允许两个进程同时访问ptable，因此要先占有ptable的锁才能对其进行访问和修改。	for(;;){        // Scan through table looking for exited children.        havekids = 0;        for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){        	if(p-&gt;parent != curproc)        		continue;            //找到了该进程的子进程            havekids = 1; //havekids表示调用的进程有子进程            if(p-&gt;state == ZOMBIE){ //有一个子进程调用了exit或因其他原因退出，处于ZOMBIE状态                // Found one.                pid = p-&gt;pid; //准备返回该子进程的pid                kfree(p-&gt;kstack); //释放子进程占用的内存空间，这里是释放子进程的内核栈                p-&gt;kstack = 0; //将进程内核栈底指针重置为0                freevm(p-&gt;pgdir); //释放页表，释放所有用户空间所占据的物理页框                p-&gt;pid = 0; //接下来是将PCB全都重置为0                p-&gt;parent = 0;                p-&gt;name[0] = 0;                p-&gt;killed = 0;                p-&gt;state = UNUSED; //该PCB状态变为UNUSED，之后调用allocproc的时候，这块PCB可能就会被分配给一个新的进程                release(&amp;ptable.lock); //释放ptable锁                return pid; //返回退出的子进程的pid   	 		}		}        // ptable的遍历已经完成，到这里没有返回，说明没有子进程处于ZOMBIE        // No point waiting if we don't have any children.        if(!havekids || curproc-&gt;killed){ //如果该进程并没有子进程，或者该进程在wait的过程中被杀死（比如用ctrl+C等），那就不需要等待了，直接释放ptable锁，返回-1.        	release(&amp;ptable.lock);        	return -1;		}		//否则，该进程有子进程， 但所有的子进程都还在运行，必须让该进程进入休眠，等待一个子进程的结束		// Wait for children to exit. (See wakeup1 call in proc_exit.)		sleep(curproc, &amp;ptable.lock);		//调用sleep进入休眠。    }}wait()函数等待它的一个子进程终止。在exit()函数中，我们看到exit()只是关闭了进程打开的文件和从目录中退出，但仍然保留了进程的信息，进程占据的内存和PCB都没有被释放，只是处于ZOMBIE状态。进程信息和内存空间的释放由wait()来完成。wait遍历过一次进程表，发现有子进程，但是都还没结束，那么它就调用sleep(curproc, &amp;ptable.lock)进入休眠。这个休眠会被子进程在exit中，调用wakeup(myproc()-&gt;parent)来唤醒。Part 2 是如何在xv6中实现Round Robin调度情况统计，统计用户进程的周转、等待、执行时间。下一篇： Part 2]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> xv6 </tag>
        
          <tag> scheduling </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OS操作系统实验：xv6调度(RR, 优先级调度, 优先级队列） 实现和分析 Part 3- xv6 优先级调度算法实现]]></title>
      <url>/lab%20report/2020/01/31/OS4-3/</url>
      <content type="text"><![CDATA[上一篇： Part 2 RR调度周转、等待时间等的统计第一篇： Part 1 xv6调度代码讲解TODO3:  实现优先级调度算法基于优先级的调度算法是一类算法，基本思想是为每一个进程赋予不同的优先级，在调度时优先选择优先级最高的进程来调度。优先级调度有许多种，大致可以分为：      非抢占式的优先级调度：    非抢占式的优先级调度算法中，每个进程只在初始时赋予一个优先级，在运行过程中不会被改变。每一次重新调度时，选择最高优先级的一个进程，将它运行完毕，然后再选择另外一个进程开始运行。一个进程在运行过程中如果有更高优先级的进程到来，它不会被打断，只是将该进程放到等待队列的队首。    xv6是开启抢占的，进程会被时钟中断打断，然后调度器选择另外一个进程（如果有的话）来代替它。        抢占式优先级调度：    抢占式优先级调度中，如果进程运行过程有更高优先级的进程到来，当它运行完这个时间片，就会被抢占。进程被赋予一个初始优先级，但这个优先级是可变的。具体的实现也有多种：                  静态优先级调度：        静态优先级调度指的是进程最初有一个优先级，运行过程中可以通过系统调用改变这个优先级，但是不会随着等待时间或运行时间的增加而自动改变。                    动态优先级调度：        进程最初被指定一个优先级，同样也可以通过系统调用改变。为了惩罚执行时间较长的进程，优先级会随着运行时间增加而逐渐降低。                    多级反馈队列调度：        在系统中设置每个优先级对应的等待队列，进程初始化时进入某个队列。调度时，首先从最高优先级队列中找进程，只有当更高优先级队列为空时，才会调度某个低优先级队列中的进程。 同一个优先级的进程按FCFS调度。        多级反馈队列也有不同的实现。静态实现时，进程优先级不在运行过程中自动改变；动态实现时，可以让进程每运行完k个时间片，就下降到低一级的等待队列，一段时间后，再将进程提高到最高优先级。这样，既可以充分考虑IO-bound、CPU-bound进程的不同特性，又考虑到进程运行过程中IO-bound到CPU-bound的动态转变，减少进程饥饿的发生。                    我将实现静态优先级调度和静态的多级反馈队列调度。静态优先级调度静态优先级调度是为每个进程赋予一个初始优先级，用户可以指定优先级。在每次重新调度时，遍历整个进程表，找到状态为RUNNABLE且优先级最高的进程来运行。优先级的范围是[1,4], 数字越低代表优先级越高。首先为进程添加priority变量：// Per-process statestruct proc {  uint sz;                     // Size of process memory (bytes)  pde_t* pgdir;                // Page table  char *kstack;                // Bottom of kernel stack for this process  .....  int priority;                // 进程的优先级, 取值范围是1-4  uint ctime;                   // 创建时间  ...优先级调度的一个问题是，如何决定进程初始化时的优先级。初始化时的优先级也是系统进程的优先级，这个优先级不能太高，否则会让用户进程响应和等待的时间过长；但也不能太低，否则当用户需要某个内核进程的服务时，等待时间也会太长。这里，先将初始优先级设置为2. 在allocproc()中，进程初始化时：found:  p-&gt;state = EMBRYO;  p-&gt;pid = nextpid++;														//modified here  p-&gt;ctime = ticks;  p-&gt;rutime = 0;  p-&gt;sltime = 0;  p-&gt;retime = 0;														//priority modified  p-&gt;priority = 2;  release(&amp;ptable.lock);新的scheduler函数，在内层for循环中，再遍历整个进程表，用phigh保存目前为止优先级最高的进程。voidscheduler(void){  struct proc *p;  struct cpu *c = mycpu();  c-&gt;proc = 0;    for(;;){    // Enable interrupts on this processor.    sti();    // Loop over process table looking for process to run.    acquire(&amp;ptable.lock);														//PRIORITY modified    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      #if defined RR      if(p-&gt;state != RUNNABLE)            // 原来的 Round Robin 调度        continue;      #elif defined PRIORITY             // 静态优先级调度      struct proc *pnow;                 // 用来遍历ptable的指针      struct proc *phigh=0;              // 遍历过程中保存最高优先级的进程      if(p-&gt;state!= RUNNABLE)         continue;            phigh = p;           // 遍历整个表之前，phigh首先是下一个RUNNABLE的进程。      for( pnow = ptable.proc; pnow&lt;&amp;ptable.proc[NPROC]; pnow++){        if(pnow-&gt;state!=RUNNABLE)     // 遍历表中所有RUNNABLE的进程          continue;         if(pnow-&gt;priority &lt; phigh-&gt;priority){   //如果有一个RUNNABLE的进程优先级比phigh高	      phigh = pnow;							//更新phigh	    }      }      p = phigh;                    //令下一个执行的进程为phigh      #endif            // Switch to chosen process.  It is the process's job      // to release ptable.lock and then reacquire it      // before jumping back to us.      if(p!=0){         cprintf("%d is being scheduled on cpu: %d, its priority is %d.\n", p-&gt;pid, c-&gt;apicid, p-&gt;priority);	 	c-&gt;proc = p;        switchuvm(p);	 	p-&gt;state = RUNNING;	 	swtch(&amp;(c-&gt;scheduler), p-&gt;context);        switchkvm();	 // Process is done running for now.	 // It should have changed its p-&gt;state before coming back.	 	c-&gt;proc = 0;      }    }    release(&amp;ptable.lock);  }}调度器的内层循环用指针p遍历整个进程表，本来每一次找到一个RUNNABLE进程，就切换到该进程。 用priority算法，每次调度一个进程后返回调度器时，p指向下一个RUNNABLE进程，phigh首先指向这个进程，然后遍历整个进程表，如果有更高级的进程，phigh会指向接下来第一个优先级最高的进程；如果没有，则下一个进程就是表中下一个可执行进程。实现系统调用set_priority：intset_priority(int pid, int priority){    struct proc *p=0;    acquire(&amp;ptable.lock);    for(p=ptable.proc; p&lt; &amp;ptable.proc[NPROC]; p++){       if(p-&gt;pid == pid){           p-&gt;priority=priority;           release(&amp;ptable.lock);           return 0;       }              }    // 找不到该pid，错误    release(&amp;ptable.lock);    return -1;}把它封装好之后，修改RRsta.c， 加入set_priority：int stdout=1;int main(int argc, char* argv[]){    if(argc!=2){        printf(1, "usage: priority_sta &lt;fork number&gt; \n");	exit();    }    int forknumber=atoi(argv[1]);    int status;    for(int i=0;i&lt;forknumber;i++)    {        status=fork();        if(status)		{	    	set_priority(status, i%4+1);   //在父进程中，将每个子进程的优先级设置为i%4+1.        }        if(status==0){	    	for(int count=0;count&lt;40; count++){				printf(1," ");		    	for(int k=0;k&lt;1000000;k++){					int result=0;					for( int j=0; j&lt;5000; j++){			    		result+=j*j;                        result-=j*j;                        result= result/j;                        result=result*j;                        result = result+result/j;					}		   		}            }                        exit();        }    }    int rutime;    int retime;    int sltime;    int pid;    int sum=0;    while((pid = waitSch(&amp;rutime,&amp;retime,&amp;sltime))!=-1){        sum+= rutime+retime+sltime;    }    printf(1, " average turn-around time of %d process is %d\n", forknumber, sum/forknumber);    exit();}在内核中，添加一些额外的输出来验证算法实现的正确性。重新编译内核，然后执行priority_sta.c, fork 4个进程：$ priority_sta 4pid为3的进程是父进程，子进程的pid分别为4， 5， 6， 7，创建后set_priority, 使它们的优先级分别为1， 2， 3， 4；进程的初始优先级被设置为2.当进程4被创建，并设置优先级为1之后，它是系统中优先级最高的进程，正确情况下会一直运行直到结束（除非它进入休眠，或者有同样优先级为1的进程到来）；因为有两个cpu，进程3可以在另一个cpu上调度，继续创建进程5，6. 这时，进程4已经结束，系统中有3，5， 6三个进程，但正确情况下只有3和5可以被调度，6必须等待直到3和5不是RUNNING或RUNNABLE状态。从这段程序的输出来看，目前的调度都是正确的。进程6一直没有被调度，直到进程3进入休眠：进程3创建了进程7（最后一个子进程）之后，它就进入休眠等待所有子进程结束。这时进程6被第一次调度，而直到进程5结束，系统只剩下3，6，7时，在进程3休眠时进程7才可被调度。统计所有进程的运行情况：优先级越高的进程平均周转时间越长。从图中可以看出，进程的运行时间相差不大，而优先级最低的进程（7，11）明显等待时间比其他进程要长很多（11）。但从单个进程来看，不一定优先级越低进程周转时间越长，例如进程10（优先级为3）的周转时间就比进程11（优先级为4）长很多，这是因为进程11到达更晚，且它到达时系统中更高优先级的进程几乎已经都执行结束。静态多级反馈队列静态多级反馈其实跟上面的静态优先级调度类似，但是它使用了多级队列的方法，使优先级相同的进程能够按照FCFS来调度。静态优先级调度则不一定是先来先服务的，因为它是遍历整个进程表找最高优先级的进程，这样，当pid序号在前的进程退出被回收之后，前面就会出现空槽，更新创建的进程反而会比老的进程pid序号更低，更先被调度。实现多级反馈队列，首先要为每个优先级定义对应的等待队列。等待队列由一个proc结构体的指针数组来表示，考虑到进队和出队的动作一般发生在进程的优先级发生改变或者状态改变的时候，都会持有ptable.lock,则访问等待队列已经可以保证互斥。因此可以省略等待队列的锁。同时，用变量size来表示等待队列目前的大小，也就是最后一个进程的下标。#if defined SMLtypedef struct queue_t{  int size;  struct proc* proc[NPROC];  int priority;    //for debug}queue_t;struct queue_t pqueue_1;   //4个优先级队列struct queue_t pqueue_2;struct queue_t pqueue_3;struct queue_t pqueue_4;#endif内核开始时，要初始化4条优先级队列，定义一个初始化函数来完成：voidpqueue_init(struct queue_t *queue, int priority){   queue-&gt;priority=priority;     //为方便输出debug，加入变量priority   queue-&gt;size=0;   for(int i=0; i&lt;NPROC; i++)    //初始时指针为0      queue-&gt;proc[i]=0;}voidpinit(void){  initlock(&amp;ptable.lock, "ptable");				  #if defined SML									//sml modified  pqueue_init(&amp;pqueue_1,1);  pqueue_init(&amp;pqueue_2,2);  pqueue_init(&amp;pqueue_3,3);  pqueue_init(&amp;pqueue_4,4);  #endif}等待队列涉及三种操作，即入队，删除队列中的一个进程，以及取等待队列中最早到达的一个RUNNABLE的进程。由于进程的优先级在运行过程中可能动态变化，所以可能要将一个进程从某个队列删除后添加到另一个队列中，这时，为了保持队列从到达时间早到晚排序，要遍历队列，找到该进程的插入位置。从最后（q-&gt;size-1)往前扫描，如果当前指针进程的到达时间ctime比新进程p-&gt;ctime大（晚），就把它往后挪。直到指向一个进程，到达时间&lt;=p-&gt;ctime，然后把p插入到它的后面。最后，要将q-&gt;size增加一。void enqueue(struct queue_t *q, struct proc *p){    cprintf("proc %d enqueueing %d.\n", p-&gt;pid, q-&gt;priority);    int pos;    pos = q-&gt;size - 1;    while(pos &gt;= 0 &amp;&amp; q-&gt;proc[pos]-&gt;ctime &gt; p-&gt;ctime){   //从后往前扫描        q-&gt;proc[pos+1] = q-&gt;proc[pos];    //如果目前进程比新插入进程到达晚，就把它向后挪一个位置		pos--;    }    // 目前pos指向的进程比插入的进程到达时间早或相同。    q-&gt;proc[pos+1] = p;       q-&gt;size ++;   }删除队列的操作，是从前往后遍历该等待队列，找到要删除的进程，如果能找到，则将从该位置开始后面的进程都往前挪一个位置，最后一个位置变为0，最后把size减1.voidremoveq(struct queue_t* q, struct proc *p){   cprintf("removing %d from q: %d\n",p-&gt;pid, q-&gt;priority);   int pos;   int found = 0;   pos = 0;   while(pos &lt; q-&gt;size){      if(q-&gt;proc[pos]==p){   //从前往后，找到该进程          found=1;          break;      }      else{        pos++;      }   }   if(found){   //若找到，此时pos指向要删除的进程      while(pos &lt; q-&gt;size-1){    //从 pos 到倒数第二个进程位置            q-&gt;proc[pos] = q-&gt;proc[pos+1];  //将所有进程往前挪一个位置            pos++;      }      q-&gt;proc[q-&gt;size-1] = 0;      q-&gt;size --;    }   }查找等待队列中第一个RUNNABLE的进程，只要从前往后遍历，找到第一个RUNNABLE的进程就返回即可。如果未找到，则返回0.struct proc*headq(struct queue_t* q){    struct proc* p = 0;    int pos;    for(pos = 0; pos &lt; q-&gt;size; pos++){      if(q-&gt;proc[pos]-&gt;state == RUNNABLE){    //找到第一个RUNNABLE进程，注意等待队列是从到达时间小到大排序的        p = q-&gt;proc[pos];        break;      }    }    return p;}接下来要就是要实现进程在执行过程中状态或优先级变化时相应的等待队列的操作。由于等待队列代价比较高，所以尽量在确定进程已经正确分配好各种资源，才将它加入到初始优先级队列中去。 在userinit中，设置默认优先级为2：  #if defined SML														//sml modified  p-&gt;priority=2;  enqueue(&amp;pqueue_2, p);  #endif在fork()中，让子进程继承父进程的优先级（默认为2），然后进入相应的等待队列：  if((np-&gt;pgdir = copyuvm(curproc-&gt;pgdir, curproc-&gt;sz)) == 0){    kfree(np-&gt;kstack);    np-&gt;kstack = 0;    np-&gt;state = UNUSED;									    return -1;  }  np-&gt;sz = curproc-&gt;sz;  np-&gt;parent = curproc;  *np-&gt;tf = *curproc-&gt;tf;														//sml modified  np-&gt;priority = curproc-&gt;priority;  #if defined SML  switch(np-&gt;priority){     //进入相应的优先级队列     case 1: enqueue(&amp;pqueue_1, np); break;     case 2: enqueue(&amp;pqueue_2, np); break;     case 3: enqueue(&amp;pqueue_3, np); break;     case 4: enqueue(&amp;pqueue_4, np); break;     default: panic("priority must be between 1-4.");  }       #endif在进程终止，被父进程回收资源时，要将它们从相应的等待队列中删除，在wait和waitSch中作同样的改变：intwait(void){  struct proc *p;  int havekids, pid;  struct proc *curproc = myproc();    acquire(&amp;ptable.lock);  for(;;){    // Scan through table looking for exited children.    havekids = 0;    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;parent != curproc)        continue;      havekids = 1;      if(p-&gt;state == ZOMBIE){        // Found one.        pid = p-&gt;pid;        kfree(p-&gt;kstack);        p-&gt;kstack = 0;        freevm(p-&gt;pgdir);                  												//SML modified        #if defined SML        switch(p-&gt;priority){                    //删除等待队列中的进程			case 1: removeq(&amp;pqueue_1, p); break;	     	case 2: removeq(&amp;pqueue_2, p); break;	     	case 3: removeq(&amp;pqueue_3, p); break;	     	case 4: removeq(&amp;pqueue_4, p); break;	     	default: ;		}		#endif	        p-&gt;pid = 0;        p-&gt;parent = 0;        p-&gt;name[0] = 0;        p-&gt;killed = 0;        												// modified        p-&gt;ctime=0;        p-&gt;rutime=0;        p-&gt;retime=0;        p-&gt;sltime=0;        												// end        p-&gt;state = UNUSED;        release(&amp;ptable.lock);调度器的算法则跟之前较为不同。RR和静态优先级调度scheduler都是内循环遍历整个表，每次寻找最高优先级或第一个RUNNABLE进程进行调度，现在，静态多级反馈队列不用遍历整个表，它从最高优先级队列开始找起，如果某个队列里面有RUNNABLE的进程，就运行它，否则就往下一个优先级队列继续寻找：    #if defined RR ||PRIORITY    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      #if defined RR      if(p-&gt;state != RUNNABLE)        continue;      #elif defined PRIORITY      struct proc *pnow;      struct proc *phigh=0;      if(p-&gt;state!= RUNNABLE)         continue;            phigh = p;      for( pnow = ptable.proc; pnow&lt;&amp;ptable.proc[NPROC]; pnow++){        if(pnow-&gt;state!=RUNNABLE)          continue;        if(pnow-&gt;priority &lt; phigh-&gt;priority){	  phigh = pnow;	}      }      p = phigh;      #endif              // Switch to chosen process.  It is the process's job      // to release ptable.lock and then reacquire it      // before jumping back to us.      if(p!=0){         cprintf("%d is being scheduled on cpu: %d, its priority is %d.\n", p-&gt;pid, c-&gt;apicid, p-&gt;priority);	 c-&gt;proc = p;         switchuvm(p);	 p-&gt;state = RUNNING;	 swtch(&amp;(c-&gt;scheduler), p-&gt;context);         switchkvm();	 // Process is done running for now.	 // It should have changed its p-&gt;state before coming back.	 c-&gt;proc = 0;      }    }    #endif                                      //=====================modified begin ================//    #if defined SML                            //多级反馈队列从这里开始    p = headq(&amp;pqueue_1);    if(p==0)      p=headq(&amp;pqueue_2);    if(p==0)      p=headq(&amp;pqueue_3);    if(p==0)      p=headq(&amp;pqueue_4);    if(p){                          //若找得到进程，则运行它，否则暂时释放ptable锁，开启中断，然后再次获取ptable锁，继续从第一优先级队列开始寻找         cprintf("%d is being scheduled on cpu: %d, its priority is %d.\n", p-&gt;pid, c-&gt;apicid, p-&gt;priority);	     c-&gt;proc = p;         switchuvm(p);	     p-&gt;state = RUNNING;	     swtch(&amp;(c-&gt;scheduler), p-&gt;context);         switchkvm();	 // Process is done running for now.	 // It should have changed its p-&gt;state before coming back.	     c-&gt;proc = 0;     }    #endif  当调用set_priority时，有可能进程的优先级改变，则要将它从旧的优先级队列中删除并放到新的优先级队列中：intset_priority(int pid, int priority){    struct proc *p=0;    acquire(&amp;ptable.lock);    for(p=ptable.proc; p&lt; &amp;ptable.proc[NPROC]; p++){       if(p-&gt;pid == pid){                  #if defined SML           if(p-&gt;priority!=priority){           	switch(p-&gt;priority){           	    case 1: removeq(&amp;pqueue_1, p);   break;           	    case 2: removeq(&amp;pqueue_2, p);   break;           	    case 3: removeq(&amp;pqueue_3, p);   break;           	    case 4: removeq(&amp;pqueue_4, p);   break;           	    default: ;           	}           	switch(priority){		       case 1: enqueue(&amp;pqueue_1, p);   break;		       case 2: enqueue(&amp;pqueue_2, p);   break;		       case 3: enqueue(&amp;pqueue_3, p);   break;		       case 4: enqueue(&amp;pqueue_4, p);   break;		       default: ;		}           }           #endif                                 p-&gt;priority=priority;               release(&amp;ptable.lock);           return 0;       }              }    release(&amp;ptable.lock);    return -1;}多级反馈队列实现已经完成。将它编译，执行priority_sta.c，还是将子进程数目设置为4，得到如下输出：进程3是父进程，4，5，6，7分别是4个子进程，优先级被设置为1，2，3，4. 可以看到，每个进程在fork完成之前，都先进入了默认的第二优先级队列。当调用set_priority时，每个进程都从原来的queue 2离开，进入到相应新的队列中（进程5除外，进程5本来就在queue 2中）。当所有进程都创建好之后，因为3比5先到达，则只有进程3和4可以被调度；当进程3在倒数第二行进入休眠之后，进程5才可以得到调度。然后，等进程4退出并被回收了，它从queue 1中被删除。当进程3进入休眠的时候，进程5和6可以得到调度，在这整个过程中，进程7的优先级都不足，无法被调度。而当进程6进入休眠时，进程7才和5一起被调度。说明算法逻辑正确。统计多级反馈下程序运行时间：这个结果是符合预期的，如果程序执行的内容都一样，优先级越低，周转时间应该越长，因为更高优先级的进程应该更早被调度。简单比较比较所有进程的平均轮转时间：            进程个数      RR      静态优先级      多级反馈队列                  4      6      4      4              10      8      4      7              30      15      10      17      （轮转时间的计算跟机器状态也有关系，有些时候机器运转较快，则用户程序的运行和调度算法都比较快，测得时间就会少）如果忽略机器状况的因素，可以发现RR算法的平均轮转时间在进程个数一定时是多于静态优先级调度的。这是因为所有进程轮流调度，本来可以更早结束的进程时间被延长了；而静态优先级调度虽然低优先级的进程周转时间比平均长很多，但优先级高的进程周转时间也对应地低很多，并且如果先来先服务，高优先级进程基本上可以到达之后就一直进行直到结束，所以平均周转时间可能会更好。但也要考虑算法的花销。在进程个数很多的时候，多级反馈队列每一次插入和删除的开销会快速增长，这导致进程的等待时间变得非常长。为了缓解饥饿的问题，可以对优先级调度算法进行优化，采用动态的多级反馈队列：      进程刚开始时进入最高优先级队列。    不同的优先级队列中，进程有不同的时间片。优先级越高，时间片越短。这样，低优先级进程可以有更多机会完成任务。  每运行完一个/k个时间片，进程就下降优先级，最低优先级的进程反而重新回到最高优先级这样做的好处是，进程刚开始创建优先级最高，可以有更快的响应时间；优先级随着进程运行下降，则可以偏向短作业，让短作业更快结束，从而缩短等待时间；每隔一段时间就将最低优先级的进程提高到高优先级，这样可以防止饥饿发生。但是，动态多级反馈队列显然需要更多的算法花销，不同时间片的设计更为复杂，且需要设计每个优先级适当的时间片大小，以及按进程特性决定创建时进入哪个队列。即便如此，动态多级反馈队列仍然是一种比较优的算法。]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> xv6 </tag>
        
          <tag> scheduling </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OS操作系统实验：xv6调度(RR, 优先级调度, 优先级队列） 实现和分析 Part 2- xv6 RR调度情况统计]]></title>
      <url>/lab%20report/2020/01/31/OS4-2/</url>
      <content type="text"><![CDATA[上一篇： Part 1-xv6 调度代码讲解TODO2： 统计RR调度情况实现waitSch系统调用todo2需要增加一个系统调用waitSch(int* rutime, int* retime, int* sltime)作为原来wait的扩展，除了执行原有功能以外，要将进程的运行时间、在等待队列中的时间和休眠时间输入到三个参数所代表的地址中。首先我们将需要维护的数据结构定义在proc.h中：struct proc {  uint sz;                     // Size of process memory (bytes)  pde_t* pgdir;                // Page table  char *kstack;                // Bottom of kernel stack for this process  ...    uint ctime;                   // 创建时间  uint rutime;                  // 处于RUNNING下的时间  uint retime;                  // 处于ready状态下的时间  uint sltime;                  // 处于Sleeping状态下的时间};然后，在proc.c中，也应该做出相关的修改。首先在进程被初始化的时候，应该将p-&gt;ctime设置为当时的时钟ticks，然后其他的几个变量应该初始化为0。考虑到第一个用户进程并不是由fork产生，我们将初始化放在allocproc中：found:  p-&gt;state = EMBRYO;  p-&gt;pid = nextpid++;														//modified here  p-&gt;ctime = ticks;   //创建时间  p-&gt;rutime = 0;        p-&gt;sltime = 0;  p-&gt;retime = 0;  release(&amp;ptable.lock);  // Allocate kernel stack.  if((p-&gt;kstack = kalloc()) == 0){    p-&gt;state = UNUSED;    p-&gt;ctime = 0;     //分配错误的时候，要恢复为0.    return 0;  }相应地，当进程结束，资源要被回收时，应该将这几个变量清空。我们不修改在exit中，因为此时这些进程运行的统计数据仍然要保留，直到父进程调用wait找到它，将它回收的时候，再把这些数据处理完清空。在wait中： if(p-&gt;state == ZOMBIE){        // Found one.        pid = p-&gt;pid;        kfree(p-&gt;kstack);        p-&gt;kstack = 0;        freevm(p-&gt;pgdir);        p-&gt;pid = 0;        p-&gt;parent = 0;        p-&gt;name[0] = 0;        p-&gt;killed = 0;        												// modified        p-&gt;ctime=0;	    p-&gt;rutime=0;	    p-&gt;retime=0;	    p-&gt;sltime=0;        												// end        p-&gt;state = UNUSED;        release(&amp;ptable.lock);        return pid;接下来是实现waitSch，waitSch的大部分逻辑都和wait一样，不同的只是要先把p-&gt;rutime赋给*rutime，把p-&gt;retime赋给*retime, 以及p-&gt;sltime→*sltime，然后再把这些变量清空:int waitSH(int* rutime, int* retime, int* sltime){  struct proc *p;  int havekids, pid;  struct proc *curproc = myproc();    acquire(&amp;ptable.lock);  for(;;){    // Scan through table looking for exited children.    havekids = 0;    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){      if(p-&gt;parent != curproc)        continue;      havekids = 1;      if(p-&gt;state == ZOMBIE){        // Found one.        pid = p-&gt;pid;        kfree(p-&gt;kstack);        p-&gt;kstack = 0;        freevm(p-&gt;pgdir);        p-&gt;pid = 0;        p-&gt;parent = 0;        p-&gt;name[0] = 0;        p-&gt;killed = 0;        																// modified	    *rutime = p-&gt;rutime;	    *sltime = p-&gt;sltime;	    *retime = p-&gt;retime;	    p-&gt;ctime=0;	    p-&gt;rutime=0;	    p-&gt;retime=0;	    p-&gt;sltime=0;        																// end        p-&gt;state = UNUSED;        release(&amp;ptable.lock);        return pid;      }    }    // No point waiting if we don't have any children.    if(!havekids || curproc-&gt;killed){      release(&amp;ptable.lock);      return -1;    }    // Wait for children to exit.  (See wakeup1 call in proc_exit.)    sleep(curproc, &amp;ptable.lock);  //DOC: wait-sleep  }} 现在的问题是，如何动态地维护进程的运行时间、等待时间和休眠时间呢？首先一个直观的思路是，在每一次进程状态变化的时候，例如由A→B，记录下这次状态转换的时间作为B状态的开始时间$t_{0}$, 等下一次再从B→C的时候，B的持续时间就加上这个时刻减去$t_{0}$. 但是这种方法实现起来非常复杂，首先进程状态转换可能在很多种情况下发生，每一次转换时，都要判断旧的状态和新的状态是什么，然后更新旧状态的持续时间，保存新状态的开始时间。这样代码会变得冗长而且容易出错，还需要为进程维护两倍的变量（对每一种状态，例如RUNNING, 至少需要维护最近一次RUNNING状态开始的时刻，以及累计运行时间）。另一种思路是，在每一次收到时钟中断时，判断进程处于何种状态，为这种状态的持续时间加一。这样的计算是一种很粗糙的近似。这样计算，是假设进程会将这种状态维持一个时间片，在中间不发生状态的变换，并假设这种状态大概在时钟中断发生时开始。然而，在两个时钟中断之间，进程是很有可能调用sleep或wait进行休眠的，也有可能在下一个时钟中断到来之前，调度器就已经调度了另一个进程并将该进程唤醒，状态变为RUNNABLE，这样，中间的sleep阶段就没有被我们的计算捕捉到。不过因为进程一般是运行和等待的时间占大多数，sleep占比很少，运行和等待之间的切换又一般是通过时钟中断引发的yield, 所以这种近似还是可以接受的。xv6 的时钟机制xv6的时钟在timer.c中实现，每过100ms，硬件就会产生一个时钟中断。每个cpu都可以独立地接收时钟中断，并陷入中断处理。在trap.c中定义了一个uint类型的变量ticks，每一次这个变量加一，代表系统的时钟往前走了一步。两个cpu收到时钟中断后，先后进入中断处理程序，为了让系统的时钟(ticks)能够真正在每次timer产生时钟中断的时候自增一，只在某一个固定的cpu收到中断时让ticks++, 因此xv6中每一次CPU0收到一个时钟中断，ticks就自增1：switch(tf-&gt;trapno){  case T_IRQ0 + IRQ_TIMER:     //收到了时钟中断    if(cpuid() == 0){          //如果是cpu0收到了这个时钟中断，才将tick增加1；这样对于不同的cpu，时钟都是保持同步的，所有的cpu都会共用这个tick变量。      acquire(&amp;tickslock);             ticks++;      wakeup(&amp;ticks);      release(&amp;tickslock);    }    lapiceoi();    cprintf("ashajh,time: %d, cpu: %d\n",ticks, cpuid());    break;编译这段代码，输出是：这段输出中，每一次cpu id 为0时，对应的时钟比上一次printf增加一，而对于cpu1则不一定是这样。如果我们要在每次时钟中断时判断进程的状态，也有两种思路。第一个是，在时钟中断的时候，调用myproc先判断cpu上是否有进程在运行，如果有，则判断它的状态并更新变量。这种思路首先因为进程会在两个cpu上调度，而在cpu1上调度时，两次中断之间ticks不一定有增加，所以要增加一个lasttick保存上一次中断时ticks的值，如果这个值有改变，才会进行判断。但即使是添加了这个逻辑，这种方法也是错误的，因为如果myproc()能返回进程的PCB， 进程一定是处于RUNNING状态（否则它就不会被cpu调度了），这样我们无法知道进程何时在休眠或等待。第二个思路是，在ticks++的时候，遍历整一个ptable表，对每个进程判断它是在何种状态，然后给变量+1.这样便可实现上面那种近似的算法。 在trap.c中作以下修改：  switch(tf-&gt;trapno){  case T_IRQ0 + IRQ_TIMER:    if(cpuid() == 0){      acquire(&amp;tickslock);      ticks++;      									//modified      update();                         //每一次ticks++时，更新进程状态变量      wakeup(&amp;ticks);      release(&amp;tickslock);    }    lapiceoi();    break;其中，判断进程状态并更新变量的函数在proc.c中实现：void update(){    struct proc *p;    acquire(&amp;ptable.lock);    //对进程变量进行修改，要先获得ptable锁。    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++){    switch(p-&gt;state) {   //遍历整个表，判断进程状态      case RUNNING:      //运行中        p-&gt;rutime++;     //则运行时间增加一个时间片        break;      case SLEEPING:    //休眠中        p-&gt;sltime++;     //休眠时间增加一个时间片        break;      case RUNNABLE:    //等待中        p-&gt;retime++;    //等待时间增加一个时间片        break;      default: ;    // default，有ZOMBIE\EMBRYO\UNUSED，这些状态我们不关心    }  }  release(&amp;ptable.lock);  //释放锁}    我们按照上次实验的步骤将waitSch包装成系统调用。 实现过程中比较重要的是，xv6syscall.c中系统调用函数的统一格式是返回值int, 不允许带参数。如果需要加入参数，则参数会被压入栈中，通过argint和argptr从栈上取出参数进行解析。在sysproc.c中包装waitSch如下：intsys_waitsch(void){    int *rutime;    int *retime;     int *sltime;    int pid;    rutime=retime=sltime=0;    if(argptr(0, (char**)&amp;rutime, sizeof(int)) &lt; 0)        return -1;    if(argptr(1, (char**)&amp;retime, sizeof(int)) &lt; 0)        return -1;    if(argptr(2, (char**)&amp;sltime, sizeof(int)) &lt; 0)        return -1;    pid = waitSH(rutime, retime, sltime);    if(pid!=-1)    //如果有子进程退出，打印出它的pid，运行时间、等待时间和休眠时间。        cprintf("pid: %d, runtime:%d, ready time: %d, sleep time %d\n", pid, *rutime, *retime, *sltime);    return pid;} 在syscall.h以及系统调用表中，为waitSch增加系统调用号，并在user.h中定义接口。waitSch的实现已经完成。编写RRsta.c 统计RR调度情况编写RRsta.c， main函数接收一个命令行参数n, fork n个子进程进行相同的大规模计算，然后父进程调用waitSch，等待每个子进程的结束，并输出每个子进程的运行时间、等待时间和休眠时间。最后统计n个进程在RR（Round Robin）调度下的平均轮转时间。#include "param.h"#include "types.h"#include "stat.h"#include "user.h"#include "fs.h"#include "fcntl.h"#include "syscall.h"#include "traps.h"#include "memlayout.h"int stdout=1;int main(int argc, char* argv[]){    int forknumber=atoi(argv[1]);   //命令行参数，为fork 子进程的个数    int status;    for(int i=0;i&lt;forknumber;i++)        {        status=fork();        if(status==0){    //在子进程内status=pid=0            for(int count=0;count&lt;40; count++){  //执行相同的运算，主要是大规模乘、除法计算。                printf(1," ");                for(int k=0;k&lt;1000000;k++){                    int result=0;                    for( int j=0; j&lt;5000; j++){                        result+=j*j;                        result-=j*j;                        result= result/j;                        result=result*j;                        result = result+result/j;                    }		    	}            }                        exit();   //一定要注意exit，否则子进程也会执行上面的fork循环。        }    }    int rutime;   //运行时间    int retime;    int sltime;    int pid;      //子进程pid    int sum=0; //周转时间总和    while((pid = waitSch(&amp;rutime,&amp;retime,&amp;sltime))!=-1){        sum += rutime+retime+sltime;    }    printf(1,"average turn-around time of %d process is %d.\n", forknumber, sum/forknumber);    exit();}将RRsta.c编译到qemu中，在shell中运行make cleanmakemake qemu$ RRsta 10得到输出：（实际上，手动计算平均周转时间的话会得到5.8， 但是xv6的printf并没有使用C的标准输出库，不支持输出浮点数，只能将小数省去了。下一篇： Part 3： 优先级和动态多及反馈队列的实现、调度算法比较]]></content>
      <categories>
        
          <category> Lab Report </category>
        
      </categories>
      <tags>
        
          <tag> OS </tag>
        
          <tag> xv6 </tag>
        
          <tag> scheduling </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[论文笔记]基于个性化注意力机制的新闻推荐]]></title>
      <url>/paper%20reading/2020/01/30/NPA/</url>
      <content type="text"><![CDATA[基于个性化注意力机制的新闻推荐论文链接：https://arxiv.org/pdf/1907.05559.pdf这篇论文是2019年SIGKDD的7篇精选论文之一。主要解决的是新闻推荐中个性化的问题。Introduction首先介绍了为什么需要新闻推荐（减少信息过载，高效获取信息）新闻推荐有什么样的难题（可以归结为如何表示新闻，以及如何表示用户兴趣）：这篇论文的作者之前已经做过一项工作，就是多视角学习的新闻推荐，所谓多视角，就是用标题、内容、类别和子类别分别去表示一篇新闻，结果是类别和子类别获得了最高的attention权重。但是在这一篇论文中，作者选择了用标题来表示新闻，原因可能是，相比内容标题更短代价更小，但是相比类别又具有更多潜在信息。但是，一个新闻标题中，并不是所有词汇都同样关键，例如That 和 Crazy的重要程度就显然不同，因此可以引入基于单词的注意力机制， 也就是给不同的单词不同的权重，来捕捉关键信息。对于第二个问题，也就是如何对用户兴趣建模，每个用户虽然点了很多篇新闻，但是，他们不是对这些新闻都同样地感兴趣，不同的新闻对用户兴趣的建模关键程度也不同。这样就可以引入一个基于新闻的注意力机制来解决。然后作者列举了已有的一些新闻推荐模型，这些模型有的也有使用attention机制来识别关键因素，但是这些attention网络都是静态的，都无法做到权重根据用户来调整。不同单词对新闻内容表示的重要程度，以及新闻对用户兴趣表示的重要程度，都是因人而异的。不同的用户看到一样的标题，他们的关注点可能不同，点击了同样的几篇文章，他们对同一篇文章的感兴趣程度也不同。因此，这两层的注意力机制还必须是个性化的，才能解决个性化推荐的问题。NPA模型模型分为三个部分：  编码新闻的news encoder  编码用户兴趣的 user encoder  预测点击新闻的概率的click predictornews encoder和user encoder中都使用了word-和new-两个层级的注意力网络，并且注意力网络的权重是个性化的， 相同单词、相同新闻对每个用户的权重可能不同。News Encoder根据新闻的标题来编码一篇新闻。分为3个子模块。  word embedding(word2vec算法)          NLP中的最小单位是单词，单词组成句子，句子再组成文章。而神经网络只能接受数值性输入，因此要将自然语言的单词转换成数值向量，就是word embedding的过程。      skip-gram方法：                  输入一个单词x，预测它的上下文y；x的表示形式用one hot encode，V是词典的大小，则输入的向量就是V维的。从x到输出f(X)是一个有一个隐层的神经网络，但是隐层是线性的，没有激活函数！ 输出也是V维的，输出是x的上下文是这V个单词的概率。          在训练完成之后，得到神经网络的权重（输入到隐层，隐层到输出层）分别是输入向量和输出向量，输入向量的维度则和隐藏层节点个数相同，因为x只有一个节点是1，其他都是0，所以输入到隐层节点的权重不会每一个都一样（否则输出就会一样），所以可以用输入向量（或者输出向量）唯一表示一个单词。这样相当于将V维的输入降维。                    论文里说用一个V*D维的word embedding矩阵来做word embedding，D的维数为300, 这里用的是已经预训练好的GloVe矩阵        [NLP] 秒懂词向量Word2vec的本质    CNN： 用来观察标题中的每一个局部，发掘出局部中隐藏的信息。这个卷积层的意义是，让每一个词的表示向量不仅包含这个单词本身的信息，还要包含窗口为2k+1的上下文信息。          这个卷积就只是一层\( N_{f} \)个卷积核，加偏置B（B的维度就是Nf维），然后用Relu激活）。      window size = 2k+1，\( \mathbf{e} \)是word embeddings连接起来得到的矩阵。输出是\( c_{1} \cdot \cdot \cdot c_{m} \)，是包含了上下文信息的词表示向量。        Word Level Attention Network： 这是这个模型的重要特点之一，这里体现了个性化的注意力机制。因为每个用户在标题中对每个单词的关注度是不同的，所以attention 网络就不能像传统的注意力网络一样，对每一个用户有相同的query vector。这里使用的方法是：          先将用户的ID进行embedding变成一个De维的向量\( e_{u} \)（ 这里的问题，user ID不是单词是各种符号的集合如何embedding?  )      再将用户IDembedding向量，经过一个单层的网络映射成preference query vector。这个网络有一个Relu激活： （fig3中的红色向量）                  每一个词表示向量的权重计算是：综合了词表示向量和query vector。 （fig3中的橙色向量就是权重向量α）                      最终，新闻表示就是所有词表示的加权和：          User Encoder  user encoder是对于一个用户，将所有他点击过的新闻（用该用户的ID embedding进行word attention 之后得到的新闻表示）作为输入，经过一个news level attention 网络，得到该用户的表示。      个性化attention网络的思路跟前面差不多，还是利用了user ID embedding，生成另一个query vector：            query vector qd 和 ID embedding 内积得到新闻表示的权重向量，用同样的方法：        最终，用户的表示向量 \( \mathbf{u} \) 是该用户点击过的新闻表示向量加权和。Click Predictor在新闻推荐中，因为用户会在看到的大量新闻中，只点击其中非常少数的新闻，所以在这个问题中，正负样本是非常不平衡的，如果直接在所有candidate新闻样本上预测，训练效果显然不好，并且训练过程要花费很多时间。因此，提出一种正负样本平衡的训练方法，即联合预测K+1个样本，其中有K个负样本，1个正样本，预测每一个样本的click score：预测值yi用 新闻的表示和用户表示向量的内积得到，得到的yi值 要在K+1个样本中softmax归一化。这样，整个predictor就可以看成是一个伪二分类问题，可以使用交叉熵loss函数来训练，loss function是：只考虑正样本的损失值这样，模型从predictor到最底层的attention、CNN的参数，都是可以用back propogation来调节的。这里使用Adam来优化。实验数据集数据在MSN新闻上采集，具体情况看table1，正负样本的比例是13. 这一小节还具体列举了模型一些超参数的设置。 测试集采用最近一个星期的数据，另外随机采样了10%的数据作为验证其中，user embedding（用user ID得到的）维度是De=50， 两个查询向量 的维度Dq和Dd的维度都是200.metrics：  AUC： AUC的涵义是ROC曲线下的面积。 TPrate是指，真值为1 的数据被模型预测为真的频率， FPrate是指，真值为0的数据被模型预测为真的频率。 ROC曲线的纵轴是TPrate，横轴是FPrate。 如果曲线是y=x，那么说明无论真值是0还是1，模型都以一样的概率预测为真或假，说明模型毫无辨别能力。 AUC则是ROC曲线下的面积，一般来说要在一定的FPrate下，TPrate越高越好，那么面积就要越大越好，最坏的情况就是0.5.  AUC的好处是同时考虑了模型对正例和负例的预测能力，可以规避正负样本很不平衡时带来的问题。如何理解机器学习和统计中的AUC？      MRR： 用来评价搜索算法的一种指标，如果第n个结果匹配，得分为 1/n ， 最后得分为总和。        DCGp： 是用来评价搜索算法排序好坏的一种指标，要人为地将结果分为几个等级，每个等级对应一个分数，然后分数根据排序位置衰减，最终DCG分数是p个结果得分的总和（论文里面有预测5个的也有预测10个的分数，10个分数肯定是更高的）        nDCG： 是相对DCG，是先将人工排好序的结果作为理想状态，计算此状态下的IDCG，然后用预测得到的结果除以IDCG,得到相对的nDCG。  实验结果  有使用神经网络的模型比使用矩阵分解的传统算法效果好  使用了negative mining的算法比不使用的算法效果更好  使用attention 机制的算法也普遍比没有使用的要好，因为新闻中的不同词以及不同新闻对于新闻本身以及用户兴趣的表现重要程度确实不同。  NPA算法在被比较的算法中表现最好。]]></content>
      <categories>
        
          <category> Paper Reading </category>
        
      </categories>
      <tags>
        
          <tag> recommend system </tag>
        
          <tag> attention </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
